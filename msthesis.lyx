#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass UNCC-thesis
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package none
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks true
\pdf_backref false
\pdf_pdfusetitle true
\pdf_quoted_options "linkcolor=blue, urlcolor=blue, citecolor=green, bookmarksnumbered=true, bookmarksdepth=1"
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 0
\use_package mathtools 1
\use_package mhchem 0
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 2
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes true
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\author 215191885 "Akash"
\author 1699939148 "arwillis"
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pagenumbering{roman}
\end_layout

\begin_layout Plain Layout


\backslash
fbmatterchapterformat
\end_layout

\begin_layout Plain Layout

% Doctype should be either dissertation proposal, dissertation, or thesis.
\end_layout

\begin_layout Plain Layout

% If you're getting a master's, specify "thesis" below.
  
\end_layout

\begin_layout Plain Layout

% If you're getting a PhD, specify "dissertation" below.
\end_layout

\begin_layout Plain Layout


\backslash
doctype{thesis}
\end_layout

\begin_layout Plain Layout

%%%%%%%%%%%%%%%%     IMPORTANT! IMPORTANT! IMPORTANT! %%%%%%%%%%%%%%%%
\end_layout

\begin_layout Plain Layout

% The rules below MUST be followed for the abstract page and chapter titles
\end_layout

\begin_layout Plain Layout

% to be correctly formatted.
\end_layout

\begin_layout Plain Layout

%
\end_layout

\begin_layout Plain Layout

% 1.
 Only the first letter of the entire title should be capitalized to allow
 the 
\end_layout

\begin_layout Plain Layout

%    title to appear as required by the graduate school on the Abstract
 page.
\end_layout

\begin_layout Plain Layout

% 2.
 Write chapter titles in ALL CAPS.
\end_layout

\begin_layout Plain Layout

%
\end_layout

\begin_layout Plain Layout


\backslash
title{
\change_deleted 215191885 1521733925
Structure From Motion
\change_inserted 215191885 1521733925
Structure-From-Motion and RGBD Depth Fusion
\change_unchanged
}
\end_layout

\begin_layout Plain Layout


\backslash
author{Akash Chandra Shekar}
\end_layout

\begin_layout Plain Layout


\backslash
degree{Master of Science}
\end_layout

\begin_layout Plain Layout


\backslash
major{Computer Science}
\end_layout

\begin_layout Plain Layout


\backslash
publicationyear{2018}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
advisor{
\change_deleted 215191885 1524264138
Dr.
 Andrew Willis
\change_inserted 215191885 1524264150
Dr.Andrew R.Willis
\change_unchanged
}
\change_inserted 215191885 1521524005

\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

% Add the full name and title of all your committee members,
\end_layout

\begin_layout Plain Layout

% apart from your advisor, one by one.
  The style file expects
\end_layout

\begin_layout Plain Layout

% 3 to 5 committee members in addition to your advisor.
\end_layout

\begin_layout Plain Layout

\change_inserted 215191885 1521524112

\end_layout

\begin_layout Plain Layout

\change_inserted 215191885 1521524116


\backslash
committeeMember{Dr.
 Min Shin}
\change_unchanged

\end_layout

\begin_layout Plain Layout


\backslash
committeeMember{Dr.

\change_deleted 215191885 1524264157
 Min Shin
\change_inserted 215191885 1521523595
Srinivas Akella
\change_unchanged
}
\end_layout

\begin_layout Plain Layout


\backslash
committeeMember{Dr.

\change_deleted 215191885 1524264159
 Jianping Fan
\change_inserted 215191885 1521523647
Hamed Tabkhi
\change_unchanged
}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

% Generate the preliminary title page and copyright page.
\end_layout

\begin_layout Plain Layout


\backslash
maketitlepage
\end_layout

\begin_layout Plain Layout


\backslash
makecopyright
\end_layout

\end_inset


\end_layout

\begin_layout Abstract

\change_deleted 215191885 1521734264
The Structure from 
\change_deleted 1699939148 1515786609
m
\change_deleted 215191885 1521734264
Motion (SfM) systems use knowledge of a camera's image formation model and
 a time-sequence of images from the camera to estimate camera motion and
 3D scene structure.
\change_deleted 1699939148 1515786717
involves two aspects, one is to track the camera
\change_deleted 215191885 1521734264
 Formulations of this problem decompose the problem into two sub-problems:
 
\change_deleted 1699939148 1515786757
trajectory by 
\change_deleted 215191885 1521734264
 (1) solving for
\change_deleted 1699939148 1515786772
ing
\change_deleted 215191885 1521734264
e 
\change_deleted 1699939148 1515786851
nonlinear
\change_deleted 215191885 1521734264
the camera trajectory 
\change_deleted 1699939148 1515786856
 equation
\change_deleted 215191885 1521734264
 and 
\change_deleted 1699939148 1515786861
other is to
\change_deleted 215191885 1521734264
solving for 
\change_deleted 1699939148 1515786868
 estimate
\change_deleted 215191885 1521734264
 the depth of 3D scene points observed by the camera in multiple images.
 Solutions are provided by 
\change_deleted 1699939148 1515786913
by solving
\change_deleted 215191885 1521734264
 finding 
\change_deleted 1699939148 1515786917
Stereo 
\change_deleted 215191885 1521734264
correspondences between 
\change_deleted 1699939148 1515786923
of sub-set of 
\change_deleted 215191885 1521734264

\begin_inset Formula $(x,y)$
\end_inset

 locations
\change_deleted 1699939148 1515787163
pixels
\change_deleted 215191885 1521734264
 in images take from different poses taken at distinct times in the camera
 trajectory.
 
\change_deleted 1699939148 1515786976
The t
\change_deleted 215191885 1521734264
Traditional SfM
\change_deleted 1699939148 1515786982
Structure From Motion
\change_deleted 215191885 1521734264
 
\change_deleted 1699939148 1515786986
functions by 
\change_deleted 215191885 1521734264
find
\change_deleted 1699939148 1515786988
ing
\change_deleted 215191885 1521734264
 these correspondences 
\change_deleted 1699939148 1515786994
between pair of images,
\change_deleted 215191885 1521734264
 by extracting and tracking 
\change_deleted 1699939148 1515786999
the 
\change_deleted 215191885 1521734264
features
\change_deleted 1699939148 1515787197
 like
\change_deleted 215191885 1521734264
, e.g., corner points, 
\change_deleted 1699939148 1515787229
lines etc
\change_deleted 215191885 1521734264
match local regions within images.
 However, these 
\change_deleted 1699939148 1515787268
features
\change_deleted 215191885 1521734264
approaches 
\change_deleted 1699939148 1515787017
do not necessarily 
\change_deleted 215191885 1521734264
provide very sparse geometric descriptions of the scene structure and omit
 much of
\change_deleted 1699939148 1515787046
all
\change_deleted 215191885 1521734264
 the geometric information 
\change_deleted 1699939148 1515787052
regarding
\change_deleted 215191885 1521734264
in the image.
 
\change_deleted 1699939148 1515787057
The 
\change_deleted 215191885 1521734264
Other 
\change_deleted 1699939148 1515787062
more accurate method
\change_deleted 215191885 1521734264
, semi-dense, approaches 
\change_deleted 1699939148 1515787072
is to 
\change_deleted 215191885 1521734264
attempt to find large collections of correspondences by solving for pixel-level
\change_deleted 1699939148 1515787101
use the pixel intensity itself
\change_deleted 215191885 1521734264
 correspondences which provide more geometric information about the scene.
 Unfortunately, unlike features, pixel values may are susceptible to viewpoint,
 illumination, and other image formation phenomena which requires semi-dense
 approaches to
\change_deleted 1699939148 1515787356
to minimize
\change_deleted 215191885 1521734264
 consider 
\change_deleted 1699939148 1515787363
the P
\change_deleted 215191885 1521734264
photometric 
\change_deleted 1699939148 1515787370
and
\change_deleted 215191885 1521734264
correction 
\change_deleted 1699939148 1515787380
Geometric errors, by defining the robust error functions.
\change_deleted 215191885 1521734264
in order to robustly determine correspondences.
 
\change_inserted 215191885 1521734267

\end_layout

\begin_layout Abstract

\change_inserted 215191885 1523498865
This article describes a technique to augment a typical RGBD sensor by integrati
ng depth estimates obtained via Structure-from-Motion (SfM) with depth measureme
nts from an RGBD sensor.
 Limitations in the RGBD depth sensing technology prevent capturing depth
 measurements in four important contexts: (1) distant surfaces (>8m), (2)
 dark surfaces, (3) brightly lit indoor scenes and (4) sunlit outdoor scenes.
 SfM technology computes depth via multi-view reconstruction from the RGB
 image sequence alone.
 As such, SfM depth estimates do not suffer the same limitations and may
 be computed in all four of the previously listed circumstances.
 This work describes a novel fusion of RGBD depth data and SfM-estimated
 depths to generate an improved depth stream that may be processed by one
 of many important downstream applications such as robot localization, robot
 mapping, robot navigation, object tracking, pose estimation, and object
 recognition.This approach is demonstrated on sequences of images that transition
 from indoor scenes, where the RGBD depth sensor can function, to outdoor
 scenes, where the RGBD depth sensor fails.
\change_unchanged

\end_layout

\begin_layout Acknowledgements

\change_inserted 215191885 1524264062
I would like to express my gratitude to Dr.
 Andrew R.
 Willis, for taking me under his wing and encouraging me at every step of
 the way with his exceptional understanding of the subject.
 His invaluable insights played a crucial role in this research.
 I would also like to thank my co-advisor, Dr.
 Min Shin, for enabling me to continue research from the Electrical department
 and for guiding me in the right direction.
 Additionally, I appreciate the members of my committee, Dr.
 Srinivas Akella, and Dr.
 Hamed Tabkhi, for their constant support.
 Last but not the least, I would like to acknowledge Mr.
 John Papadakis for all the thought-provoking discussions.
\change_deleted 215191885 1524264064
If you decide to have a acknowledgements page, your acknowledgement text
 would go here.
\end_layout

\begin_layout Acknowledgements

\change_deleted 215191885 1524264064
The Acknowledgement page should be brief, simple, and free of sentimentality
 or trivia.
 It is customary to recognize the role of the advisor, the other members
 of the advisory committee, and only those organizations or individuals
 who actually aided in the project.
 Further, you should acknowledge any outside source of financial assistance,
 such as GASP grants, contracts, or fellowships.
\change_unchanged

\end_layout

\begin_layout Dedication
If you decide to have a dedication page, your dedication text would go here.
\end_layout

\begin_layout Dedication
The Dedication page, if used, pays a special tribute to a person(s) who
 has given extraordinary encouragement or support to one's academic career.
\end_layout

\begin_layout Standard

\change_deleted 215191885 1524264440
If you decide to have an introduction page, your introduction text would
 go here.
 
\end_layout

\begin_layout Standard

\change_deleted 215191885 1524264440
Depending on the discipline or the requirements of the student's advisory
 committee, an Introduction may be included as a preliminary page.
\change_unchanged

\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Standard
\begin_inset FloatList figure

\end_inset


\end_layout

\begin_layout Standard
\begin_inset FloatList table

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newpage
\end_layout

\begin_layout Plain Layout


\backslash
renewcommand{
\backslash
nomname}{LIST OF ABBREVIATIONS}
\end_layout

\begin_layout Plain Layout

% uncomment line below to title your nomenclature list as LIST OF SYMBOLS
\end_layout

\begin_layout Plain Layout

%
\backslash
renewcommand{
\backslash
nomname}{LIST OF SYMBOLS}
\end_layout

\begin_layout Plain Layout

%
\end_layout

\begin_layout Plain Layout

% NOTE: IF YOU USE A LIST OF ABBREVIATIONS / LIST OF SYMBOLS and are using
 command-line LaTeX (not LyX) 
\end_layout

\begin_layout Plain Layout

% YOU MUST COMPILE THE NOMENCLATURE INDEX
\end_layout

\begin_layout Plain Layout

% example:
\end_layout

\begin_layout Plain Layout

% bash$> pdflatex msthesis.tex
\end_layout

\begin_layout Plain Layout

% bash$> makeindex msthesis.nlo -s nomencl.ist -o msthesis.nls
\end_layout

\begin_layout Plain Layout

% bash$> pdflatex msthesis.tex
\end_layout

\begin_layout Plain Layout

%
\end_layout

\begin_layout Plain Layout


\backslash
addcontentsline{toc}{chapter}{
\backslash
nomname}
\end_layout

\end_inset


\begin_inset CommandInset nomencl_print
LatexCommand printnomenclature
set_width "auto"

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1524686201
\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "ECE"
description "An acronym for Electrical and Computer Engineering."

\end_inset


\change_inserted 215191885 1524686217

\end_layout

\begin_layout Standard

\change_inserted 215191885 1524695867
\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "BA"
description "Bundle adjustment"

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1524695867
\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "IR"
description "Infrared ray"

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1524695389
\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "LSD SLAM"
description "Large Scale Direct Simultaneous localization and mapping"

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1524696496
\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "LIDAR"
description "Light Detection and Ranging "

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1524696496
\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "LKT"
description "Lucas Kanade Tomasi"

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1524689149
\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "LVM"
description "Levenberg Marquardt"

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1524696273
\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "RGBD"
description "Red Green Blue Depth"

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1524696273
\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "ROS"
description "Robot Operating Syste"

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1524687837
\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "SE"
description "Special euclidean"

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1524687964
\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "SfM"
description "Structure from motion"

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1524696399
\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "SIM"
description "Similarity"

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1524696400
\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "SIFT"
description "Scale-Invariant Feature Transform"

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1524696579
\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "SLAM"
description "Simultaneous localization and mapping."

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1524696579
\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "SURF"
description "Speeded-Up Robust Features"

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1524695629
\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "SO"
description "Special orthogonal"

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1524695629
\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "SONAR"
description "Sound Navigation And Ranging"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newpage
\end_layout

\begin_layout Plain Layout


\backslash
setcounter{page}{1}
\end_layout

\begin_layout Plain Layout


\backslash
pagenumbering{arabic}
\end_layout

\begin_layout Plain Layout

% 2 inch top spacing for new chapters
\end_layout

\begin_layout Plain Layout


\backslash
bodychapterformat
\end_layout

\end_inset


\end_layout

\begin_layout Chapter
INTRODUCTION
\change_inserted 215191885 1521736201

\end_layout

\begin_layout Standard

\change_inserted 215191885 1521736202
RGBD sensors are a relatively new class of image sensors.
 Their key novel feature is the ability to simultaneously capture color
 
\begin_inset Quotes eld
\end_inset

RGB
\begin_inset Quotes erd
\end_inset

 images of the scene and depth 
\begin_inset Quotes eld
\end_inset

D
\begin_inset Quotes erd
\end_inset

 images of scene; hence the term 
\begin_inset Quotes eld
\end_inset

RGBD.
\begin_inset Quotes erd
\end_inset

 RGB images are capture using a conventional visible light camera that incorpora
tes a lens that focuses light rays from scene locations onto distinct light-sens
itive pixels of image sensor.
 RGBD integrate the three devices: (1) an IR projector, (2) and IR camera
 and (3) a RGB camera in a rigid relative geometry to create a single sensor
 that captures color-attributed 
\begin_inset Formula $(X,Y,Z)$
\end_inset

 surface data at ranges up to ~6m with frame rates up to 30 Hz.
 RGBD sensors have a wide range of applications which include mapping, localizat
ion, pose estimation, and object recognition.
 They have become popular for their ease-of-use and low cost in comparison
 with the other visual sensor technologies such as LIDAR, and have been
 incorporated into consumer products like mobile phones, gaming consoles,
 and automobiles 
\begin_inset CommandInset citation
LatexCommand cite
key "litomisky2012consumer"

\end_inset

.
\end_layout

\begin_layout Section

\change_inserted 215191885 1521736202
RGBD Sensing Technology and Limitations
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521736202
Depth image formation is accomplished using structured light technology
 to measure the geometric position of viewed surfaces.
 This is accomplished by illuminating scene surfaces with an infrared (IR)
 projector having a known pattern and then using an IR camera to capture
 the projected pattern 
\begin_inset CommandInset citation
LatexCommand cite
key "Zhang:2012:MKS:2225053.2225203"

\end_inset

.
 Deformation of the projected pattern over scene object is analyzed and
 used to triangulate the depth of scene surfaces with respect to the camera's
 optical axis.
 The IR projector operates outside the visible light frequencies and, as
 such, does not interfere with the captured RGB stream pixel values.
 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521736202
Despite the popularity of RGBD sensors, their utility for generic depth
 measurement is limited in several ways due to shortcomings associated with
 structured light depth estimation.
 One significant shortcoming is that RGBD sensors often fail to provide
 meaningful depth values in sunlit outdoor scenes.
 Here the IR radiation from sunlight interferes with the projected pattern
 causing the depth estimation process to fail.
 This phenomenon also occurs in sunlit indoor scenes.
 RGBD sensors also fail to collect measurements from surfaces having specific
 reflectance properties.
 This includes the following three reflectance contexts: (1) 
\begin_inset Quotes eld
\end_inset

dark
\begin_inset Quotes erd
\end_inset

 surfaces, i.e., surfaces having a low reflectance, (2) specular, i.e., mirror-like,
 surfaces and (3) transparent surfaces
\series bold
 
\series default

\begin_inset CommandInset citation
LatexCommand cite
key "8211432"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Kadambi2014"

\end_inset

.
\end_layout

\begin_layout Section

\change_inserted 215191885 1521736202
The Structure from Motion 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521736202
The Structure from Motion (SfM) algorithm leverages ideas originally drawn
 from photogrammetry to estimate the three-dimensional structure of a scene
 from a time series of RGB images from a moving single camera.
 This is achieved by calibrating the camera 
\begin_inset CommandInset citation
LatexCommand cite
key "Zhang:2000:FNT:357014.357025"

\end_inset

 to develop a highly-accurate model to describe how 3D positions are projected
 into camera images.
 Using this image formation model, the SfM algorithm matches together pixels
 in separate images that correspond to projections of the same 3D location
 as the camera moves in the scene.
 Using the camera projection model and the assumption that matched pixels
 are measurements of the same 3D world coordinates, the SfM algorithm solves
 for both the pose of the camera within the global coordinate system and
 the set of 3D surface positions provided by matched image pixels 
\begin_inset CommandInset citation
LatexCommand cite
key "6751290"

\end_inset

.
 The SfM problem is non-linear in the unknowns and is typically solved in
 a two-stage sequence.
 Stage 1 solves for the relative pose of the cameras at the instant the
 images were recorded.
 Stage 2 conditions on the estimated camera pose values and solves for the
 3D scene structure.
 Both stages use correspondences between pixels from different images to
 solve the non-linear equations in the unknown variables.
 The camera pose tracking problem of Stage 1 is often solved by finding
 a map that transforms pixels from the original 
\begin_inset Formula $(x,y)$
\end_inset

 coordinate field to new coordinate positions 
\begin_inset Formula $(x',y')$
\end_inset

 such that both locations correspond to images of the same 3D scene point.
 The multi-view 3D surface reconstruction of Stage 2 is often solved using
 the bundle adjustment algorithm 
\begin_inset CommandInset citation
LatexCommand cite
key "Triggs:1999:BAM:646271.685629"

\end_inset

.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521737049
While scene reconstruction via SfM produces depth images in contexts where
 depth cameras fail, this modality for depth estimation also has several
 shortcomings.
 Specifically, the theoretical formulation of the SfM problem shows that
 the scale of the estimated 3D structure cannot be known without prior or
 outside information.
 This complicates both the mathematical and computational SfM solutions.
 SfM also presumes that viewed surfaces are static, i.e., they do not move,
 and when this assumption is violated reconstructed surfaces are highly
 inaccurate.
\end_layout

\begin_layout Section

\change_inserted 215191885 1521757796
Contribution
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521757798
\begin_inset Float figure
wide true
sideways false
status collapsed

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521757798
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521757798
\begin_inset Graphics
	filename images/teaser_rgb.png
	lyxscale 10
	height 1.3in

\end_inset


\end_layout

\begin_layout Plain Layout

\change_inserted 215191885 1521757798
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1521757798

\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521757798
\begin_inset Graphics
	filename images/teaser_rgbd.png
	lyxscale 10
	height 1.3in

\end_inset


\end_layout

\begin_layout Plain Layout

\change_inserted 215191885 1521757798
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1521757798

\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521757798
\begin_inset Graphics
	filename images/teaser_sfm.png
	lyxscale 10
	height 1.3in

\end_inset


\end_layout

\begin_layout Plain Layout

\change_inserted 215191885 1521757798
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1521757798

\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521757798
\begin_inset Graphics
	filename images/teaser_fused.png
	lyxscale 10
	height 1.3in

\end_inset


\end_layout

\begin_layout Plain Layout

\change_inserted 215191885 1521757798
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1521757798

\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1523985059
\begin_inset Argument 1
status open

\begin_layout Plain Layout

\change_inserted 215191885 1523985273
The depth fusion introduction
\change_unchanged

\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:gussian fussion"

\end_inset

An overview of the proposed RGBD and SfM fusion algorithm (a) shows a grayscale
 image of the scene.
 (b) shows the sensed RGBD depth image.
 (c) shows the SfM-estimated depth image, and (d) shows the fused image.
 The fused image has been color-coded as follows: (white) denotes depth
 locations sensed only by the RGBD sensor, (yellow) denotes depth locations
 only sensed via SfM, (red) denotes fused (RGBD+SfM) depth locations and
 (black) denotes depth locations without RGBD or SfM measurements.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521737050
This article seeks to leverage the strengths of RGBD-derived and SfM-derived
 depth measurements by fusing these measurements into an improved depth
 image that provides depth measurements in contexts where at least one of
 the two depth estimation approaches succeeds.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:gussian fussion"

\end_inset

 shows an RGBD-SfM fusion result for an indoor scene and how the fusion
 result (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:gussian fussion"

\end_inset

(c,d)) captures more scene geometry than either approach independently.
 Our proposed method to fuse RGBD and SfM depth imagery includes consideration
 of the RGBD sensor depth image noise model, the SfM algorithm depth noise
 model and also copes with the inherent unknown scale and scale-drift problems
 intrinsic to SfM.
 To our knowledge these technical issues have not been discussed elsewhere
 in the literature.
\end_layout

\begin_layout Standard

\change_deleted 215191885 1521736635
Structure from Motion (SfM) is 
\change_deleted 1699939148 1515787416
a
\change_deleted 215191885 1521736635
the photogrammetric process of estimating the three-dimensional structure
 of a scene from a set of two-dimensional images, this is achieved by tracking
 the motion of the camera
\change_deleted 1699939148 1515787425
s
\change_deleted 215191885 1521736635
 
\change_deleted 1699939148 1515787428
corresponding
\change_deleted 215191885 1521736635
generating 
\change_deleted 1699939148 1515787431
to 
\change_deleted 215191885 1521736635
these images and correlating information within the time-sequence of images.
 SfM has many applications in the field of robotics including augmented
 reality
\change_deleted 1699939148 1515787468
 and
\change_deleted 215191885 1521736635
, geoscience 
\change_deleted 1699939148 1515787471
etc
\change_deleted 215191885 1521736635
and 
\series bold
\emph on
[put more here]
\series default
\emph default
.
 In 
\change_deleted 1699939148 1515787485
R
\change_deleted 215191885 1521736635
robotics, SfM is 
\change_deleted 1699939148 1515787489
mainly 
\change_deleted 215191885 1521736635
applied 
\change_deleted 1699939148 1515787498
to
\change_deleted 215191885 1521736635
as one approach for
\change_deleted 1699939148 1515787504
 implement the
\change_deleted 215191885 1521736635
 visual odometry, the process of estimating 
\change_deleted 1699939148 1515787514
where 
\change_deleted 215191885 1521736635
the egomotion of an robot 
\change_deleted 1699939148 1515787571
is estimated 
\change_deleted 215191885 1521736635
using only the inputs of cameras attached to it.
 In the field of augmented reality, SfM used to estimate the depth maps
 of the scene, which are later used to implement basic physical interaction
 with the environment [depth map not defined, basic physical interaction
 vague].
 
\end_layout

\begin_layout Standard

\change_deleted 215191885 1521736635
\begin_inset Note Comment
status open

\begin_layout Plain Layout

\change_inserted 1699939148 1515787697
Maybe explain 3D reconstruction via triangulation
\change_unchanged

\end_layout

\end_inset

The core of the SfM involves solving the trigonometry [??? unclear what
 trig?] on set of images from camera with unknown calibration to extract
 the depth of an object [3d reconstruction explanation perhaps].
 The main requirement to do this is to find a correspondence between pair
 of images, traditionally this was done by feature extraction and matching
\begin_inset CommandInset citation
LatexCommand cite
key "klein07parallel"

\end_inset

.
 The feature extraction is the process of selecting the key points in the
 image which are unique and distinguishable and represent them in a efficient
 descriptor,which are later used for matching in the other image.
 There are many possible choices for features and descriptors, like SIFT,
 SURF, ORB etc.
 However in more recent implementations
\begin_inset CommandInset citation
LatexCommand cite
key "Engel2014"

\end_inset

, instead of features, the image intensities are used directly for matching.
 Once the correspondence are established, the Pose of the cameras are estimated
 by imposing the Epipolar constraint, now with the estimated camera pose
 the three-dimensional structure (depth) is computed.
 There mainly two variants of SfM, 
\change_deleted 1699939148 1515787706
I
\change_deleted 215191885 1516181838
incremental and 
\change_deleted 1699939148 1515787709
G
\change_deleted 215191885 1516181838
global
\change_deleted 1699939148 1515787713
 SfM
\change_deleted 215191885 1521736635
.
 Incremental SfM
\begin_inset CommandInset citation
LatexCommand cite
key "Snavely:2006:PTE:1141911.1141964"

\end_inset

 begins by first estimating the 3D structure and camera poses of just two
 cameras based on their relative pose.
 Then additional cameras are added on incrementally and 3D structure is
 refined as new parts of the scene are observed.
 On the other hand 
\change_deleted 1699939148 1515787728
G
\change_deleted 215191885 1521736635
global SfM
\begin_inset CommandInset citation
LatexCommand cite
key "Wilson2014"

\end_inset

 considers the entire problem at once
\change_deleted 1699939148 1515787735
,
\change_deleted 215191885 1521736635
.
 Here the objective is 
\change_deleted 1699939148 1515787745
it tries 
\change_deleted 215191885 1521736635
to estimate the global camera poses and 3D structure by removing outliers
 and by applying an averaging scheme [cite ???].
 [why is this discussed? are their results different? is one of interest
 to you?] Over the years many approaches have be suggested to tackle this
 problem, one of them is the Parallel Tracking and Mapping (PTAM)
\begin_inset CommandInset citation
LatexCommand cite
key "klein07parallel"

\end_inset

, it is a feature based system, here the tracking and mapping are split
 into two separate tasks, processed in parallel threads on dual core computer
 [global or local?].
 The map is represented by a collection of point features located in a world
 coordinate frame 
\change_deleted 1699939148 1515787826
W
\change_deleted 215191885 1516599029

\begin_inset Formula $W$
\end_inset

.
 These points feature represents a locally planar textured patch in the
 world, each point has coordinates in world frame, an unit patch normal
 and a reference to the patch source pixels.
 The map also contains 
\change_deleted 1699939148 1515787809
N
\change_deleted 215191885 1516599029

\begin_inset Formula $N$
\end_inset

 [use math font for variables] key frames, each key frame has an associated
 camera-center coordinate frame and the transformation between the frames
 and also stores a four level pyramid of gray-scale 8bpp (bits-per-pixel)
 images.
 The tracking is a two-stage process done from coarse-to-fine,when the new
 image is acquired, an initial coarse search searches only for 50 map points
 which appear at the highest levels of the current frame’s image pyramid,
 and this search is performed (with sub-pixel refinement) over a large search
 radius.
 A new pose is then calculated from these measurements.
 After this, up to 1000 of the remaining potentially visible image patches
 are re-projected into the image, and now the patch search is performed
 over a far tighter search region.
 Sub-pixel refinement is performed only on a high-level subset of patches.
 The final frame pose is calculated from both coarse and fine sets of image
 measurements together.The pose update is computed iteratively by minimizing
 a robust objective function of the re-projection error:
\end_layout

\begin_layout Standard

\change_deleted 215191885 1516599029
\begin_inset Formula 
\[
\mu^{\prime}=\underset{\mu}{argmin}\underset{j\in s}{\sum}Obj\left(\frac{\left\Vert e_{j}\right\Vert }{\sigma_{j}},\sigma_{T}\right)
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516599029
Where 
\begin_inset Formula $e_{j}$
\end_inset

is the re-projection error vector:
\end_layout

\begin_layout Standard

\change_deleted 215191885 1516599029
\begin_inset Formula 
\[
e_{j}=\left(\begin{array}{c}
\hat{u_{j}}\\
\hat{v_{j}}
\end{array}\right)-CamProj(exp(\mu)E_{CWp_{j}})
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516599029
\begin_inset Formula $Obj(.,\sigma_{T})$
\end_inset

 is the Tukey bi-weight objective function and 
\begin_inset Formula $\sigma_{T}$
\end_inset

 a robust (median-based) estimate of the distribution’s standard deviation
 derived from all the residuals.
 [why are you talking about PTAM? is it related to your thesis? what is
 your research here?]
\end_layout

\begin_layout Standard

\change_deleted 215191885 1516599029
Associated with the the key-frame in the map is a set 
\begin_inset Formula $S_{i}$
\end_inset

 of image measurements..
 For example, the 
\begin_inset Formula $jth$
\end_inset

 map point measured in key-frame 
\begin_inset Formula $i$
\end_inset

 would have been found at 
\begin_inset Formula $(\hat{u}_{ji},\hat{v}_{ji})^{T}$
\end_inset

 with standard deviation of 
\begin_inset Formula $\sigma_{ji}$
\end_inset

 pixels.
 Writing the current state of the map as 
\begin_inset Formula $\left\{ E_{k_{1}W}....E_{k_{N}w}\right\} $
\end_inset

 and 
\begin_inset Formula $\left\{ p_{1}...p_{M}\right\} $
\end_inset

, each image measurement also has an associated re-projection error 
\begin_inset Formula $e_{ji}$
\end_inset

.
 Bundle adjustment is applied to iteratively adjust the map so as to minimize
 the robust objective function:
\end_layout

\begin_layout Standard

\change_deleted 215191885 1521736635
\begin_inset Formula 
\[
\left\{ \left\{ \mu_{2..}\mu_{N}\right\} ,\left\{ p_{1}^{\prime}..p_{M}^{\prime}\right\} \right\} =\underset{\{\{\mu\},\{p\}\}}{argmin}\stackrel[i=1]{N}{\sum}\underset{j\in s}{\sum}Obj\left(\frac{\left\Vert e_{ji}\right\Vert }{\sigma_{ji}},\sigma_{T}\right)
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1521736635
LSD SLAM 
\begin_inset CommandInset citation
LatexCommand cite
key "Engel2014"

\end_inset

 on the other hand is the direct method [you have not defined sparse methods,
 dense methods, direct methods, or indirect methods do this in a review
 of approaches above – similar to that from Direct Sparse Odometry article
 (https://arxiv.org/pdf/1607.02565.pdf) from TUM], this circumvent the drawback
 of PTAM, which is a feature base method and only the information that conforms
 to the feature type can be used.
 LSD LSD SLAM on the other hand optimizes the geometry directly on the image
 intensities, which enables using all information in the image.In addition
 to higher accuracy and robustness in particular in environments with little
 key-points, this provides substantially more information about the geometry
 of the environment.Images are aligned by Gauss -Newton minimization of the
 photometric error.
\end_layout

\begin_layout Standard

\change_deleted 215191885 1521736635
\begin_inset Formula 
\[
E(\xi)=\underset{i}{\sum}(I_{ref}(P_{i})-I(\omega(p_{i},D_{ref}(p_{i}),\xi)))^{2}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1521736635
Where 
\begin_inset Formula $D_{ref}$
\end_inset

 is the estimated depth of reference frame,
\begin_inset Formula $\xi\in se(3)$
\end_inset

 is Lie-algebra representation of rigid body motion and 
\begin_inset Formula $\omega$
\end_inset

 is the affine wrap function.
 The above error function gives the maximum-likelihood estimator for 
\begin_inset Formula $\xi$
\end_inset

 assuming i.i.d.
 Gaussian residuals.
 
\begin_inset Formula $\delta\xi^{(n)}$
\end_inset

 is computed for each iteration by solving for the minimum of Gauss-Newton
 second-order approximation of 
\change_deleted 1699939148 1515788038
E
\change_deleted 215191885 1521736635

\begin_inset Formula $E$
\end_inset

: [no description of image warping needed here, briefly discuss image warping
 for direct methods as a methods for image pixel matching see Baker2004
 http://www.ncorr.com/download/publications/bakerunify.pdf]
\end_layout

\begin_layout Standard

\change_deleted 215191885 1521736635
\begin_inset Formula 
\[
\delta\xi^{(n)}=-(J^{T}J)^{-1}J^{T}r(\xi^{(n)})
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1521736635
with 
\end_layout

\begin_layout Standard

\change_deleted 215191885 1521736635
\begin_inset Formula 
\[
J=\frac{\partial r(\epsilon\circ\delta\xi^{(n)})}{\partial\epsilon}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1521736635
The new estimate is then obtained by multiplication with the computed update
\end_layout

\begin_layout Standard

\change_deleted 215191885 1521736635
\begin_inset Formula 
\[
\xi^{(n+1)}=\delta\xi^{(n)}\circ\xi^{(n)}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1521736635
The overall system is composed of three major components, tracking, depth
 map estimation and map optimization.
\end_layout

\begin_layout Standard

\change_deleted 215191885 1521736635
The tracking component continuously estimates the rigid body pose with respect
 to the current keyframe, using the pose of the previous frame as initialization.
LSD slam tracks new frame by minimizing the variance-normalized photometric
 error
\end_layout

\begin_layout Standard

\change_deleted 215191885 1521736635
\begin_inset Formula 
\[
E_{p}(\xi_{ji})=\underset{p\in\Omega_{D_{i}}}{\sum}\left\Vert \frac{r_{p}^{2}(p,\xi_{ji})}{\sigma_{r_{p}(p,\xi_{ji})}^{2}}\right\Vert _{\delta}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1521736635
with
\begin_inset Formula 
\[
r_{p}(p,\xi_{ji})\coloneqq I_{i}(p)-I_{j}(\omega(p,D_{i}(p),\xi_{ji}))
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1521736635
\begin_inset Formula 
\[
\sigma_{r_{p}(p,\xi_{ji})}^{2}\coloneqq2\sigma_{I}^{2}+\left(\frac{\partial r_{p}(p,\xi_{ji})}{\partial D_{i}(p)}\right)^{2}V_{i}(p)
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1521736635
where 
\begin_inset Formula $\left\Vert .\right\Vert $
\end_inset

 is the Huber norm
\end_layout

\begin_layout Standard

\change_deleted 215191885 1521736635
\begin_inset Formula 
\[
\left\Vert r^{2}\right\Vert _{\delta}\coloneqq\begin{cases}
\frac{r^{2}}{2\delta}if\left\Vert r\right\Vert \leq\delta & ,\left\Vert r\right\Vert -\frac{\delta}{2}\end{cases}otherwise
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1521736635
applied to the normalized residual.
 The depth map estimation component uses tracked frames to either refine
 or replace the current key-frame.
 Depth is refined by filtering over many per-pixel, small-baseline stereo
 comparisons coupled with interleaved spatial regularization.
 If the camera has moved too far, a new key-frame is initialized by projecting
 points from existing, close-by key-frames into it.
 Each key-frame is scaled such that its mean inverse depth is one, which
 enables more small-baseline stereo comparisons.
 For every new key-frame added, the possibility of loop closure is checked
 by performing the reciprocal tracking check.
 The map optimization component is responsible for the updating depth map
 into global map, it detect loop closure and scale drift by estimating similarit
y transform (sim(3)) to close by existing key-frames.The global map is represente
d as a pose graph consisting of key-frames as vertices's with 3D similarity
 transforms as edges, elegantly incorporating changing scale of the environment
 and allowing to detect and correct accumulated drift.
 Each key-frame consists of a camera image, an inverse depth map and variance
 of the inverse depth.Edges between key-frames contain their relative alignment
 as similarity transform, as well as corresponding covariance matrix.
 The map, consisting of a set of key-frames and tracked sim(3)-constraints,
 is continuously optimized in the background using pose graph optimization.
 The error function that is minimized is defined by (W defining the world
 frame) 
\end_layout

\begin_layout Standard

\change_deleted 215191885 1521736635
\begin_inset Formula 
\[
E(\xi_{W1}....\xi_{Wn})\coloneqq\underset{(\xi_{ji},\Sigma_{ji})\in\varepsilon}{\sum}(\xi_{ji}\circ\xi_{Wi}^{-1}\circ\xi_{Wj})\Sigma_{ji}^{-1}(\xi_{ji}\circ\xi_{Wi}^{-1}\circ\xi_{Wj})
\]

\end_inset


\change_inserted 215191885 1521031169

\end_layout

\begin_layout Chapter

\change_inserted 215191885 1521031399
RELATED WORK AND BACKGROUND INFORMATION
\end_layout

\begin_layout Standard

\change_inserted 215191885 1523573186
This article proposes fusion of SfM-estimated depths with depths captured
 from an RGBD image sensor.
 This section is dedicated to discussing the relevant aspects of the SfM
 algorithm and the sensed RGBD measurements necessary to explain the proposed
 fusion method.
 Specifically, this section reviews the theoretical details of the SfM algorithm
, methods for processing depth images including computing depth images for
 arbitrary camera poses, and details existing knowledge regarding the sensor
 measurement noise for RGBD depth measurements.
\end_layout

\begin_layout Standard

\change_deleted 215191885 1521031758
Methodology
\change_unchanged

\end_layout

\begin_layout Standard

\change_deleted 215191885 1521031840
Structure from motions (SfM) is the process of triangulating the three-dimension
al structure from two-dimensional images, along with estimating the motion
 of camera (visual odometer), hence it is some time called as Visual SLAM.

\change_unchanged
 
\change_inserted 215191885 1523294985

\end_layout

\begin_layout Section

\change_inserted 215191885 1523295046
RGBD Camera
\end_layout

\begin_layout Standard

\change_inserted 215191885 1523665701
RGBD cameras are the class of sensors that capture both intensity values
 (RGB values) and depth values for every pixel in the frame.
 The RGB values are measured by a traditional camera with the thin lens
 model, discussed in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Camera-model"

\end_inset

.
 And, depths are measured with the help of infrared ray (IR) projectors
 and IR sensors.
 There are two different approaches of using the IR rays to measure the
 depth.
 The first approach relies on the principles of the structured light 
\begin_inset CommandInset citation
LatexCommand cite
key "Zanuttigh2016_1"

\end_inset

, where the known pattern of infrared light is projected onto the environment,
 and the deformed pattern from the environment interactions are observed
 and triangulated for the depth measurement.
 The second approach uses the concept of Time of Flight (TOF) 
\begin_inset CommandInset citation
LatexCommand cite
key "Zanuttigh2016"

\end_inset

, where the IR rays are continuously emitted into the environment, and the
 time taken for the IR rays to hit the object and travel back, along with
 the change in its phase, are measured to estimate depths.
 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1523573576
For this study, the ORBBEC Astra Pro RGBD sensor was used, and it has the
 following specifications:
\end_layout

\begin_layout Itemize

\change_inserted 215191885 1523300050
a field of view of 60° horiz x 49.5° vert.
 
\end_layout

\begin_layout Itemize

\change_inserted 215191885 1523573589
a range of 0.6 - 8 m.
 
\end_layout

\begin_layout Itemize

\change_inserted 215191885 1523573590
captures RGB images of dimension 640 * 480 @ 30FPS.
 
\end_layout

\begin_layout Itemize

\change_inserted 215191885 1523573590
captures depth images of dimension 640 * 480 @ 30FPS.
 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1523994021
Experimental results reflects this accuracies.
 For which, single channel, 8 bit unsigned integer of intensity images,
 and 32 bit floats of depth images were recorded for various scenarios as
 a Robot Operating System (ROS) bag.
 Each bag was recorded for approximately 30 seconds @ 30FPS, which produces
 a ROS bag of size approximately 1.3 giga-bytes.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1523299834
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1523299834
\begin_inset Graphics
	filename images/rgbd_sensor.png
	lyxscale 25
	width 12cm
	height 6cm

\end_inset


\end_layout

\begin_layout Plain Layout

\change_inserted 215191885 1523299834
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1523299857
ORBBEC Astra RGBD sensor
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection

\change_inserted 215191885 1523335650
RGBD Measurement Noise
\begin_inset CommandInset label
LatexCommand label
name "subsec:RGBD_Measurement-Noise"

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1523335650
The proposed fusion algorithm relies on experimental studies of accuracy
 and noise for RGBD sensor measurement, e.g., the Kinect sensor.
 Research in 
\begin_inset CommandInset citation
LatexCommand cite
key "7925286"

\end_inset

 shows that a Gaussian noise model provides a good fit to observed measurement
 errors on planar targets where the distribution parameters are mean 
\begin_inset Formula $0$
\end_inset

 and standard deviation
\begin_inset Formula $\sigma_{Z}=\frac{m}{2f_{x}b}Z^{2}$
\end_inset

 for depth measurements where 
\begin_inset Formula $\frac{m}{f_{x}b}=-2.85e^{-3}$
\end_inset

 is the linearized slope for the normalized disparity empirically found
 in 
\begin_inset CommandInset citation
LatexCommand cite
key "7925286"

\end_inset

.
 Since 3D the coordinates for 
\begin_inset Formula $(X,Y)$
\end_inset

 are a function of both the pixel location and the depth, their distributions
 are also known as shown below:
\end_layout

\begin_layout Standard

\change_inserted 215191885 1523335650
\begin_inset Formula 
\begin{equation}
\begin{array}{ccc}
\sigma_{X} & = & \frac{x_{im}-o_{x}+\delta_{x}}{f_{x}}\sigma_{Z}=\frac{x_{im}-o_{x}+\delta_{x}}{f_{x}}(1.425e^{-3})Z^{2}\\
\sigma_{Y} & = & \frac{y_{im}-o_{y}+\delta_{y}}{f_{y}}\sigma_{Z}=\frac{y_{im}-o_{y}+\delta_{y}}{f_{y}}(1.425e^{-3})Z^{2}\\
\sigma_{Z} & = & \frac{m}{f_{x}b}Z^{2}\sigma_{d'}=(1.425e^{-3})Z^{2}
\end{array}\label{eq:noise_models}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1524670418
These equations indicate that 3D coordinate measurement uncertainty increases
 as a quadratic function of the depth for all three coordinate values, where
 
\begin_inset Formula $Z$
\end_inset

 denotes the sensed depth at image position 
\begin_inset Formula $(x_{im},y_{im})$
\end_inset

, 
\begin_inset Formula $(f_{x},f_{y})$
\end_inset

 denotes the camera focal length (in pixels), 
\begin_inset Formula $(o_{x},o_{y})$
\end_inset

 denotes the pixel coordinate of the image center, i.e., the principal point,
 and 
\begin_inset Formula $(\delta_{x},\delta_{y})$
\end_inset

 denote adjustments of the projected pixel coordinate to correct for camera
 lens distortion.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1523494878
However, the quadratic coefficient for the 
\begin_inset Formula $(X,Y)$
\end_inset

 coordinate standard deviation is at most half that in the depth direction,
 i.e., 
\begin_inset Formula $(\sigma_{X},\sigma_{Y})\approx0.5\sigma_{Z}$
\end_inset

 at the image periphery where 
\begin_inset Formula $\frac{x-c_{x}}{f}\approx0.5$
\end_inset

, and this value is significantly smaller for pixels close to the optical
 axis.
\end_layout

\begin_layout Subsection

\change_inserted 215191885 1523495159
Limitation of RGBD sensors
\end_layout

\begin_layout Standard

\change_inserted 215191885 1523665886
Despite their advancements over the years, RGBD sensors have limitations.
 These limitations are primarily caused due to the use of infrared rays.
 The main limitation of RGBD sensors is that they cannot be used outdoors
 for depth measurements.
 The IR rays emitted by the Sun overwhelms the IR rays projected by RGBD
 sensors, corrupting the depth measurements, as shown in the figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:RGBD limitatons-a"

\end_inset

.
 For the same reason, the RGBD sensors fail to provide correct depth measurement
s for sunlit indoors.
 Secondly, the material property of an object in the environment affects
 the fidelity of depth measurements.
 For example, the highly specular objects like a mirror or a shiny metal
 reflect IR rays, as shown in the figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:RGBD limitatons-c"

\end_inset

.
 While the refractive surface like glass make them pass through, as shown
 in the figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:RGBD limitatons-b"

\end_inset

.
 And the diffuse black objects observe them.
 All this phenomenon modify the IR rays, resulting in incorrect depth measuremen
ts.
 Finally, RGBD sensors have a hardware limitation; they can measure depths
 for an only limited range.
 For ORBBEC Astra Pro it is 0.6 - 8 m, as shown in the figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:RGBD limitatons-c"

\end_inset

.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1523497149
\begin_inset Float figure
wide true
sideways false
status collapsed

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1523497812
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1523497149
\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "2in"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1523497828
\begin_inset Graphics
	filename images/in_to_out_with_transformation_error/intensity.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1523497149
\begin_inset Graphics
	filename images/in_to_out_with_transformation_error/sensor_measurment.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\change_inserted 215191885 1523497149
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1523499053
\begin_inset CommandInset label
LatexCommand label
name "fig:RGBD limitatons-a"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1523497149
\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "2in"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1523497834
\begin_inset Graphics
	filename images/exp3/intenstiy.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1523497149
\begin_inset Graphics
	filename images/exp3/sensor_depth.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\change_inserted 215191885 1523497149
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1523499064
\begin_inset CommandInset label
LatexCommand label
name "fig:RGBD limitatons-b"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1523497241
\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "2in"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1523497841
\begin_inset Graphics
	filename images/exp2/intensity.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1523497241
\begin_inset Graphics
	filename images/exp2/sensor_measurments.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\change_inserted 215191885 1523497241
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1523499088
\begin_inset CommandInset label
LatexCommand label
name "fig:RGBD limitatons-c"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\change_unchanged

\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1523497149
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1523985297
\begin_inset Argument 1
status open

\begin_layout Plain Layout

\change_inserted 215191885 1523985313
Limitations of RGBD sensors
\change_unchanged

\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:RGBD limitatons"

\end_inset

 (a) shows the RGBD failure for outdoor.
 (b) shows the RGBD failure for a black object and a refractive door.
 (c) shows the RGBD failure for specular surface and its the range limitation.
\end_layout

\end_inset


\end_layout

\end_inset


\change_unchanged

\end_layout

\begin_layout Section

\change_inserted 215191885 1523671980
Structure from Motion
\end_layout

\begin_layout Standard

\change_inserted 215191885 1523575038
The SfM algorithm uses a time sequence of images from a moving camera to
 recover the 3D geometry of objects viewed by the camera.
 While this problem can be solved without a calibrated camera, reconstruction
 accuracy will adversely affected.
 This work assumes that the camera calibration parameters are known, estimated
 via camera calibration see subsection 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Camera-Calibration"

\end_inset

.
 The SfM algorithm can be broken down into two key steps:
\end_layout

\begin_layout Enumerate

\change_inserted 215191885 1521040943
estimation of the camera pose, i.e., position and orientation, at the time
 each image was recorded,
\end_layout

\begin_layout Enumerate

\change_inserted 215191885 1521040943
estimation of the 3D structure of the scene.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1523368693
As previously mentioned, typical SfM systems solve (1) by computing a map
 that associates pixels from the original 
\begin_inset Formula $(x,y)$
\end_inset

 coordinate field to new coordinate positions 
\begin_inset Formula $(x',y')$
\end_inset

 such that both locations correspond to images of the same 3D scene point
 and (2) via multi-view 3D surface reconstruction algorithm, e.g., bundle
 adjustment 
\begin_inset CommandInset citation
LatexCommand cite
key "Triggs:1999:BAM:646271.685629"

\end_inset

.
 In the following sections we provide an overview of aspects of the SfM
 algorithm necessary for the development of the proposed RGBD-SfM depth
 fusion algorithm.
\end_layout

\begin_layout Subsection

\change_inserted 215191885 1523338315
Camera model
\begin_inset CommandInset label
LatexCommand label
name "subsec:Camera-model"

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1524666218
The SfM operates on two-dimensional images, and it is important to understand
 how these images are generated.
 An image is formed by capturing the light energy (irradiance) for every
 pixel.
 This process can be mathematically represented by the thin lens camera
 model, which describes the relationship between the three-dimensional coordinat
es and its projection onto the two-dimensional image plane.
 In this model, the center of the lens is called an optic center, and the
 line passing through the optic center (o) is called an optic axis.
 The plane perpendicular to the optic axis is called the focal plane.
 The thin lens itself is characterized by its focal length (f) and diameter.
 The focal length is the distance from the optic center,  where all the
 ray intersects the optic axis.
 The point of the intersection itself is called the focus of the lens.
 One of the important properties to consider is that the rays entering the
 lens through the optic center are undeflected, while the rays entering
 the lens in all the other places are refracted.
 With this model, the irradiance at each pixel is computed as an integration
 of all the energy emitted from a region of an environment, which is determined
 by all the rays converged at that pixel.
  
\end_layout

\begin_layout Standard

\change_inserted 215191885 1523338315
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1523338315
\begin_inset Graphics
	filename images/thin_lens_Cam.png
	lyxscale 25
	width 8cm
	height 6cm

\end_inset


\end_layout

\begin_layout Plain Layout

\change_inserted 215191885 1523338315
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1523405733
Thin lens camera model
\begin_inset CommandInset label
LatexCommand label
name "fig:Thin-lens-camera"

\end_inset

 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1523405829
The fundamental equation of the thin lens is obtained using similar triangles
 from figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Thin-lens-camera"

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1523338315
\begin_inset Formula 
\[
\frac{1}{Z}+\frac{1}{z}=\frac{1}{f}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1524236189
For the simplification of calculation, we consider an ideal camera model
 called the pinhole camera model.
 In this case, the aperture of a thin lens is assumed to be zero, all rays
 are forced to go through the optical center o; therefore they remain undeflecte
d.
 Consequently, as the aperture decreases to zero, the only points that contribut
e to the irradiance at the image pixel 
\begin_inset Formula $\boldsymbol{x}=\left[x,y\right]$
\end_inset

 are on a line through the center o of the lens.
 If a point p has coordinates 
\begin_inset Formula $\boldsymbol{X}=\left[X,Y,Z\right]$
\end_inset

 relative to a reference frame centered at the optical center o, with its
 z-axis being the optical axis of the lens, then it is immediate from the
 similar triangles in the figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Pin-hole-camera-model-1"

\end_inset

 that the coordinates of p and its image x are related by the so-called
 ideal perspective projection.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1523338315
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1523338315
\begin_inset Graphics
	filename images/pinhole_cam.png
	width 8cm
	height 6cm

\end_inset


\end_layout

\begin_layout Plain Layout

\change_inserted 215191885 1523338315
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1523338315
Pin-hole camera model 
\begin_inset CommandInset label
LatexCommand label
name "fig:Pin-hole-camera-model-1"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1523338315
\begin_inset Formula 
\begin{equation}
x=-f\frac{X}{Z}\label{eq:pinhole camera x-1}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1523338315
\begin_inset Formula 
\begin{equation}
y=-f\frac{Y}{Z}\label{eq:pinhole camera y-1}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1523338315
This mapping of 3D point to 2D is called projection and is represented by
 
\begin_inset Formula $\pi$
\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1523338315
\begin_inset Formula 
\[
\pi:\mathbb{R}^{3}\rightarrow\mathbb{R}^{2}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1523338315
This is also written as 
\begin_inset Formula $\boldsymbol{x}=\pi(\boldsymbol{X})$
\end_inset

.
 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1524236271
The negative sign in the eq 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:pinhole camera x-1"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:pinhole camera y-1"

\end_inset

 makes the object appear upside down on the image plane.
 Since we are working with digital camera, we can handle this by moving
 the image plane to front of the optic center to 
\begin_inset Formula $\{z=-f\}$
\end_inset

, which will make 
\begin_inset Formula $(x,y)\rightarrow(-x,-y).$
\end_inset

 Then the equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:pinhole camera x-1"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:pinhole camera y-1"

\end_inset

 can be updated as 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1523338315
\begin_inset Formula 
\[
x=f\frac{X}{Z}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1523338315
\begin_inset Formula 
\[
y=f\frac{Y}{Z}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1523338315
This can be represented in matrix form as 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1523338315
\begin_inset Formula 
\[
\boldsymbol{x}=\left[\begin{array}{c}
x\\
y
\end{array}\right]=\frac{f}{Z}\left[\begin{array}{c}
X\\
Y
\end{array}\right]
\]

\end_inset

 In homogeneous coordinates, this relationship can be modified as
\end_layout

\begin_layout Standard

\change_inserted 215191885 1523338315
\begin_inset Formula 
\begin{equation}
Z\left[\begin{array}{c}
x\\
y\\
1
\end{array}\right]=\left[\begin{array}{cccc}
f & 0 & 0 & 0\\
0 & f & 0 & 0\\
0 & 0 & 1 & 0
\end{array}\right]\left[\begin{array}{c}
X\\
Y\\
Z\\
1
\end{array}\right]\label{eq:homogennous}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1524236302
The equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:homogennous"

\end_inset

 can be further decomposed into
\begin_inset Formula 
\[
\left[\begin{array}{cccc}
f & 0 & 0 & 0\\
0 & f & 0 & 0\\
0 & 0 & 1 & 0
\end{array}\right]=\left[\begin{array}{ccc}
f & 0 & 0\\
0 & f & 0\\
0 & 0 & 1
\end{array}\right]\left[\begin{array}{cccc}
1 & 0 & 0 & 0\\
0 & 1 & 0 & 0\\
0 & 0 & 1 & 0
\end{array}\right]
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1523338315
with
\end_layout

\begin_layout Standard

\change_inserted 215191885 1523338315
\begin_inset Formula 
\[
\boldsymbol{K}_{f}=\left[\begin{array}{cccc}
f & 0 & 0 & 0\\
0 & f & 0 & 0\\
0 & 0 & 1 & 0
\end{array}\right]\in R^{3\times3},\boldsymbol{\Pi}_{0}=\left[\begin{array}{cccc}
1 & 0 & 0 & 0\\
0 & 1 & 0 & 0\\
0 & 0 & 1 & 0
\end{array}\right]\in\mathbb{R}{}^{3\times4}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1524236311
The matrix 
\begin_inset Formula $\Pi_{0}$
\end_inset

 is a standard projection matrix.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1523666737
With the rigid body representation for camera from the equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:2"

\end_inset

, we can represent the overall geometric model for an ideal camera as:
\end_layout

\begin_layout Standard

\change_inserted 215191885 1523338315
\begin_inset Formula 
\begin{equation}
c\left[\begin{array}{c}
x^{\prime}\\
y^{\prime}\\
1
\end{array}\right]=\left[\begin{array}{ccc}
f & 0 & 0\\
0 & f & 0\\
0 & 0 & 1
\end{array}\right]\left[\begin{array}{cccc}
1 & 0 & 0 & 0\\
0 & 1 & 0 & 0\\
0 & 0 & 1 & 0
\end{array}\right]\left[\begin{array}{cc}
\boldsymbol{R} & \boldsymbol{T}\\
0 & 1
\end{array}\right]\left[\begin{array}{c}
X_{0}\\
Y_{0}\\
Z_{0}\\
1
\end{array}\right]\label{eq:ideal camera model-1}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1523338315
Where 
\begin_inset Formula $c$
\end_inset

 is the unknown scale factor.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1524236413
The equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:ideal camera model-1"

\end_inset

 represents the ideal camera model, where the retinal frame is centered
 at the optical center, and its axis aligned with the optical axis.
 But in practice, this does not true and the origin of the image coordinate
 frame typically in the upper-left corner of the image.
 We need to address this distortion between the retinal plane coordinate
 frame and the pixel array in our camera model.
 This distortion can be corrected by a special matrix :
\end_layout

\begin_layout Standard

\change_inserted 215191885 1523338315
\begin_inset Formula 
\begin{equation}
\boldsymbol{K}_{s}=\left[\begin{array}{ccc}
s_{x} & s_{\theta} & o_{x}\\
0 & s_{y} & o_{y}\\
0 & 0 & 1
\end{array}\right]\in\mathbb{R}^{3\times3}\label{eq:intrensic matrix-1-1}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1523338315

\end_layout

\begin_layout Standard

\change_inserted 215191885 1523338315
\begin_inset Formula 
\begin{equation}
\boldsymbol{K}=\boldsymbol{K}_{s}\boldsymbol{K_{f}}=\left[\begin{array}{ccc}
s_{x} & s_{\theta} & o_{x}\\
0 & s_{y} & o_{y}\\
0 & 0 & 1
\end{array}\right]\left[\begin{array}{ccc}
f & 0 & 0\\
0 & f & 0\\
0 & 0 & 1
\end{array}\right]=\left[\begin{array}{ccc}
fs_{x} & s_{\theta} & o_{x}\\
0 & fs_{y} & o_{y}\\
0 & 0 & 1
\end{array}\right]\label{eq:intresic matrix-1}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1523338315
where
\end_layout

\begin_layout Itemize

\change_inserted 215191885 1523338315
\begin_inset Formula $o_{x}$
\end_inset

: x-coordinate of the principal point in pixels, 
\end_layout

\begin_layout Itemize

\change_inserted 215191885 1523338315
\begin_inset Formula $o_{y}$
\end_inset

: y-coordinate of the principal point in pixels, 
\end_layout

\begin_layout Itemize

\change_inserted 215191885 1523338315
\begin_inset Formula $fs_{x}$
\end_inset

= 
\begin_inset Formula $\alpha_{x}$
\end_inset

 : size of unit length in horizontal pixels also called as focal length
 in x axis, 
\end_layout

\begin_layout Itemize

\change_inserted 215191885 1523338315
\begin_inset Formula $fs_{y}$
\end_inset

= 
\begin_inset Formula $\alpha_{y}$
\end_inset

 : size of unit length in vertical pixels also called as focal length in
 y axis, 
\end_layout

\begin_layout Itemize

\change_inserted 215191885 1523338315
\begin_inset Formula $\frac{\alpha_{x}}{\alpha_{y}}$
\end_inset

: aspect ratio 
\begin_inset Formula $\sigma$
\end_inset

, 
\end_layout

\begin_layout Itemize

\change_inserted 215191885 1523338315
\begin_inset Formula $fs_{\theta}$
\end_inset

: skew of the pixel, often close to zero.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1524236466
Since the parameters in matrix 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:intresic matrix-1"

\end_inset

 are unique to every camera and are not influenced by any external factors,
 they are called an intrinsic parameter, and the matrix itself is called
 an intrinsic matrix 
\begin_inset Formula $K$
\end_inset

.
 With matrix 
\begin_inset Formula $K$
\end_inset

, the ideal camera model from equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:ideal camera model-1"

\end_inset

 can be updated as 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1523338315
\begin_inset Formula 
\begin{equation}
c\left[\begin{array}{c}
x^{\prime}\\
y^{\prime}\\
1
\end{array}\right]=\left[\begin{array}{ccc}
s_{x} & s_{\theta} & o_{x}\\
0 & s_{y} & o_{y}\\
0 & 0 & 1
\end{array}\right]\left[\begin{array}{cccc}
1 & 0 & 0 & 0\\
0 & 1 & 0 & 0\\
0 & 0 & 1 & 0
\end{array}\right]\left[\begin{array}{cc}
\boldsymbol{R} & \boldsymbol{T}\\
0 & 1
\end{array}\right]\left[\begin{array}{c}
X_{0}\\
Y_{0}\\
Z_{0}\\
1
\end{array}\right]\label{eq:camera model with internsic parameter-1}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1523338315
In the matrix notation,
\end_layout

\begin_layout Standard

\change_inserted 215191885 1523338315
\begin_inset Formula 
\begin{equation}
c\boldsymbol{x}=\boldsymbol{K}\boldsymbol{\Pi}_{0}\boldsymbol{\xi}\boldsymbol{X}_{0}\label{eq:camera model with internsic parameter matrix rep-1}
\end{equation}

\end_inset

 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1524236569
To summarize, equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:camera model with internsic parameter matrix rep-1"

\end_inset

 represents the projection of three-dimensional coordinates 
\begin_inset Formula $\boldsymbol{X}_{0}$
\end_inset

 by camera with orientation 
\begin_inset Formula $\xi$
\end_inset

 (also called as extrinsic parameters) and camera calibration K, onto two-dimens
ional coordinate 
\begin_inset Formula $x$
\end_inset

 with known scale 
\begin_inset Formula $\lambda$
\end_inset


\end_layout

\begin_layout Subsection

\change_inserted 215191885 1523338315
Camera Calibration
\begin_inset CommandInset label
LatexCommand label
name "sec:Camera-Calibration"

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1523666842
The camera calibration is the process of determining all intrinsic parameters
 of the camera.
 The images generated by a typical camera usually have a distortions.
 Since, these images are used for the estimation of intrinsic parameters,
 the distortions should be fixed, in order to prevent errors in camera calibrati
on.
 There are mainly of two types of distortions, the radial distortion and
 tangential distortion.
 The radial distortion effects image by curving the string lines, its effect
 is more as we move away from the center of image.
 This distortion can be corrected by Brown's distortion model 
\begin_inset CommandInset citation
LatexCommand cite
key "s16060807"

\end_inset

 as following.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1523338315
\begin_inset Formula 
\[
x_{corrected}=x(1+k_{1}r^{2}+k_{2}r^{4}+k_{3}r^{6})
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1523338315
\begin_inset Formula 
\[
y_{corrected}=y(1+k_{1}r^{2}+k_{2}r^{4}+k_{3}r^{6})
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1524237314
where 
\begin_inset Formula $(x,y)$
\end_inset

 are distorted image intensities and 
\begin_inset Formula $(x_{corrected},y_{corrected})$
\end_inset

 are corrected image intensities, where 
\begin_inset Formula $k_{x}$
\end_inset

 is the distortion coefficient of the camera to be determined, and r is
 the distance of pixel from the principal point.
 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1523666853
Tangential distortion are due to misalignment of lens with imaging plane.
 As a consequence some areas in image may look nearer than expected.
 This distortion can be corrected by The Brown–Conrady 
\begin_inset CommandInset citation
LatexCommand cite
key "s16060807"

\end_inset

 model as following 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1523338315
\begin_inset Formula 
\[
x_{corrected}=x+[2p_{1}xy+p_{2}(r^{2}+2x^{2})]
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1523338315
\begin_inset Formula 
\[
y_{corrected}=y+[p_{1}(r^{2}+2y^{2})+2p_{2}xy]
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1523338315
In total there are 5 parameters, known as distortion coefficients given
 by:
\end_layout

\begin_layout Standard

\change_inserted 215191885 1523338315
\begin_inset Formula 
\begin{equation}
D_{coff}=(k_{1},k_{2},p_{1},p_{2},k_{3})\label{eq:distortion coeff}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1524237372
For any given camera, all the essential parameters from equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:intresic matrix-1"

\end_inset

 and equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:distortion coeff"

\end_inset

 are determined by measuring the difference between the position of key
 features in captured image and their supposed true position.
 For this process, images of known pattern are captured from different orientati
ons.
 Since the estimation of corners are convenient and less prone to error,
 the two dimensional checkerboard pattern are often used for camera calibration
 
\begin_inset CommandInset citation
LatexCommand cite
key "Zhang2014,Zhang:2000:FNT:357014.357025"

\end_inset

.
 There are many standard library implementations of this process and we
 have used the one in OpenCV library 
\begin_inset CommandInset citation
LatexCommand cite
key "5534797"

\end_inset

.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1523338315
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1523338315
\begin_inset Graphics
	filename images/checker_board.png
	width 8cm
	height 6cm

\end_inset


\end_layout

\begin_layout Plain Layout

\change_inserted 215191885 1523338315
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1523985331
Checkerboard for camera calibration
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection

\change_inserted 215191885 1523672004
Estimation of the Camera pose
\begin_inset CommandInset label
LatexCommand label
name "subsec:Estimation-of-the-pose"

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1524237671
The camera pose is represented by the rigid body motion, which is introduced
 in the section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Rigid-Body-Motion"

\end_inset

, these camera poses between pair of images can be estimated by solving
 for their alignment, as discussed in the section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Solving-for-Image"

\end_inset

.
 Further, this process of pose estimation along with the knowledge of depth,
 can be applied to estimate the camera trajectory over the sequence of images,
 which discussed in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Visual-Odometer"

\end_inset

.
\end_layout

\begin_layout Subsubsection

\change_inserted 215191885 1523375611
Image alignment
\begin_inset CommandInset label
LatexCommand label
name "sec:Image-alignment"

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1524237689
The image alignment is the process of transforming (warping) one image with
 respect to another image, with a goal to minimizing the total difference
 between the intensities 
\begin_inset CommandInset citation
LatexCommand cite
key "Baker:2004:LYU:964568.964604"

\end_inset

.
\begin_inset Formula 
\begin{equation}
\hat{\xi}=\min_{\xi}\underset{\mathbf{x}}{\sum}\left(\mathbf{I}(\omega(\mathbf{x},\boldsymbol{\xi}))-\mathbf{I_{ref}}(\mathbf{x})\right)^{2}\label{eq:Direct image alignment}
\end{equation}

\end_inset


\change_unchanged

\end_layout

\begin_layout Standard

\change_inserted 215191885 1524674757
In equation (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Direct image alignment"

\end_inset

), the warp function, 
\begin_inset Formula $\omega(\mathbf{x},\boldsymbol{\xi})$
\end_inset

, which is a rigid body motion, maps pixel locations, 
\begin_inset Formula $\mathbf{x}$
\end_inset

, in the reference image 
\begin_inset Formula $\mathbf{I_{ref}}$
\end_inset

 to pixel locations in the image 
\begin_inset Formula $\mathbf{I}$
\end_inset

 with the current estimate of the transformation parameters 
\begin_inset Formula $\xi$
\end_inset

.
 With correspondence estimation, 
\begin_inset Formula $\xi$
\end_inset

 is a pose transformation of the viewing camera represented as 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:sim3"

\end_inset

.
 We then seek the camera pose transformation parameters, 
\begin_inset Formula $\hat{\xi}$
\end_inset

, that minimize the error in equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Direct image alignment"

\end_inset

, which provides the camera pose change that best explain the differences
 in these two of images of the same scene.
 Given two images and the camera pose change between them, one can take
 the information in one image, and through the warp function map these values
 into the viewpoint of the other image to establish a correspondence.
 In this case the theoretical difference between expected and observed values
 for the image pair is zero if the camera pose change is known exactly and
 sensor noise and other outside influences are ignored.
 The wrap function which achieve this goal can be estimated incremental
 by non linear optimization.
 To preform an image alignment, the correspondence between images should
 be established, and the wrap function 
\begin_inset Formula $\omega$
\end_inset

 should be defined.
\end_layout

\begin_layout Subsubsection

\change_inserted 215191885 1523375611
Solving for Image Pixel Correspondences
\begin_inset CommandInset label
LatexCommand label
name "subsec:Solving-for-Image"

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1523375611
There are generically two different approaches for finding corresponding
 observations of the same 3D surface location in multiple images referred
 to as 
\emph on
direct 
\emph default
and 
\emph on
indirect 
\emph default

\begin_inset CommandInset citation
LatexCommand cite
key "7898369"

\end_inset


\emph on
.
 
\emph default
In this discussion, we refer to the correspondence problem as a source-to-target
 matching problem.
 Let 
\begin_inset Formula $\mathbf{I}_{t}(x,y)$
\end_inset

 denote an image recorded at time 
\begin_inset Formula $t$
\end_inset

 and 
\begin_inset Formula $\mathbf{I}_{t+\Delta t}(x,y)$
\end_inset

 denote a subsequent image measured at time 
\begin_inset Formula $t+\Delta t$
\end_inset

.
 The correspondence problem seeks to find a map that transforms pixels from
 the original 
\begin_inset Formula $(x,y)$
\end_inset

 coordinate field of 
\begin_inset Formula $\mathbf{I}_{t}$
\end_inset

 to new coordinate positions 
\begin_inset Formula $(x',y')$
\end_inset

 in 
\begin_inset Formula $\mathbf{I}_{t+\Delta t}$
\end_inset

 such that 
\begin_inset Formula $\mathbf{I}_{t}(x,y)$
\end_inset

 and 
\begin_inset Formula $\mathbf{I}_{t+\Delta t}(x',y')$
\end_inset

 correspond to images of the same 3D scene point.
 
\end_layout

\begin_layout Paragraph*

\change_inserted 215191885 1523375611
Indirect Methods
\end_layout

\begin_layout Standard

\change_inserted 215191885 1523375611
Indirect methods compute this mapping by detecting special 
\begin_inset Formula $(x,y)$
\end_inset

 locations referred to as features locations with a purpose-built feature
 detection algorithm, e.g., the Harris corner detector 
\begin_inset CommandInset citation
LatexCommand cite
key "Harris88acombined"

\end_inset

.
 A description of the image patch in the vicinity of each detected 
\begin_inset Formula $(x,y)$
\end_inset

 location is computed using some feature descriptor algorithm, e.g., Lowe's
 SIFT descriptor 
\begin_inset CommandInset citation
LatexCommand cite
key "Lowe2004"

\end_inset

.
 Feature descriptors seek to provide a vector of values from the image patch
 data that is invariant to the image variations that occur during camera
 motion.
 These include but are not limited to the following effects: illumination
 variation, affine and/or projective invariance, photometric invariance
 (brightness constancy), and scale invariance.
 Popular feature descriptors often prioritize scale and affine invariance
 as their strengths.
 The invariance property allows for correspondences to be computed by finding
 the mapping from the feature descriptor set calculated from image 
\begin_inset Formula $\mathbf{I}_{t}(x,y)$
\end_inset

 to the feature descriptor set calculated from image 
\begin_inset Formula $\mathbf{I}_{t+\Delta t}(x,y)$
\end_inset

.
\end_layout

\begin_layout Paragraph*

\change_inserted 215191885 1523375611
Direct Methods
\end_layout

\begin_layout Standard

\change_inserted 215191885 1523375611
Direct methods on the other hand typically iteratively solve for a set of
 transformation parameters that best align a pair of images by the minimization
 of pixel-wise errors.
 An image warping function, 
\begin_inset Formula $\omega(\mathbf{x})$
\end_inset

, maps a pixel location, 
\begin_inset Formula $\mathbf{x}=[x,y]^{t}$
\end_inset

, in the original coordinate field to new coordinate positions, 
\begin_inset Formula $\mathbf{x}'$
\end_inset

, such that both locations correspond to images.
 A classical solution to this problem is given by the Lucas-Kanade-Tomassi
 (LKT) camera tracking algorithm 
\begin_inset CommandInset citation
LatexCommand cite
key "Baker:2004:LYU:964568.964604"

\end_inset

.
 
\change_unchanged

\end_layout

\begin_layout Subsubsection
Rigid Body Motion
\change_inserted 215191885 1521044093

\begin_inset CommandInset label
LatexCommand label
name "subsec:Rigid-Body-Motion"

\end_inset


\change_unchanged

\end_layout

\begin_layout Standard

\change_deleted 215191885 1521032805
One of the goals of SfM it to estimate the
\change_inserted 215191885 1521032806
The
\change_unchanged
 
\change_inserted 215191885 1523505095
warp function in 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Direct image alignment"

\end_inset

 is a c
\change_deleted 215191885 1523375680
C
\change_unchanged
amera 
\change_inserted 215191885 1523413615
transformation between a pair of images,
\change_deleted 215191885 1523372190
trajectory
\change_inserted 215191885 1523372195
 
\change_deleted 215191885 1521032811
, which 
\change_inserted 215191885 1523506460
 which 
\change_unchanged
is a rigid body motion
\change_inserted 215191885 1523413625
.
\change_deleted 215191885 1523341834
.

\change_unchanged
 
\change_inserted 215191885 1523505986
W
\change_deleted 215191885 1523341836
W
\change_unchanged
e need an efficient model to represent 
\change_deleted 215191885 1523505970
and compute this
\change_inserted 215191885 1523505971
 these
\change_unchanged
 rigid body motion
\change_inserted 215191885 1523505951
s
\change_unchanged
.
 The camera position is represented by a 
\change_inserted 215191885 1523372230
three dimensional 
\change_deleted 215191885 1523372225
3D
\change_unchanged
 
\change_inserted 215191885 1523372223
v
\change_deleted 215191885 1523372223
V
\change_unchanged
ector in an Euclidean 
\change_deleted 1699939148 1515788126
S
\change_unchanged
space
\change_inserted 215191885 1523506484
 
\begin_inset Formula $R^{3}$
\end_inset

,
\change_deleted 215191885 1523342048
, this camera position is chosen to represent the world frame and specify
 the translation and rotation of the scene relative to that frame.

\change_unchanged
 
\change_deleted 215191885 1523342059
The 
\change_inserted 215191885 1523372232
 and the 
\change_unchanged
rigid body motion
\change_inserted 215191885 1523506118
 of this camera
\change_unchanged
 
\change_deleted 215191885 1523372261
itself
\change_unchanged
 is composed of a rotation and translation.
 
\end_layout

\begin_layout Standard
Traditionally, rotation is represented by a 
\begin_inset Formula $3\times3$
\end_inset

 special orthogonal 
\change_deleted 215191885 1523507724
matrix
\change_inserted 215191885 1523507728
group
\change_unchanged
 called rotational matrix.
 Special Orthogonal matrix SO(3) 
\change_deleted 215191885 1523413785
are
\change_inserted 215191885 1523413786
is
\change_unchanged
 
\change_deleted 215191885 1523507097
the
\change_inserted 215191885 1523507097
a
\change_unchanged
 matrix which satisfy 
\begin_inset Formula $\boldsymbol{R}^{T}\boldsymbol{R}=\boldsymbol{R}\boldsymbol{R}^{T}=\boldsymbol{I}$
\end_inset

 and have a determinant of 
\begin_inset Formula $+1$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
SO(3)=\{\boldsymbol{R}\in\mathbb{R}^{3\times3}\mid\boldsymbol{R}^{T}\boldsymbol{R}=I,det(\boldsymbol{R})=+1\}
\]

\end_inset


\end_layout

\begin_layout Standard
The rotation transformation of 
\change_deleted 215191885 1523506161
the coordinates
\change_inserted 215191885 1523506167
a camera position
\change_unchanged
 
\begin_inset Formula $X_{c}$
\end_inset


\change_deleted 215191885 1523372951
of a point p relative to
\change_inserted 215191885 1523372953
 
\change_deleted 215191885 1523506224
 
\change_inserted 215191885 1523506533
from its local cartesian 
\change_unchanged
frame C
\change_inserted 215191885 1523506367
,
\change_unchanged
 to its 
\change_deleted 215191885 1523506184
coordinates
\change_inserted 215191885 1523506189
position
\change_unchanged
 
\begin_inset Formula $X_{w}$
\end_inset


\change_inserted 215191885 1523506402
,
\change_unchanged
 
\change_deleted 215191885 1523506398
relative to
\change_inserted 215191885 1523506558
in the world
\change_unchanged
 
\change_inserted 215191885 1523506385
cartesian 
\change_unchanged
frame W is 
\change_inserted 215191885 1523506420
represented as
\change_unchanged

\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\boldsymbol{X}_{w}=\boldsymbol{R}_{wc}\boldsymbol{X}_{c}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1523413694
Also, b
\change_inserted 215191885 1523413694
B
\change_unchanged
ecause the rotational matrix 
\change_deleted 215191885 1523413769
are
\change_inserted 215191885 1523413769
is
\change_unchanged
 orthogonal, we have
\change_inserted 215191885 1523506573
 
\change_unchanged

\begin_inset Formula $\boldsymbol{R}^{-1}=\boldsymbol{R}^{T}$
\end_inset

, 
\change_deleted 215191885 1523506582
on this line
\change_inserted 215191885 1523506585
with this,
\change_unchanged
 the inverse transformation of coordinates 
\change_deleted 215191885 1523413778
are
\change_inserted 215191885 1523413778
is
\change_unchanged
 
\change_inserted 215191885 1523506597
achieved by
\change_unchanged

\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\boldsymbol{X}_{c}=\boldsymbol{R}_{wc}^{-1}\boldsymbol{X}_{w}=\boldsymbol{R}_{wc}^{T}\boldsymbol{X}_{w}
\]

\end_inset


\end_layout

\begin_layout Standard
The continuous rotation of a camera is described as a trajectory 
\begin_inset Formula $R(t):t\rightarrow SO(3)$
\end_inset

 in the space
\change_inserted 215191885 1523507143
 of
\change_unchanged
 
\begin_inset Formula $SO(3)$
\end_inset

.

\change_inserted 215191885 1523413813
 
\change_unchanged
When
\change_inserted 215191885 1523507586
 the start time of camera motion is not zero, the rotation of the camera
 from time 
\begin_inset Formula $t_{1}$
\end_inset

 to time 
\begin_inset Formula $t_{2}$
\end_inset

 is denoted by 
\begin_inset Formula $R(t_{2},t_{1})$
\end_inset

.
 When we have more then one camera rotation, their composition is represented
 as
\change_unchanged

\end_layout

\begin_layout Standard

\change_deleted 215191885 1523507455
 the starting time is not t = 0, the relative motion between time 
\begin_inset Formula $t_{2}$
\end_inset

 and time 
\begin_inset Formula $t_{1}$
\end_inset

 will be denoted as 
\begin_inset Formula $R(t_{2},t_{1})$
\end_inset

.
 The composition law of the rotation group implies
\change_unchanged

\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\boldsymbol{R}(t_{2,}t_{0})=\boldsymbol{R}(t_{2,}t_{1})\times\boldsymbol{R}(t_{1,}t_{0}),\vee t_{0}<t_{1}<t_{2}\in\boldsymbol{R}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1523413862
On the other hand t
\change_inserted 215191885 1523413862
T
\change_unchanged
he translation 
\change_inserted 215191885 1523506690
of a camera 
\change_unchanged
is represented by a 
\begin_inset Formula $T\in R^{3}$
\end_inset

,
\change_inserted 215191885 1523507614
 
\change_unchanged

\begin_inset Formula $1\times3$
\end_inset

 vector
\change_inserted 215191885 1523413869
,
\change_unchanged
 which 
\change_deleted 215191885 1523413909
adds
\change_inserted 215191885 1523506712
accounts for
\change_unchanged
 the 
\change_inserted 215191885 1523413926
amount of 
\change_unchanged
translation
\change_inserted 215191885 1523506797
 
\change_deleted 215191885 1523506796
 values
\change_unchanged
 
\change_deleted 215191885 1523413879
in each
\change_inserted 215191885 1523506791
in every
\change_unchanged
 dimension.

\change_inserted 215191885 1523372997
 
\change_unchanged
With this, the complete rigid body motion is represented by 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\boldsymbol{X}_{w}=\boldsymbol{R}_{wc}\boldsymbol{X}_{c}+\boldsymbol{T}_{wc}\label{eq:1}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
However, the above equation is not linear but affine.
 We may convert this to linear by using homogeneous coordinates, where we
 append
\change_inserted 215191885 1523505370
 the value
\change_unchanged
 1 for 
\begin_inset Formula $1\times3$
\end_inset

 vector and make it a 
\begin_inset Formula $1\times4$
\end_inset

 vector,
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\bar{\boldsymbol{X}}=\left[\begin{array}{c}
\boldsymbol{X}\\
1
\end{array}\right]=\left[\begin{array}{c}
X\\
Y\\
Z\\
1
\end{array}\right]\in\mathbb{R}^{4}
\]

\end_inset


\end_layout

\begin_layout Standard
With this new notation for point, we can rewrite the transformation from
 equation 
\change_inserted 215191885 1523414014

\begin_inset CommandInset ref
LatexCommand ref
reference "eq:1"

\end_inset


\change_deleted 215191885 1523413986
6
\change_unchanged
 as 
\change_deleted 215191885 1523505841
following
\change_unchanged

\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\bar{\boldsymbol{X}}_{w}=\left[\begin{array}{c}
\boldsymbol{X}_{w}\\
1
\end{array}\right]=\left[\begin{array}{cc}
\boldsymbol{R}_{wc} & \boldsymbol{T}_{wc}\\
0 & 1
\end{array}\right]\left[\begin{array}{c}
\boldsymbol{X}_{c}\\
1
\end{array}\right]=\bar{g}_{wc}\bar{\boldsymbol{X}}_{c}\label{eq:2}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where
\change_inserted 215191885 1523414025
,
\change_unchanged
 the 
\begin_inset Formula $4\times4$
\end_inset

 matrix 
\begin_inset Formula $\bar{g}_{wc}\in R^{4\times4}$
\end_inset


\change_inserted 215191885 1523505402
 
\change_unchanged
is called the homogeneous representation of the rigid-body motion.
\end_layout

\begin_layout Standard

\change_deleted 215191885 1523505608
The
\change_inserted 215191885 1523505648
The
\change_unchanged
 set of all possible 
\change_deleted 215191885 1523505612
configurations
\change_inserted 215191885 1523505612
rigid body transformation
\change_unchanged
 
\change_deleted 215191885 1523505612
of
\change_unchanged
 
\change_deleted 215191885 1523507665
a
\change_unchanged
 
\change_deleted 215191885 1523505612
rigid body
\change_inserted 215191885 1523507670
 involving a rotation and a translation
\change_unchanged
 can 
\change_inserted 215191885 1523505568
be 
\change_deleted 215191885 1523505612
then
\change_inserted 215191885 1523505576
represented by a
\change_deleted 215191885 1523505612
 be described by the space of rigid-body motions or 
\change_inserted 215191885 1523414047
 
\change_unchanged
special Euclidean 
\change_deleted 215191885 1523507683
transformations
\change_inserted 215191885 1523507688
grorp
\change_unchanged
 called SE(3)
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
SE(3)=\{\boldsymbol{\xi}=\left[\begin{array}{cc}
\boldsymbol{R} & \boldsymbol{T}\\
0 & 1
\end{array}\right]\mid\boldsymbol{R}\in SO(3),\boldsymbol{T}\in\mathbb{R}^{3}\}\subset\mathbb{R}^{4\times4}\label{eq:se3}
\end{equation}

\end_inset


\change_inserted 215191885 1516636715

\end_layout

\begin_layout Standard

\change_inserted 215191885 1523505652
This representation can extend to include an variable for uniform scaling
 in all the dimensions.
 With 
\begin_inset Formula $\alpha$
\end_inset

 representing the scalar value for uniform scaling, the transformation from
 equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:2"

\end_inset

 is updated as fallowing
\end_layout

\begin_layout Standard

\change_inserted 215191885 1523373354
\begin_inset Formula 
\begin{equation}
\bar{\boldsymbol{X}}_{w}=\alpha\boldsymbol{\xi}_{wc}\bar{\boldsymbol{X}}_{c}\label{eq:uniform scalling}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1523505720
Similar to 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:se3"

\end_inset

 the set of possible configurations of a rigid body with uniform scaling
 is represented by a Similarity transformation 
\begin_inset Formula $Sim(3)$
\end_inset

, which is the composition of a rotation, translation and a uniform scale,
 and hence this representation has 7 degree of freedom, 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1516637515
\begin_inset Formula 
\begin{equation}
Sim(3)=\{\bar{g}=\left[\begin{array}{cc}
\boldsymbol{R} & \boldsymbol{T}\\
0 & \alpha^{-1}
\end{array}\right]\mid\boldsymbol{R}\in SO(3),\boldsymbol{T}\in\mathbb{R}^{3},\alpha\in\mathbb{R}\}\subset\mathbb{R}^{4\times4}\label{eq:sim3}
\end{equation}

\end_inset


\change_unchanged

\end_layout

\begin_layout Subsubsection
Exponential Map
\end_layout

\begin_layout Standard
The special orthogonal group 
\change_deleted 215191885 1523415270
in
\change_inserted 215191885 1523415271
for a
\change_unchanged
 three dimension
\change_inserted 215191885 1523415290
al transformation 
\change_deleted 215191885 1523415284
s
\change_unchanged
 
\change_deleted 215191885 1523374325
can be
\change_unchanged
 
\change_inserted 215191885 1523374327
is 
\change_unchanged
represented by a 
\begin_inset Formula $3\times3$
\end_inset

 rotation matrix 
\begin_inset Formula $R\in SO(3)$
\end_inset

, 
\change_deleted 215191885 1523374195
which must satisfy
\change_inserted 215191885 1523374198
 with
\change_unchanged
 the constraint 
\begin_inset Formula $R^{T}R=I$
\end_inset

, this
\change_inserted 215191885 1523374520
 constraint
\change_unchanged
 implies that the 
\begin_inset Formula $SO(3)$
\end_inset

 transformations 
\change_inserted 215191885 1523374404
does not affect the volume of the rigid body, i.e the value of the quantity
 
\begin_inset Formula $x^{2}+y^{2}+z^{2}$
\end_inset

 is not changed by the rotation transformation.
 
\change_deleted 215191885 1523374308
leaves the quantity 
\begin_inset Formula $x^{2}+y^{2}+z^{2}$
\end_inset

 invariant
\change_unchanged
.
 The group 
\begin_inset Formula $SO(3)$
\end_inset

 has 
\change_deleted 215191885 1523507849
9
\change_inserted 215191885 1523507850
nine
\change_unchanged
 parameters, but the invariance of the 
\change_deleted 215191885 1523374416
length
\change_inserted 215191885 1523374421
volume
\change_unchanged
 
\change_deleted 215191885 1523374712
produces
\change_inserted 215191885 1523375038
implies
\change_deleted 215191885 1523374712
 
\change_inserted 215191885 1523507858
 
\change_unchanged
six independent 
\change_deleted 215191885 1523374821
conditions
\change_inserted 215191885 1523374823
constraints
\change_unchanged
, 
\change_deleted 215191885 1523374429
leaving
\change_inserted 215191885 1523375058
making only
\change_unchanged
 three free parameters
\change_inserted 215191885 1523415365
.
\change_deleted 215191885 1523415365
,
\change_unchanged
 
\change_inserted 215191885 1523415399
It is obvious from this intuition, that the six out of nine parameters in
 
\begin_inset Formula $SO(3)$
\end_inset

 representation are redundant, and we can have better a representation for
 rigid body motion.
 
\change_deleted 215191885 1523374841
Hence, the dimension of the space of rotation matrices 
\begin_inset Formula $SO(3)$
\end_inset

 should be only three, and six parameters out of the nine are in fact redundant.W
e can use this to have better representation of Rigid body motion.
\change_inserted 215191885 1523374992

\end_layout

\begin_layout Standard

\change_inserted 215191885 1516618836
The 
\begin_inset Formula $SO(3)$
\end_inset

, 
\begin_inset Formula $SE(3)$
\end_inset

 and 
\begin_inset Formula $SIM(3)$
\end_inset

are categorized under the special group called the Lie group, which represents
 the smooth differentiable manifolds.
 Every Lie group has a tangent space at identity called the Lie algebra,
 which is a vector space used to study the infinitesimal transformations.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1516618136
For rotation group 
\begin_inset Formula $SO(3)$
\end_inset

 its lie algebra 
\begin_inset Formula $so(3)$
\end_inset

 is represented by
\end_layout

\begin_layout Standard

\change_inserted 215191885 1516619360
\begin_inset Formula 
\[
so(3)\doteq\left\{ \hat{\boldsymbol{\omega}}\in\mathbb{R}^{3\times3}\mid\boldsymbol{\omega}\in\mathbb{R}^{3}\right\} 
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1523507901
where 
\begin_inset Formula $\hat{\omega}$
\end_inset

 is a skew symmetric matrix representation for the vector 
\begin_inset Formula $\omega$
\end_inset

 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1516618683
The map from the space 
\begin_inset Formula $so(3)$
\end_inset

 to 
\begin_inset Formula $SO(3)$
\end_inset

 is called the exponential map.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1516618685
\begin_inset Formula 
\[
exp:so(3)\rightarrow SO(3);\hat{\boldsymbol{\omega}}\mapsto e^{\hat{\boldsymbol{\omega}}}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1516618599
\begin_inset Formula 
\begin{equation}
R(t)=e^{\hat{\boldsymbol{\omega}}t}\label{exp map for so(3)}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1516619295
The equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "exp map for so(3)"

\end_inset

 represents a rotation around the axis 
\begin_inset Formula $\omega\in\mathbb{R}^{3}$
\end_inset

 by an angle of 
\begin_inset Formula $t$
\end_inset

 radians.
 And the inverse mapping is obtained by logarithm of SO(3)
\end_layout

\begin_layout Standard

\change_inserted 215191885 1516618286
\begin_inset Formula 
\begin{equation}
log:SO(3)\rightarrow so(3);log(\boldsymbol{R})\mapsto\hat{\boldsymbol{\omega}}\label{eq:inv_SO}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1516618390
We can extend this to full rigid body motion which also involves the translation
 along with the rotation, for rotation group 
\begin_inset Formula $SE(3)$
\end_inset

 its lie algebra 
\begin_inset Formula $se(3)$
\end_inset

 is represented by .
\end_layout

\begin_layout Standard

\change_inserted 215191885 1516618391
\begin_inset Formula 
\[
se(3)\doteq\left\{ \hat{\boldsymbol{\xi}}=\left[\begin{array}{cc}
\hat{\boldsymbol{\omega}} & \boldsymbol{\upsilon}\\
0 & 0
\end{array}\right]\mid\hat{\boldsymbol{\omega}}\in so(3),\boldsymbol{\upsilon}\in\mathbb{R}^{3}\right\} \subset\mathbb{R}^{4\times4}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1516618477
with 
\begin_inset Formula $\boldsymbol{\upsilon}(t)=\dot{T}(t)-\hat{\boldsymbol{\omega}}(t)T(t)\in\mathbb{R}^{3}$
\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1516618546
Similarly, the exponential map from the space 
\begin_inset Formula $se(3)$
\end_inset

 to 
\begin_inset Formula $SE(3)$
\end_inset

is given by
\end_layout

\begin_layout Standard

\change_inserted 215191885 1516618478
\begin_inset Formula 
\[
exp:se(3)\rightarrow SE(3);\hat{\boldsymbol{\xi}}\mapsto e^{\hat{\boldsymbol{\xi}}}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1523504808
Similar to 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:inv_SO"

\end_inset

, the inverse to the exponential map is defined by logarithm 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1516639609
\begin_inset Formula 
\[
log:SE(3)\rightarrow se(3);log(g)\mapsto\hat{\xi}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618781
We know that the continuous rotational motion represented by 
\begin_inset Formula $R(t):R\in SO(3)$
\end_inset

, must satisfy the following constraint
\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618781
\begin_inset Formula 
\[
R(t)R^{T}(t)=I
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618781
Differentiating the above equation with respect to time t gives
\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618781
\begin_inset Formula 
\[
\dot{R}(t)R^{T}(t)+R(t)\dot{R}^{T}(t)=0
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618781
\begin_inset Formula 
\[
\dot{R}(t)R^{T}(t)=-(\dot{R}(t)R^{T}(t))^{T}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618781
This shows that the matrix 
\begin_inset Formula $\dot{R}(t)R^{T}(t)\in\mathbb{R}^{3\times3}$
\end_inset

is a skew-symmetric matrix.This implies that there must exist a vector, say
 
\begin_inset Formula $\omega(t)\in\mathbb{R}^{3}$
\end_inset

,such that
\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618781
\begin_inset Formula 
\begin{equation}
\dot{R}(t)R^{T}(t)=\hat{\omega}(t)\label{eq:3-1}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618781
Multiplying both sides by 
\begin_inset Formula $R(t)$
\end_inset

 on the right yields
\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618781
\begin_inset Formula 
\begin{equation}
\dot{R}(t)=\hat{\omega}(t)R(t)\label{eq:3}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618781
In above equation, if 
\begin_inset Formula $R(t_{0})=I$
\end_inset

 for for 
\begin_inset Formula $t=t_{0}$
\end_inset

, we have 
\begin_inset Formula $\dot{R}(t_{0})=\hat{\omega}(t_{0})$
\end_inset

.
 Hence, around the identity matrix I, a skew-symmetric matrix gives a first-
 order approximation to a rotation matrix:
\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618781
\begin_inset Formula 
\[
R(t_{0}+dt)\thickapprox I+\hat{\omega}(t_{0})dt
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618781
This space of all skew-symmetric matrix represents the tangent space of
 the rotation group 
\begin_inset Formula $SO(3)$
\end_inset

 and it is the lie algebra 
\begin_inset Formula $so(3)$
\end_inset

 of the corresponding lie group.
\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618781
\begin_inset Formula 
\[
so(3)\doteq\left\{ \hat{\omega}\in\mathbb{R}^{3\times3}\mid\omega\in\mathbb{R}^{3}\right\} 
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618781
Once we have so(3), we need a way to map SO(3) to so(3).
 
\change_deleted 1699939148 1515788165
,
\change_deleted 215191885 1516618781
It is obvious that the solution for 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:3"

\end_inset

 is the matrix exponential 
\begin_inset Formula $e^{\hat{\omega}t}$
\end_inset

, where
\change_inserted 1699939148 1515788319
 
\change_deleted 215191885 1521776300
[THIS IS TOO SIMILAR to source work elsewhere, bordering on plagiarism –
 REMOVE!, you should not be going into this much detail for the differential
 properties of rotations as this is not something you are comfortable with,
 write down the most important relationships, i.e., the differential of the
 rotation and why you need it for SfM, you will want to define sim3 transforms
 and how they are distinct from se3 transforms then leave this topic]
\change_unchanged

\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
\begin_inset Formula 
\[
e^{\hat{\omega}t}=I+\hat{\omega}t+\frac{(\hat{\omega}t)^{2}}{2!}+\cdots+\frac{(\hat{\omega}t)^{n}}{n!}+\cdots
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
Hence, we have 
\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
\begin_inset Formula 
\begin{equation}
R(t)=e^{\hat{\omega}t}\label{eq:4}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
The above equation represents a rotation around the axis 
\begin_inset Formula $\omega\in\mathbb{R}^{3}$
\end_inset

 by an angle of 
\begin_inset Formula $t$
\end_inset

 radians.This map from the space 
\begin_inset Formula $so(3)$
\end_inset

 to 
\begin_inset Formula $SO(3)$
\end_inset

 is called the exponential map.
\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
\begin_inset Formula 
\[
exp:so(3)\rightarrow SO(3);\hat{\omega}\mapsto e^{\hat{\omega}}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
And the inverse mapping is obtained by logarithm of SO(3)
\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
\begin_inset Formula 
\[
log:SO(3)\rightarrow so(3);log(R)\mapsto\hat{\omega}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
We can extend this to full rigid body motion which also involves the translation
 along with the rotation.From 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:2"

\end_inset

, the continuous rigid body trajectory on SE(3) is given by
\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
\begin_inset Formula 
\[
g(t)=\left[\begin{array}{cc}
R(t) & T(t)\\
0 & 1
\end{array}\right]\in\mathbb{R}^{4\times4}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
With above representation, we have
\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
\begin_inset Formula 
\[
\dot{g}(t)g^{-1}(t)=\left[\begin{array}{cc}
\dot{R}(t)R^{T}(t) & \dot{T}(t)-\dot{R}(t)R^{T}(t)T(t)\\
0 & 0
\end{array}\right]\in\mathbb{R}^{4\times4}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
with 
\begin_inset Formula $\upsilon(t)=\dot{T}(t)-\hat{\omega}(t)T(t)\in\mathbb{R}^{3}$
\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:3-1"

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
\begin_inset Formula 
\begin{equation}
\hat{\xi(t)}=\left[\begin{array}{cc}
\hat{\omega}(t) & \upsilon(t)\\
0 & 0
\end{array}\right]\in\mathbb{R}^{4\times4}\label{eq:6}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
then we have
\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
\begin_inset Formula 
\[
\dot{g}(t)=(\dot{g}(t)g^{-1}(t))g(t)=\hat{\xi(t)}g(t)
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
where 
\begin_inset Formula $\hat{\xi}$
\end_inset

 is called the twist and can be used to approximate g(t) locally
\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
\begin_inset Formula 
\[
g(t+dt)\approx g(t)+\hat{\xi(t)}g(t)d(t)=(I+\hat{\xi(t)}dt)g(t)
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
Also the twist represent the tangent space (or Lie algebra) of the matrix
 group SE(3).
\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
\begin_inset Formula 
\[
se(3)\doteq\left\{ \hat{\xi}=\left[\begin{array}{cc}
\hat{\omega} & \upsilon\\
0 & 0
\end{array}\right]\mid\hat{\omega}\in so(3),\upsilon\in\mathbb{R}^{3}\right\} \subset\mathbb{R}^{4\times4}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
Similar 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:4"

\end_inset

.
 with the initial condition of 
\begin_inset Formula $g(0)=1,$
\end_inset

we have
\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
\begin_inset Formula 
\[
g(t)=e^{\hat{\xi}t}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
where
\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
\begin_inset Formula 
\[
e^{\hat{\xi}t}=I+\hat{\xi}t+\frac{(\hat{\xi}t)^{2}}{2!}+\cdots+\frac{(\hat{\xi}t)^{n}}{n!}+\cdots
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
This defines the the exponential map from the space 
\begin_inset Formula $se(3)$
\end_inset

 to 
\begin_inset Formula $SE(3)$
\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
\begin_inset Formula 
\[
exp:se(3)\rightarrow SE(3);\hat{\xi}\mapsto e^{\hat{\xi}}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
and as before inverse to the exponential map is defined by logarithm 
\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
\begin_inset Formula 
\[
log:SE(3)\rightarrow se(3);log(g)\mapsto\hat{\xi}
\]

\end_inset


\change_inserted 215191885 1523371695

\end_layout

\begin_layout Subsubsection*

\change_deleted 215191885 1523338302
Camera model
\end_layout

\begin_layout Standard

\change_deleted 215191885 1523338302
The 2D image is formed by capturing the light energy (irradiance) for every
 pixel, this process can be mathematically represented by thin lens camera
 model, which describes the relationship between the three-dimensional coordinat
ors to its projection onto the image plane.
 The thin lens model is represented by a optical axis and a perpendicular
 plane called the focal plane.The thin lens itself is characterized by its
 focal length and diameter, the focal length is the distance from optic
 center to where all the ray intersect the optic axis, while the point of
 intersection itself is called the focus of the lens.
 One of the important properties to consider is that the rays entering the
 lens through optic center are undeflected while the rest of the rays are.
 With this model the irradiance on the image plane is obtained by the integratio
n of all the energy emitted from region of space contained in the cone determine
d by the geometry of the lens.
 
\end_layout

\begin_layout Standard

\change_deleted 1699939148 1515788341
\begin_inset Graphics
	filename images/thin_lens_Cam.png
	width 8cm
	height 6cm

\end_inset


\change_deleted 215191885 1523338302

\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 1699939148 1515788341
\begin_inset Graphics
	filename images/thin_lens_Cam.png
	lyxscale 25
	width 8cm
	height 6cm

\end_inset


\change_unchanged

\end_layout

\begin_layout Plain Layout

\change_inserted 1699939148 1515788341
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1516609076
Thin lens camera model 
\change_deleted 215191885 1516609014
Put caption here explain variables and what I'm looking at
\change_unchanged

\end_layout

\end_inset


\change_unchanged

\end_layout

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1523338302
Using similar triangles, from above figure, we obtain the following fundamental
 equation of the thin lens
\end_layout

\begin_layout Standard

\change_deleted 215191885 1523338302
\begin_inset Formula 
\[
\frac{1}{Z}+\frac{1}{z}=\frac{1}{f}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1523338302
For the simplification of calculation, we consider a Ideal camera model
 called the pinhole camera model, here the aperture of a thin lens is assumed
 to decreased to zero, all rays are forced to go through the optical center
 o, and therefore they remain undeflected.Consequently,as the aperture of
 the cone decreases to zero, the only points that contribute to the irradiance
 at the image point 
\begin_inset Formula $x=\left[x,y\right]$
\end_inset

 are on a line through the center 0 of the lens.
 If a point p has coordinates 
\begin_inset Formula $X=\left[X,Y,Z\right]$
\end_inset

 relative to a reference frame centered at the optical center 0, with its
 z-axis being the optical axis (of the lens), then it is immediate to see
 from similar triangles in Figure that the coordinates of p and its image
 x are related by the so-called ideal perspective projection.
\end_layout

\begin_layout Standard

\change_deleted 1699939148 1515788385
\begin_inset Graphics
	filename images/pinhole_cam.png
	width 8cm
	height 6cm

\end_inset


\change_deleted 215191885 1523338302

\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 1699939148 1515788385
\begin_inset Graphics
	filename images/pinhole_cam.png
	width 8cm
	height 6cm

\end_inset


\change_unchanged

\end_layout

\begin_layout Plain Layout

\change_inserted 1699939148 1515788385
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1516609093
Pin-hole camera model 
\change_deleted 215191885 1516609091
Caption here
\change_inserted 215191885 1523325114

\begin_inset CommandInset label
LatexCommand label
name "fig:Pin-hole-camera-model"

\end_inset


\change_unchanged

\end_layout

\end_inset


\change_unchanged

\end_layout

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1523338302
\begin_inset Formula 
\begin{equation}
x=-f\frac{X}{Z}\label{eq:pinhole camera x}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1523338302
\begin_inset Formula 
\begin{equation}
y=-f\frac{Y}{Z}\label{eq:pinhole camera y}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1523338302
[write projection as a matrix transform similar to ROS]This mapping of 3D
 point to 2D is called projection and is represented by 
\begin_inset Formula $\pi$
\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1523338302
\begin_inset Formula 
\[
\pi:R^{3}\rightarrow R^{2}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1523338302
This is also written as 
\begin_inset Formula $x=\pi(X)$
\end_inset

.
 
\end_layout

\begin_layout Standard

\change_deleted 215191885 1523338302
The negative sign in the 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:pinhole camera x"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:pinhole camera y"

\end_inset

 makes the object appear upside down on the image plan, we can handle this
 by moving the image plane to front of optic center to 
\begin_inset Formula $\{z=-f\}$
\end_inset

 this will make 
\begin_inset Formula $(x,y)\rightarrow(-x,-y).$
\end_inset

There for the equation 2 and 3 changes to 
\end_layout

\begin_layout Standard

\change_deleted 215191885 1523338302
\begin_inset Formula 
\[
x=f\frac{X}{Z}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1523338302
\begin_inset Formula 
\[
y=f\frac{Y}{Z}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1523338302
This can be represented in matrix form as 
\end_layout

\begin_layout Standard

\change_deleted 215191885 1523338302
\begin_inset Formula 
\[
x=\left[\begin{array}{c}
x\\
y
\end{array}\right]=\frac{f}{Z}\left[\begin{array}{c}
X\\
Y
\end{array}\right]
\]

\end_inset

 In homogeneous coordinates, this relationship can be modified as
\end_layout

\begin_layout Standard

\change_deleted 215191885 1523338302
\begin_inset Formula 
\[
Z\left[\begin{array}{c}
x\\
y\\
1
\end{array}\right]=\left[\begin{array}{cccc}
f & 0 & 0 & 0\\
0 & f & 0 & 0\\
0 & 0 & 1 & 0
\end{array}\right]\left[\begin{array}{c}
X\\
Y\\
Z\\
1
\end{array}\right]
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1523338302
The above equation can be decomposed into
\begin_inset Formula 
\[
\left[\begin{array}{cccc}
f & 0 & 0 & 0\\
0 & f & 0 & 0\\
0 & 0 & 1 & 0
\end{array}\right]=\left[\begin{array}{ccc}
f & 0 & 0\\
0 & f & 0\\
0 & 0 & 1
\end{array}\right]\left[\begin{array}{cccc}
1 & 0 & 0 & 0\\
0 & 1 & 0 & 0\\
0 & 0 & 1 & 0
\end{array}\right]
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1523338302
with
\end_layout

\begin_layout Standard

\change_deleted 215191885 1523338302
\begin_inset Formula 
\[
K_{f}=\left[\begin{array}{cccc}
f & 0 & 0 & 0\\
0 & f & 0 & 0\\
0 & 0 & 1 & 0
\end{array}\right]\in R^{3\times3},\Pi_{0}=\left[\begin{array}{cccc}
1 & 0 & 0 & 0\\
0 & 1 & 0 & 0\\
0 & 0 & 1 & 0
\end{array}\right]\in\mathbb{R}{}^{3\times4}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1523338302
The matrix 
\begin_inset Formula $\Pi_{0}$
\end_inset

is a standard projection matrix.
\end_layout

\begin_layout Standard

\change_deleted 215191885 1523338302
With the rigid body transformation from 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:2"

\end_inset

, we can represent the overall geometric model for an ideal camera as below
\end_layout

\begin_layout Standard

\change_deleted 215191885 1523338302
\begin_inset Formula 
\begin{equation}
\lambda\left[\begin{array}{c}
x^{\prime}\\
y^{\prime}\\
1
\end{array}\right]=\left[\begin{array}{ccc}
f & 0 & 0\\
0 & f & 0\\
0 & 0 & 1
\end{array}\right]\left[\begin{array}{cccc}
1 & 0 & 0 & 0\\
0 & 1 & 0 & 0\\
0 & 0 & 1 & 0
\end{array}\right]\left[\begin{array}{cc}
R & T\\
0 & 1
\end{array}\right]\left[\begin{array}{c}
X_{0}\\
Y_{0}\\
Z_{0}\\
1
\end{array}\right]\label{eq:ideal camera model}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1523338302
Where 
\begin_inset Formula $\lambda$
\end_inset

 is the unknown scale factor.
\end_layout

\begin_layout Standard

\change_deleted 215191885 1523338302
However, the above equation represents the ideal model where the retinal
 frame centered at the optical center with one axis aligned with the optical
 axis.But in practice, this does not true and the origin of the image coordinate
 frame typically in the upper-left corner of the image.we need to address
 this relationship between the retinal plane coordinate frame and the pixel
 array in our camera model.
 This can be represented by a special matrix as following
\end_layout

\begin_layout Standard

\change_deleted 215191885 1523338302
\begin_inset Formula 
\begin{equation}
K_{s}=\left[\begin{array}{ccc}
s_{x} & s_{\theta} & o_{x}\\
0 & s_{y} & o_{y}\\
0 & 0 & 1
\end{array}\right]\in\mathbb{R}^{3\times3}\label{eq:intrensic matrix-1}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1523338302
with these new parameters the we can represent the interstice parameters
 of camera as following
\end_layout

\begin_layout Standard

\change_deleted 215191885 1523338302
\begin_inset Formula 
\begin{equation}
K=K_{s}K_{f}=\left[\begin{array}{ccc}
s_{x} & s_{\theta} & o_{x}\\
0 & s_{y} & o_{y}\\
0 & 0 & 1
\end{array}\right]\left[\begin{array}{ccc}
f & 0 & 0\\
0 & f & 0\\
0 & 0 & 1
\end{array}\right]=\left[\begin{array}{ccc}
fs_{x} & s_{\theta} & o_{x}\\
0 & fs_{y} & o_{y}\\
0 & 0 & 1
\end{array}\right]\label{eq:intresic matrix}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1523338302
where
\end_layout

\begin_layout Standard

\change_deleted 215191885 1523338302
\begin_inset Formula $o_{x}$
\end_inset

: x-coordinate of the principal point in pixels, 
\end_layout

\begin_layout Standard

\change_deleted 215191885 1523338302
\begin_inset Formula $o_{y}$
\end_inset

: y-coordinate of the principal point in pixels, 
\end_layout

\begin_layout Standard

\change_deleted 215191885 1523338302
\begin_inset Formula $fs_{x}$
\end_inset

= 
\begin_inset Formula $\alpha_{x}$
\end_inset

 : size of unit length in horizontal pixels, 
\end_layout

\begin_layout Standard

\change_deleted 215191885 1523338302
\begin_inset Formula $fs_{y}$
\end_inset

= 
\begin_inset Formula $\alpha_{y}$
\end_inset

 : size of unit length in vertical pixels, 
\end_layout

\begin_layout Standard

\change_deleted 215191885 1523338302
\begin_inset Formula $\frac{\alpha_{x}}{\alpha_{y}}$
\end_inset

: aspect ratio 
\begin_inset Formula $\sigma$
\end_inset

, 
\end_layout

\begin_layout Standard

\change_deleted 215191885 1523338302
\begin_inset Formula $fs_{\theta}$
\end_inset

: skew of the pixel, often close to zero.Now, the ideal camera model 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:ideal camera model"

\end_inset

can be updated as 
\end_layout

\begin_layout Standard

\change_deleted 215191885 1523338302
\begin_inset Formula 
\begin{equation}
\lambda\left[\begin{array}{c}
x^{\prime}\\
y^{\prime}\\
1
\end{array}\right]=\left[\begin{array}{ccc}
s_{x} & s_{\theta} & o_{x}\\
0 & s_{y} & o_{y}\\
0 & 0 & 1
\end{array}\right]\left[\begin{array}{cccc}
1 & 0 & 0 & 0\\
0 & 1 & 0 & 0\\
0 & 0 & 1 & 0
\end{array}\right]\left[\begin{array}{cc}
R & T\\
0 & 1
\end{array}\right]\left[\begin{array}{c}
X_{0}\\
Y_{0}\\
Z_{0}\\
1
\end{array}\right]\label{eq:camera model with internsic parameter}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1523338302
In the matrix notation,
\end_layout

\begin_layout Standard

\change_deleted 215191885 1523338302
\begin_inset Formula 
\begin{equation}
\lambda x=K\Pi_{0}gX_{0}\label{eq:camera model with internsic parameter matrix rep}
\end{equation}

\end_inset

 
\end_layout

\begin_layout Standard

\change_deleted 215191885 1523338302
To summarize, 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:camera model with internsic parameter matrix rep"

\end_inset

 represents the projection of three-dimensional coordinates 
\begin_inset Formula $X_{0}$
\end_inset

 by camera with orientation (extrinsic parameters)
\begin_inset Formula $g$
\end_inset

 and intrinsic parameters K, onto two-dimensional coordinate 
\begin_inset Formula $x$
\end_inset

 with known scale 
\begin_inset Formula $\lambda$
\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1523325388
In addition to above linear distortion, if a camera has a wide field of
 view, there will be a significant distortion along radial directions called
 radial distortion.
 Such a distortion can be models by 
\end_layout

\begin_layout Standard

\change_deleted 215191885 1523325388
\begin_inset Formula 
\[
x=x_{d}(1+a_{1}r^{2}+a_{2}r^{4})
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1523325388
\begin_inset Formula 
\[
y=y_{d}(1+a_{1}r^{2}+a_{2}r^{4})
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1523325388
where 
\begin_inset Formula $(x_{d},y_{d})$
\end_inset

 are the coordinates of the distorted points, 
\begin_inset Formula $a_{1},a_{2}$
\end_inset

are the coefficients of radial distortion and 
\begin_inset Formula $r$
\end_inset

 is the radius of the radial distortion.
\change_inserted 215191885 1523661935

\end_layout

\begin_layout Subsection

\change_inserted 215191885 1523753565
Estimation of the 3D structure
\begin_inset CommandInset label
LatexCommand label
name "subsec:Estimation-of-the-depth"

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1524747679
The image of a three-dimensional structure is generated by the principles
 of projection as depicted in the equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:camera model with internsic parameter matrix rep-1"

\end_inset

.
 By doing so, we lose the depth value of the structure, however, given a
 pair of images, generated from the same camera of known intrinsic parameters,
 represented in the matrix 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:intrensic matrix-1-1"

\end_inset

 , we could estimate the depth of structure by imposing an epiploar constraint
 on them.
\end_layout

\begin_layout Subsubsection
Epipolar geometry
\change_inserted 215191885 1521650966

\begin_inset CommandInset label
LatexCommand label
name "subsec:Epipolar-geometry"

\end_inset


\change_unchanged

\end_layout

\begin_layout Standard

\change_inserted 215191885 1524747784
The figure 
\begin_inset CommandInset ref
LatexCommand eqref
reference "fig:Epipolar-geometry"

\end_inset

 represents an epipolar geometry, in which a pair of cameras at their respective
 origins 
\begin_inset Formula $o_{1}$
\end_inset

 and 
\begin_inset Formula $o_{2}$
\end_inset

, with a relative pose of 
\begin_inset Formula $\xi=(\boldsymbol{R},\boldsymbol{T})$
\end_inset

, where 
\begin_inset Formula $\boldsymbol{R}\in SO(3)$
\end_inset

 is the relative orientation and 
\begin_inset Formula $\boldsymbol{T}\in\mathbb{R}^{3}$
\end_inset

 is translation, are capturing two dimensional images of the same three
 dimensional point 
\begin_inset Formula $p$
\end_inset

.
 Here, the projection of a camera center 
\begin_inset Formula $o_{1}$
\end_inset

 and 
\begin_inset Formula $o_{2}$
\end_inset

 onto their counter stereo image pair are called epipoles 
\begin_inset Formula $e_{1}$
\end_inset

 and 
\begin_inset Formula $e_{2}$
\end_inset

 respectively.
 The triangle formed between the camera origins 
\begin_inset Formula $o_{1}$
\end_inset

 , 
\begin_inset Formula $o_{2}$
\end_inset

 and the three dimensional point 
\begin_inset Formula $p$
\end_inset

 is in the same plane, called an epiploar plane.
 The lines, 
\begin_inset Formula $l_{1}$
\end_inset

 and 
\begin_inset Formula $l_{1}$
\end_inset

, formed by intersection of epiploar plane with image plane are called epipolar
 lines.
 
\change_deleted 215191885 1524181810
Consider two images of the same point p from two camera position with relative
 pose 
\begin_inset Formula $(R,T)$
\end_inset

, where 
\begin_inset Formula $R\in SO(3)$
\end_inset

 is the relative orientation and 
\begin_inset Formula $T\in\mathbb{R}^{3}$
\end_inset

is the relative position,then
\change_unchanged
 
\change_inserted 215191885 1524747783
I
\change_deleted 215191885 1524181814
i
\change_unchanged
f 
\begin_inset Formula $X_{1},X_{2}\in\mathbb{R}^{3}$
\end_inset

 are the 3-D coordinates of a point p relative to the two camera frames,
 by the rigid-body transformation we have
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\boldsymbol{X}_{2}=\boldsymbol{R}\boldsymbol{X}_{1}+\boldsymbol{T}
\]

\end_inset

Now,
\change_inserted 215191885 1523509864
 
\change_unchanged
let 
\begin_inset Formula $x_{1},x_{2}\in\mathbb{R}^{3}$
\end_inset


\change_inserted 215191885 1524238533
 
\change_unchanged
be the homogeneous coordinates of the projection of the same point p in
 the two image planes with respective unknown scales of 
\begin_inset Formula $s_{1}$
\end_inset

and 
\begin_inset Formula $s_{2}$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
s_{2}\boldsymbol{x}_{2}=\boldsymbol{R}s_{1}\boldsymbol{x}_{1}+\boldsymbol{T}
\]

\end_inset


\end_layout

\begin_layout Standard
By 
\change_inserted 215191885 1524187033
left-
\change_unchanged
multiplying both the side by 
\begin_inset Formula $\hat{T}$
\end_inset


\change_inserted 215191885 1524186770
, where 
\begin_inset Formula $\hat{T}$
\end_inset

 is a skew symmetric representation of the vector T
\change_unchanged

\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
s_{2}\boldsymbol{\hat{T}}\boldsymbol{x}_{2}=\hat{\boldsymbol{T}}\boldsymbol{R}s_{1}\boldsymbol{x}_{1}
\]

\end_inset


\end_layout

\begin_layout Standard
By 
\change_inserted 215191885 1524187038
left-
\change_unchanged
multiplying both the side by 
\begin_inset Formula $x_{2}^{T}$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
0=\boldsymbol{x}_{2}^{T}\boldsymbol{\hat{T}}\boldsymbol{R}s_{1}\boldsymbol{x}_{1}\label{eq:epipolar constraint}
\end{equation}

\end_inset


\change_inserted 215191885 1523630316

\end_layout

\begin_layout Standard

\change_inserted 215191885 1523630319
with
\end_layout

\begin_layout Standard

\change_inserted 215191885 1523630322
\begin_inset Formula 
\begin{equation}
\boldsymbol{E}=\hat{\boldsymbol{T}}\boldsymbol{R}\label{eq:essnetial matrix}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521916613
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521916613
\begin_inset Graphics
	filename images/epipolar_geo.png
	width 8cm
	height 6cm

\end_inset


\end_layout

\begin_layout Plain Layout

\change_inserted 215191885 1521916613
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1523633145
Epipolar geometry
\begin_inset CommandInset label
LatexCommand label
name "fig:Epipolar-geometry"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\change_unchanged

\end_layout

\begin_layout Standard

\change_deleted 215191885 1516609323
\begin_inset Graphics
	filename images/epipolar_geo.png
	width 8cm
	height 6cm

\end_inset


\change_unchanged

\end_layout

\begin_layout Standard

\change_deleted 215191885 1523633103
This
\change_inserted 215191885 1523633111
Equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:epipolar constraint"

\end_inset


\change_unchanged
 is the epipolar constraint and the matrix 
\begin_inset Formula $\boldsymbol{E}=\hat{\boldsymbol{T}}\boldsymbol{R}$
\end_inset

 is called the essential matrix.

\change_inserted 215191885 1523339593
 
\change_unchanged
It encodes the relative pose between the two cameras.
 Geometrically, it 
\change_deleted 215191885 1523511406
impose
\change_inserted 215191885 1524181933
proves
\change_unchanged
 that 
\change_inserted 215191885 1524182056

\begin_inset Formula $o_{1}$
\end_inset

 , 
\begin_inset Formula $o_{2}$
\end_inset

 and 
\begin_inset Formula $p$
\end_inset

 
\change_deleted 215191885 1524182052
the
\change_unchanged
 
\change_deleted 215191885 1523339600
The
\change_unchanged
 
\change_deleted 215191885 1524182031
vector connecting the first camera center 
\begin_inset Formula $o_{1}$
\end_inset

 and the point p, the vector connecting 
\begin_inset Formula $o_{2}$
\end_inset

 and p, and the vector connecting the two optical centers 
\begin_inset Formula $o_{1}$
\end_inset

 and 
\begin_inset Formula $o_{2}$
\end_inset

 clearly
\change_unchanged
 form
\change_inserted 215191885 1524182055
s
\change_unchanged
 a triangle
\change_deleted 215191885 1524182093
.

\change_unchanged
 
\change_deleted 215191885 1524182073
Therefore
\change_inserted 215191885 1524182073
and
\change_unchanged
, 
\change_deleted 215191885 1524238579
the three vectors
\change_unchanged
 lie
\change_inserted 215191885 1524182090
s
\change_unchanged
 on the same plane
\change_inserted 215191885 1524182055
, i.e., an epiploar plane,
\change_deleted 215191885 1523633578
.

\change_inserted 215191885 1524182055
 h
\change_deleted 215191885 1523633589
H
\change_unchanged
ence 
\change_deleted 215191885 1523633592
the
\change_unchanged
 their triple product which measures the volume of the parallelepiped is
 zero.

\change_inserted 215191885 1524182055
 
\change_deleted 215191885 1524181843
 
\change_inserted 215191885 1524182055

\end_layout

\begin_layout Subsubsection

\change_inserted 215191885 1523634665
Solving for Scene Geometry
\end_layout

\begin_layout Standard

\change_inserted 215191885 1523634665
There are generically two different approaches for 3D reconstruction referred
 to as 
\emph on
sparse
\emph default
 and 
\emph on
dense
\emph default
.
 Sparse methods reconstruct the 3D scene geometry only for a select subset
 of the entire image data 
\begin_inset CommandInset citation
LatexCommand cite
key "klein07parallel"

\end_inset

.
 This subset is often corner locations or locations marked by some type
 of feature extraction, e.g., SIFT or SURF.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Lowe2004,Bay:2008:SRF:1370312.1370556"

\end_inset

 This results in a sparse description of the 3D scene in terms of a point
 cloud.
 In contrast, dense methods 
\begin_inset CommandInset citation
LatexCommand cite
key "6696650"

\end_inset

 reconstruct as many 3D geometric locations as possible and seek to provide
 a complete description of the 3D scene.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1523634665
Sparse reconstructions often benefit from having a lower computational cost
 but provide few 3D measurements.
 Dense reconstructions have higher computational cost but provide a much
 more complete description of the 3D scene.
 Dense reconstruction techniques have seen much recent interest, although
 a highly accurate, dense, and real-time SfM approach has remained elusive.
 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1523634665
A third class of algorithms, referred to as 
\emph on
semi-dense
\emph default
 algorithms 
\begin_inset CommandInset citation
LatexCommand cite
key "6751290"

\end_inset

, seeks to strike a compromise between the sparse and dense methods.
 The reconstruction techniques used are most similar to dense methods, however,
 only a subset of all image pixels are reconstructed.
 These approaches leverage the high accuracy of dense reconstruction techniques,
 but are sparse enough to allow for real-time operation.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1523634665
Here, reconstruction is limited to those pixels which possess high intensity
 gradient values.
 These regions often correspond to scene geometries such as edges, corners,
 and curves and to other areas of the scene that are highly textured.
 The thought here is that regions of the image that possess large changes
 in intensity convey more information than regions that possess less, thus
 semi-dense reconstructions provide a compressed version of the total scene
\end_layout

\begin_layout Subsubsection

\change_inserted 215191885 1523987430
Stereo correspondence and Disparity Estimation
\begin_inset CommandInset label
LatexCommand label
name "subsec:Stereo-corr-and-disparity-est"

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1524241453
We can estimate the depth of the object from a pair of images captured by
 the same camera with a small transnational change but the same orientation.
 The amount of distance that the object has shifted from the first image
 to the second is called the disparity 
\begin_inset CommandInset citation
LatexCommand cite
key "Schmidt2002DenseDM"

\end_inset

, and this information is useful in computing the depth of the object itself.
 With the epiplore constraint 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Epipolar-geometry"

\end_inset

 and known translation between the pair of images, we can efficiently compute
 disparity of the pixel, this approach is called the fixed baseline disparity
 mapping since the baseline between the pair of images are known, and they
 are fixed.
 This process starts with finding the correspondence between images.
 Stereo correspondence is the problem of finding which part of one image
 correspond to which parts of another image, where the difference is due
 to the movement of the camera.
 As discussed in 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Solving-for-Image"

\end_inset

 the correspondence can be either dense or sparse.
 In dense correspondence, image intensities are used, whereas in the sparse,
 only image feature are used.
 Also, with epiploar constraint in place, the correspondence search can
 be narrowed down along the epipolar lines.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521916660
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521916660
\begin_inset Graphics
	filename images/disparity_2.png
	width 8cm
	height 6cm

\end_inset


\end_layout

\begin_layout Plain Layout

\change_inserted 215191885 1521916660
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1521916731
Disparity Computation
\begin_inset CommandInset label
LatexCommand label
name "fig:Disparity-Computation"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1524239466
Let 
\begin_inset Formula $x_{L}$
\end_inset

 and 
\begin_inset Formula $x_{R}$
\end_inset

 be the one of the established correspondences in the image pair 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Disparity-Computation"

\end_inset

 and 
\begin_inset Formula $P=(x_{p},y_{p},z_{p})$
\end_inset

 be the 3D location of object, from the epiploar constraint 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:epipolar constraint"

\end_inset

, we know that these corresponding image points and 3D position of the object
 forms a triangle as depicted in the image 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Disparity-Computation"

\end_inset

 Since the focal length 
\begin_inset Formula $f$
\end_inset

 of the camera and baseline 
\begin_inset Formula $B=2l$
\end_inset

 are known, we can compute the disparity 
\begin_inset Formula $\lambda$
\end_inset

, and consequently the inverse depth of object 
\begin_inset Formula $invdepth$
\end_inset

 with properties of similarity triangles as following
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521739289
\begin_inset Formula 
\[
\lambda=x_{L}-x_{R}=f\left(\frac{x_{p}+l}{z_{p}}-\frac{x_{p}-l}{z_{p}}\right)
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1524162208
\begin_inset Formula 
\[
\lambda=\frac{2fl}{z_{p}}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1524162278
\begin_inset Formula 
\[
\frac{1}{z_{p}}=\frac{\lambda}{fB}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1524162320
\begin_inset Formula 
\begin{equation}
invdepth=\frac{\lambda}{fB}\label{eq:inverse_depth}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1524162331
\begin_inset Formula 
\begin{equation}
invdepth\propto disparity\label{eq:relation_invDep_and_diparity}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1524748250
However, the problem with the fixed baseline disparity computation is that
 the error grows quadratically with depth, error of the depth is proportional
 to the length of the baseline, as a result, for the shorter baseline, the
 nearer depths have better accuracy but not the further depths, and vise-versa.
 This can addressed efficiently by using variable baseline/resolution instead
 of fixed baseline
\begin_inset CommandInset citation
LatexCommand cite
key "4587671"

\end_inset

, where the baselines and focal length are selected with proportion to the
 depth, which helps us archive the constant accuracy over all the depths.
 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1524241162
Further accuracy and performance improvements could be achieved by adopting
 a probabilistic approach of adaptive baseline
\begin_inset CommandInset citation
LatexCommand cite
key "6751290"

\end_inset

, which explicitly takes advantage of the fact that in a video, small baseline
 frames are available before large-baseline frames.
 In this case, instead of estimating depth for every pixel, only the subset
 of them which contributes to the high accuracy of disparity search were
 preferred.
 For every qualified pixel, a suitable reference frame is selected from
 the stack of earlier tracked frames for disparity search, in which, the
 one dimensional disparity search is performed along the epiploar line.
 This selection of the frame is based on the age of the pixel, for a given
 pixel, the oldest frame in which disparity search range and the observation
 angle was not above certain threshold was selected.
 If a disparity search with this frame is unsuccessful, the pixels age is
 increased, such that subsequent disparity search use newer frames.
 This adaptive baseline disparity search maximizes the stereo accuracy.
 Further, the prior knowledge from an earlier iteration of disparity search
 is utilized to minimize the disparity search range.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1524241855
The disparity search is performed along the epiploar line, and it is subjected
 to two sources of errors 
\begin_inset CommandInset citation
LatexCommand cite
key "6751290"

\end_inset

, the geometric error, which accounts for the noise in rigid body motion
 
\begin_inset Formula $\xi$
\end_inset

 and the projection 
\begin_inset Formula $\pi$
\end_inset

.
 And, the photometric error, which is caused by the intensity difference
 between image pair.
 The image gradient plays a crucial role in depth estimation since it determines
 the extent of both the errors.
 The direction of the image gradient defines the geometric error, and its
 magnitude defines the photometric error.
 i.e., the geometric error on the epipolar line leads a small disparity error,
 if the epipolar line is parallel to the image gradient, and a large otherwise.
 Similarly, small image intensity errors have a large effect on the estimated
 disparity if the image gradient is small, and a small effect otherwise.
 This intuition helps us prevent the erroneous depth computation in the
 earlier stage itself, as soon as we have the epipolar lines constructed
 for given pixel, we can compare the direction and magnitude of the epipolar
 line to the local gradient of the image around the pixel for a threshold.
 As a consequence, the region of the image with the rich pattern, which
 results in large gradient change, have a high probability of depth estimation,
 then the region with no gradient, resulting in the semi dense depth estimation.
 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1524242201
Let 
\begin_inset Formula $l$
\end_inset

 and 
\begin_inset Formula $g$
\end_inset

 represent a normalized epipolar line and a normalized image gradient respective
ly, if 
\begin_inset Formula $\epsilon_{l}$
\end_inset

 is the isotropic Gaussian noise, with variance 
\begin_inset Formula $\sigma_{l}^{2}$
\end_inset

 , associated with the absolute position of the origin of the epipolar line
 
\begin_inset Formula $l$
\end_inset

, then the variance of the geometric disparity error is 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1524176062
\begin_inset Formula 
\begin{equation}
\sigma_{\lambda(\xi,\pi)}^{2}=\frac{\sigma_{l}^{2}}{\left\langle g,l\right\rangle ^{2}}\label{eq:geo_error_var}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1524201147
It is evident in the equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:geo_error_var"

\end_inset

 that the direction of the image gradient and epipolar line defines this
 error, and this originates from the noise in camera orientation 
\begin_inset Formula $\xi$
\end_inset

 and the camera calibration 
\begin_inset Formula $\pi$
\end_inset

 and independent of noise in intensity of image.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1524242233
Let 
\begin_inset Formula $g_{p}$
\end_inset

 representing the gradient of a image intensity on the epipolar line at
 disparity 
\begin_inset Formula $\lambda$
\end_inset

, and 
\begin_inset Formula $\sigma_{i}^{2}$
\end_inset

 representing the variance of the image intensity noise, we can have the
 variance of the photometric disparity error as
\end_layout

\begin_layout Standard

\change_inserted 215191885 1524178352
\begin_inset Formula 
\begin{equation}
\sigma_{\lambda(I)}^{2}=\frac{2\sigma_{i}^{2}}{g_{p}^{2}}\label{eq:intensity_var_var}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1524242274
Equation 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:intensity_var_var"

\end_inset

 shows that the photometric disparity error depends on the magnitude of
 gradient of image, which directly depends on the intensity of image and
 hence is independent of the geometric disparity error.
 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1524179685
With equation 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:geo_error_var"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:intensity_var_var"

\end_inset

 the total the observation variance of the inverse depth estimated using
 equation 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:inverse_depth"

\end_inset

 is
\end_layout

\begin_layout Standard

\change_inserted 215191885 1524179792
\begin_inset Formula 
\begin{equation}
\sigma_{d,obs}^{2}=\alpha^{2}(\sigma_{\lambda(\xi,\pi)}^{2}+\sigma_{\lambda(I)}^{2})\label{eq:depth_observation_var}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1524242336
where 
\begin_inset Formula $\alpha$
\end_inset

 is the proportionality constant defined for each pixel, and can be calculated
 as 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1524179902
\begin_inset Formula 
\[
\alpha=\frac{\delta_{d}}{\delta_{\lambda}}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1524180115
where 
\begin_inset Formula $\delta_{d}$
\end_inset

 is the length of the searched inverse depth interval, and 
\begin_inset Formula $\delta_{\lambda}$
\end_inset

 the length of the searched epipolar line segment.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1524180580
For a given pixel, the constructed depth from equation 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:inverse_depth"

\end_inset

 is represented as Gaussian distribution with the variance 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:depth_observation_var"

\end_inset

 as
\end_layout

\begin_layout Standard

\change_inserted 215191885 1524180298
\begin_inset Formula 
\begin{equation}
\mathcal{N}(d_{obs},\sigma_{d,obs}^{2})\label{eq:gassuain_stereo_depth}
\end{equation}

\end_inset


\end_layout

\begin_layout Subsubsection

\change_inserted 215191885 1523672918
Depth propagation and fusion
\begin_inset CommandInset label
LatexCommand label
name "subsec:Depth-propagation"

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1524242569
The depths estimated for every pair of images are propagated and can be
 used as an prior knowledge for the next iteration of depth estimation for
 the new pair of images.
 For the small camera rotation, the inverse depth and its variance from
 the earlier frame is propagated and is assigned to closet integer pixel
 position.
 If 
\begin_inset Formula $d_{0}$
\end_inset

 is the inverse depth for a pixel from the previous frame, and the inverse
 depth 
\begin_inset Formula $d_{1}$
\end_inset

 at the same pixel for the current frame with the camera translation of
 
\begin_inset Formula $t_{z}$
\end_inset

 along the optical axis is estimated as 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1523637725
\begin_inset Formula 
\[
d_{1}(d_{0})=(d_{0}^{-1}-t_{z})^{-1}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1523637857
and its variance 
\begin_inset Formula $\sigma_{d_{1}}^{2}$
\end_inset

 is given by 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1523637862
\begin_inset Formula 
\[
\sigma_{d_{1}}^{2}=\left(\frac{d_{1}}{d_{0}}\right)^{4}\sigma_{d_{0}}^{2}+\sigma_{p}^{2}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1524242419
where, 
\begin_inset Formula $\sigma_{d_{0}}^{2}$
\end_inset

 is the variance of inverse depth from previous frame and 
\begin_inset Formula $\sigma_{d_{p}}^{2}$
\end_inset

 is the inverse depth prediction uncertainty.
 These depths are fused in similar fashion of the extended Kalman filter
\begin_inset CommandInset citation
LatexCommand cite
key "Welch:1995:IKF:897831"

\end_inset

 , where 
\begin_inset Formula $\sigma_{d_{p}}^{2}$
\end_inset

 corresponds to variance from to the prediction step and 
\begin_inset Formula $\sigma_{d_{0}}^{2}$
\end_inset

 corresponds to the variance from the measurement step.
 Finally, the new observation of inverse depth is incorporated into the
 prior, by multiplying two distributions, for better standard deviation.
 i.e., if 
\begin_inset Formula $\mathcal{N}(d_{p},\sigma_{p}^{2})$
\end_inset

 is the prior distribution and the 
\begin_inset Formula $\mathcal{N}(d_{o},\sigma_{o}^{2})$
\end_inset

 is the noise observation, the posterior inverse depth is 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1523671854
\begin_inset Formula 
\[
\mathcal{N}(\frac{\sigma_{p}^{2}d_{o}+\sigma_{o}^{2}d_{p}}{\sigma_{p}^{2}+\sigma_{o}^{2}},\frac{\sigma_{p}^{2}\sigma_{o}^{2}}{\sigma_{p}^{2}+\sigma_{o}^{2}})
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1523673331
The eight-point linear algorithm
\end_layout

\begin_layout Standard

\change_deleted 215191885 1523673331
With epipolar constrain between two images, we should be able to retrieve
 the relative pose of the cameras.
 The eight-point linear algorithm is a simple closed-form algorithm, it
 consists of two steps: First a matrix E is recovered from a number of epipolar
 constraints; then relative translation and orientation are extracted from
 E.
\end_layout

\begin_layout Standard

\change_deleted 215191885 1523673331
The entries of E are denoted by 
\end_layout

\begin_layout Standard

\change_deleted 215191885 1523673331
\begin_inset Formula 
\[
E=\left[\begin{array}{ccc}
e_{11} & e_{12} & e_{13}\\
e_{21} & e_{22} & e_{23}\\
e_{31} & e_{32} & e_{33}
\end{array}\right]\in\mathbb{R}^{3\times3}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1523673331
The matrix E is reshaped into vector 
\begin_inset Formula $E\in\mathbb{R}^{9}$
\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1523673331
\begin_inset Formula 
\[
E=\left[e_{11},e_{21},e_{31},e_{12},e_{22},e_{32},e_{13},e_{23},e_{33}\right]^{T}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1523673331
and for 
\begin_inset Formula $x_{1}=\left[x_{1,}y_{1},z_{1}\right]^{T}\in\mathbb{R}^{3}$
\end_inset

and 
\begin_inset Formula $x_{2}=\left[x_{2,}y_{2},z_{2}\right]^{T}\in\mathbb{R}^{3}$
\end_inset

 define 
\end_layout

\begin_layout Standard

\change_deleted 215191885 1523673331
\begin_inset Formula 
\[
a=[x_{1}x_{2},x_{1}y_{2},x_{1}z_{2},y_{1}x_{2},y_{1}y_{2},y_{1}z_{2},z_{1}x_{2},z_{1}y_{2},z_{1}z_{2}]\in\mathbb{R}^{9}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1523673331
With these new notations, we can rewrite the 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:epipolar constraint"

\end_inset

 as below
\end_layout

\begin_layout Standard

\change_deleted 215191885 1523673331
\begin_inset Formula 
\[
a^{T}E=0
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1523673331
this representation emphasizes the linear dependence of the epipolar constraint
 on the elements of the essential matrix.
\end_layout

\begin_layout Standard

\change_deleted 215191885 1523673331
Now,with a set of corresponding image points 
\begin_inset Formula $(x_{1}^{j},x_{2}^{j}),j=1,2,....,n$
\end_inset

 we can define a matrix 
\begin_inset Formula $\chi\in\mathbb{R}^{n\times9}$
\end_inset

 associated with these measurements to be
\end_layout

\begin_layout Standard

\change_deleted 215191885 1523673331
\begin_inset Formula 
\[
\chi=\left[a^{1},a^{2},...,a^{n}\right]^{T}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1523673331
In the absence of noise, the vector E satisfies
\end_layout

\begin_layout Standard

\change_deleted 215191885 1523673331
\begin_inset Formula 
\[
\chi E=0
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1523673331
In order to obtain the unique solution, the rank of the matrix 
\begin_inset Formula $\chi\in\mathbb{R}^{n\times9}$
\end_inset

 needs to be exactly eight.
 This should be the case when we have 
\begin_inset Formula $n\geq8$
\end_inset

 "ideal" corresponding points, hence the name The eight-point linear algorithm.
\end_layout

\begin_layout Standard

\change_deleted 215191885 1523673331
Because of errors in correspondences, we try to find the E that minimizes
 the least-squares error function 
\begin_inset Formula $\left\Vert \chi E\right\Vert ^{2}$
\end_inset

.We do this by choosing eigenvector of 
\begin_inset Formula $\chi^{T}\chi$
\end_inset

 that corresponds to its smallest eigenvalue.
\end_layout

\begin_layout Standard

\change_deleted 215191885 1523673331
Once we have E, we need to extract the pose (
\begin_inset Formula $R\in SO(3)$
\end_inset

 and 
\begin_inset Formula $T\in\mathbb{R}^{3}$
\end_inset

) from E, we know that 
\begin_inset CommandInset citation
LatexCommand cite
key "41368"

\end_inset

a nonzero matrix 
\begin_inset Formula $E\in\mathbb{R}^{3}$
\end_inset

 is an essential matrix if and only if E has a singular value decomposition
 
\begin_inset Formula $(SVD)E=U\varSigma V^{T}$
\end_inset

with 
\end_layout

\begin_layout Standard

\change_deleted 215191885 1523673331
\begin_inset Formula 
\[
\varSigma=diag\left\{ \sigma,\sigma,0\right\} 
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1523673331
for some 
\begin_inset Formula $\sigma>0$
\end_inset

 and 
\begin_inset Formula $U,V,\in SO(3)$
\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1523673331
With this we can obtain two relative pose
\end_layout

\begin_layout Standard

\change_deleted 215191885 1523673331
\begin_inset Formula 
\[
(\hat{T}_{1},R_{1})=(UR_{Z}(+\frac{\pi}{2})\Sigma U^{T},UR_{Z}^{T}(+\frac{\pi}{2})V^{T})
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1523673331
\begin_inset Formula 
\[
(\hat{T}_{2},R_{2})=(UR_{Z}(-\frac{\pi}{2})\Sigma U^{T},UR_{Z}^{T}(-\frac{\pi}{2})V^{T})
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1523673331
Among these one with that gives the meaningful (positive) depth are selected
 as the valid pose.
\end_layout

\begin_layout Standard

\change_deleted 215191885 1523673331
With 
\begin_inset Formula $R_{Z}(\pm\frac{\pi}{2})=\left[\begin{array}{ccc}
0 & \pm1 & 0\\
\pm1 & 0 & 0\\
0 & 0 & 1
\end{array}\right]$
\end_inset


\end_layout

\begin_layout Paragraph

\change_deleted 215191885 1523673331
Structure Reconstruction
\end_layout

\begin_layout Standard

\change_deleted 215191885 1523673331
One remaining thing to find is the position of points in three-dimension
 by recovering their depths relative to each camera frame.With the estimated
 pose (Translation is T is defined up to the scale 
\begin_inset Formula $\gamma$
\end_inset

) and point correspondence, we have
\end_layout

\begin_layout Standard

\change_deleted 215191885 1523673331
\begin_inset Formula 
\begin{equation}
\lambda_{2}^{j}x_{2}^{j}=\lambda_{1}^{j}Rx_{1}^{j}+\gamma T\label{eq:st_recon}
\end{equation}

\end_inset

 for 
\begin_inset Formula $j=1,2,3...,n$
\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1523673331
where, 
\begin_inset Formula $\lambda_{1}$
\end_inset

and 
\begin_inset Formula $\lambda_{2}$
\end_inset

are depths with respect to the first and second camera frames, respectively.
 One of the depths is redundant, if 
\begin_inset Formula $\lambda_{1}$
\end_inset

 is known, we can estimate 
\begin_inset Formula $\lambda_{2}$
\end_inset

 as a function of 
\begin_inset Formula $(R,T)$
\end_inset

.Hence we can eliminate, say, 
\begin_inset Formula $\lambda_{2}$
\end_inset

 from the above equation by multiplying both sides by 
\begin_inset Formula $\hat{x_{2}}$
\end_inset

 
\end_layout

\begin_layout Standard

\change_deleted 215191885 1523673331
\begin_inset Formula 
\[
0=\lambda_{1}^{j}x_{2}^{j}Rx_{1}^{j}+\gamma x_{2}^{j}T
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1523673331
This is represented as linear equations
\end_layout

\begin_layout Standard

\change_deleted 215191885 1523673331
\begin_inset Formula 
\[
M^{j}\bar{\lambda^{j}}=\left[\hat{x_{2}^{j}}Rx_{1}^{j},\hat{x_{2}^{j}}T\right]\left[\begin{array}{c}
\lambda_{1}^{j}\\
\gamma
\end{array}\right]=0
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1523673331
where 
\begin_inset Formula $M^{j}=\left[\hat{x_{2}^{j}}Rx_{1}^{j},\hat{x_{2}^{j}}T\right]\in\mathbb{R}^{3\times2}$
\end_inset

and 
\begin_inset Formula $\bar{\lambda^{j}}=\left[\begin{array}{c}
\lambda_{1}^{j}\\
\gamma
\end{array}\right]\in\mathbb{R}^{2}$
\end_inset

 for 
\begin_inset Formula $j=1,2,3...,n$
\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1523673331
since all n equations above share the same 
\begin_inset Formula $\gamma$
\end_inset

; we define a vector 
\begin_inset Formula $\vec{\lambda}=\left[\lambda_{1}^{1},\lambda_{1}^{2},...,\lambda_{1}^{2},\gamma\right]$
\end_inset

 and a matrix 
\begin_inset Formula $M\in\mathbb{R}^{3n\times(n+1)}$
\end_inset

as
\end_layout

\begin_layout Standard

\change_deleted 215191885 1523673331
\begin_inset Formula 
\[
M=\left[\begin{array}{cccccc}
\hat{x}_{2}^{1}Rx_{1}^{1} & 0 & 0 & 0 & 0 & \hat{x}_{2}^{1}T\\
0 & \hat{x}_{2}^{2}Rx_{1}^{2} & 0 & 0 & 0 & \hat{x}_{2}^{2}T\\
0 & 0 & \ddots & 0 & 0 & \vdots\\
0 & 0 & 0 & \hat{x}_{2}^{n-1}Rx_{1}^{n-1} & 0 & \hat{x}_{2}^{n-1}T\\
0 & 0 & 0 & 0 & \hat{x}_{2}^{n}Rx_{1}^{n} & \hat{x}_{2}^{n}T
\end{array}\right]
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1523673331
Then the equation
\end_layout

\begin_layout Standard

\change_deleted 215191885 1523673331
\begin_inset Formula 
\begin{equation}
M\overrightarrow{\lambda}=0\label{eq:depth_est}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1523673331
determines all the unknown depths up to a single universal scale.
 The linear least-squares estimate of 
\begin_inset Formula $\overrightarrow{\lambda}$
\end_inset

 is simply the eigenvector of 
\begin_inset Formula $M^{T}M$
\end_inset

s that corresponds to its smallest eigenvalue.
\change_inserted 215191885 1523420195

\end_layout

\begin_layout Section
Non linear Optimization
\change_inserted 215191885 1524197371

\begin_inset CommandInset label
LatexCommand label
name "sec:Non-linear-Optimization"

\end_inset


\change_unchanged

\end_layout

\begin_layout Standard
In practice, because of the noise in image correspondence and other errors
\change_inserted 215191885 1524746524
,
\change_unchanged
 we cannot measure the actual coordinates but only their noisy versions,
 say
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\widetilde{x}_{1}^{j}=x_{1}^{j}+\omega_{1}^{j}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\widetilde{x}_{2}^{j}=x_{2}^{j}+\omega_{2}^{j}
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $x_{1}^{j}$
\end_inset

and 
\begin_inset Formula $x_{2}^{j}$
\end_inset


\change_inserted 215191885 1524114491
 
\change_unchanged
are the ideal image coordinates and 
\begin_inset Formula $\omega_{1}^{j}=\left[\omega_{11}^{j},\omega_{12}^{j},0\right]^{T}$
\end_inset

and 
\begin_inset Formula $\omega_{2}^{j}=\left[\omega_{21}^{j},\omega_{22}^{j},0\right]^{T}$
\end_inset

are localization errors 
\change_deleted 215191885 1523494300
(called residuals)
\change_unchanged
 in the correspondence
\change_inserted 215191885 1523494305
 called residuals
\change_unchanged
.

\change_inserted 215191885 1523494307
 
\change_unchanged
Therefore, we need a way to optimize the parameters 
\begin_inset Formula $(x,R,T)$
\end_inset

 that minimize this errors.
\end_layout

\begin_layout Standard
One of the 
\change_deleted 215191885 1523983781
minimalistic
\change_unchanged
 approach to 
\change_deleted 215191885 1523983788
optimality
\change_inserted 215191885 1523983798
optimize the error,
\change_unchanged
 is to minimize the squared 2-norm of residuals, if we 
\change_deleted 215191885 1523983808
choose
\change_inserted 215191885 1523983810
consider
\change_unchanged
 the first camera frame as the reference 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\phi(x,\boldsymbol{R},\boldsymbol{T},c)=\stackrel[j=1]{n}{\sum}\left\Vert \omega_{1}^{j}\right\Vert +\left\Vert \omega_{2}^{j}\right\Vert ^{2}=\stackrel[j=1]{n}{\sum}\left\Vert \widetilde{x}_{1}^{j}-x_{1}^{j}\right\Vert ^{2}+\left\Vert \widetilde{x}_{2}^{j}-\pi(\boldsymbol{R}c^{j}x_{1}^{j}+\boldsymbol{T})\right\Vert ^{2}\label{eq:residual}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
The 
\change_deleted 215191885 1523983831
above
\change_unchanged
 error
\change_inserted 215191885 1524743788
 in the equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:residual"

\end_inset


\change_unchanged
 is often called the 
\begin_inset Quotes eld
\end_inset

re-projection error
\begin_inset Quotes erd
\end_inset

, since 
\begin_inset Formula $x_{1}^{j}$
\end_inset

 and 
\begin_inset Formula $x_{2}^{j}$
\end_inset


\change_inserted 215191885 1524265189
 
\change_unchanged
are the recovered 
\change_deleted 215191885 1523983867
3-D
\change_inserted 215191885 1523983869
three dimensional
\change_unchanged
 points projected back onto the image planes.
 This process of minimizing the 
\change_deleted 215191885 1523983882
above
\change_unchanged
 expression
\change_inserted 215191885 1523983892
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:residual"

\end_inset


\change_unchanged
 for the unknowns 
\begin_inset Formula $(R,T,x_{1},\lambda)$
\end_inset

 is 
\change_inserted 215191885 1523983903
also 
\change_unchanged
known as bundle adjustment
\begin_inset CommandInset citation
LatexCommand cite
key "Triggs:1999:BAM:646271.685629"

\end_inset


\change_inserted 215191885 1523984115
, since, we are adjusting bundles of light rays to reduce the error.
\change_deleted 215191885 1523983905
.
\change_inserted 215191885 1524106243

\end_layout

\begin_layout Standard

\change_inserted 215191885 1524107675
For the purpose of simplification consider a function
\end_layout

\begin_layout Standard

\change_inserted 215191885 1524106658
\begin_inset Formula 
\begin{equation}
y_{i}=f(x_{i})\label{eq:simple eq for optimization}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1524107840
the goal is to find the parameters 
\begin_inset Formula $x$
\end_inset

, which minimizes the squared error between observation 
\begin_inset Formula $y$
\end_inset

 and estimation 
\begin_inset Formula $f(x),$
\end_inset

 i.e residual, 
\begin_inset Formula 
\begin{equation}
r_{i}(x)^{2}=(f(x_{i})-y_{i})^{2}\label{eq:simple_residual}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521910863
One of the simplest ways to minimize this squared error is the gradient
 descent method.
 It is a first-order optimization method which aims to determine a local
 minimum of a non-convex cost function by iteratively stepping in the direction
 in which the energy decreases most.
 Hence this method is also called as the steepest descent method.
 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521910909
\begin_inset Formula 
\begin{equation}
x_{t+1}=x_{t}-\alpha\boldsymbol{g}\label{eq:gradiant_decent}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521910964
with step size 
\begin_inset Formula $\alpha$
\end_inset

 and gradient g
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521910909
\begin_inset Formula 
\begin{equation}
\boldsymbol{g}_{j}=2\underset{i}{\sum}r_{i}\frac{\partial r_{i}}{\partial x_{i}}\label{eq:gradiant}
\end{equation}

\end_inset


\change_unchanged

\end_layout

\begin_layout Standard

\change_deleted 215191885 1521909550
One of the many ways to minimize this squared error is the
\change_inserted 215191885 1524746734
The problem with the gradient descent is that, since it is only a first
 order approximation, it it may end up in local minimum which is not necessarily
 the global minimum.
 We can achieve a better results with second order approximation, and it
 is done by an efficient approach called the
\change_unchanged
 Gauss-Newton method.
 Gauss-Newton is a iterative method for finding the value of the variables
 which minimizes the sum of squares
\change_inserted 215191885 1523717305
, this method achieves this by assuming that the least squares function
 is locally quadratic and tries to find the the minimum of the quadratic.
\change_deleted 215191885 1524110813
,it starts with the initial guess and this method does not need the second
 derivatives (Hessian matrix) of the of function, which is often expensive
 and sometimes not possible to compute, instead the Hessian is approximated
 with the Jacobian matrix of the function.
 
\change_unchanged

\end_layout

\begin_layout Standard

\change_deleted 215191885 1524108412
For the 
\change_inserted 215191885 1524110830
For the residual from the equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:simple_residual"

\end_inset

 
\change_deleted 215191885 1524108471
least-square function of form
\change_inserted 215191885 1524108974
we have to minimize 
\change_unchanged

\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\underset{x}{min}\underset{i}{\sum}r_{i}(x)^{2}\label{eq:res_min}
\end{equation}

\end_inset


\change_inserted 215191885 1524109020

\end_layout

\begin_layout Standard

\change_inserted 215191885 1524110867
the parameters 
\begin_inset Formula $x$
\end_inset

 that minimizes the equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:res_min"

\end_inset

 is determined by differentiating it with respect to 
\begin_inset Formula $x$
\end_inset

 and equating it to zero
\end_layout

\begin_layout Standard

\change_inserted 215191885 1524109194
\begin_inset Formula 
\begin{equation}
2\underset{i}{\sum}r_{i}\frac{\partial r_{i}}{\partial x_{i}}=0\label{eq:residual derivatiive}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1524109483
But the equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:residual derivatiive"

\end_inset

 cannot be solved directly.
 
\change_unchanged
Gauss-Newton method iteratively 
\change_deleted 215191885 1524109545
solves
\change_inserted 215191885 1524109559
approximate the Taylor approximation of residual function
\end_layout

\begin_layout Standard

\change_inserted 215191885 1524109780
\begin_inset Formula 
\begin{equation}
r(x)\simeq r(x_{t})+\boldsymbol{g}^{T}(x-x_{t})+\frac{1}{2}(x-x_{t})^{T}\boldsymbol{H}(x-x_{t})\label{eq:Tylor_approximation_of_resduial}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1524110153
\begin_inset Formula 
\[
x_{t+1}=x_{t}-H^{-1}g
\]

\end_inset


\change_unchanged

\end_layout

\begin_layout Standard

\change_deleted 215191885 1524108558
with gradient g
\change_unchanged

\end_layout

\begin_layout Standard

\change_deleted 215191885 1524108555
\begin_inset Formula 
\[
g_{j}=2\underset{i}{\sum}r_{i}\frac{\partial r_{i}}{\partial x_{i}}
\]

\end_inset


\change_unchanged

\end_layout

\begin_layout Standard

\change_deleted 215191885 1524109872
and the Hessian H
\change_inserted 215191885 1524111886
The equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Tylor_approximation_of_resduial"

\end_inset

 is the Taylor approximation of residual upto second derivative, where 
\begin_inset Formula $g$
\end_inset

 is the gradient as defined in equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:gradiant"

\end_inset

 and the Hessian H is 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\boldsymbol{H}_{jk}=2\underset{i}{\sum}r_{i}(\frac{\partial r_{i}}{\partial x_{j}}\frac{\partial r_{i}}{\partial x_{k}}+r_{i}\frac{\partial^{2}r_{i}}{\partial x_{i}\partial x_{k}})\label{eq:full hassain}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1523981785
by
\change_inserted 215191885 1523982302
The equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:full hassain"

\end_inset

 is full hassian, by assuming the the problem is quadratic, we can
\change_unchanged
 
\change_inserted 215191885 1524265305
ignore 
\change_deleted 215191885 1524114559
dropping 
\change_unchanged
the second order term 
\change_inserted 215191885 1523982441
in it, and, 
\change_unchanged
we can approximate the Hessian matrix with Jacobian matrix 
\change_inserted 215191885 1523982179
as :
\change_unchanged

\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\boldsymbol{H}_{jk}=2\underset{i}{\sum}\boldsymbol{J}_{ij}\boldsymbol{J}_{ik}\label{eq:hassian_from_jacobian}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
with
\begin_inset Formula 
\begin{equation}
\boldsymbol{J}_{ij}=\frac{\partial r_{i}}{\partial x_{j}}\label{eq:jacobian}
\end{equation}

\end_inset


\change_inserted 215191885 1524109966

\end_layout

\begin_layout Standard

\change_inserted 215191885 1524110017
Now, the equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Tylor_approximation_of_resduial"

\end_inset

 can be differentiated as 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1524187930
\begin_inset Formula 
\begin{equation}
\frac{\partial r}{\partial x}=\boldsymbol{g}+\boldsymbol{H}(x-x_{t})=0\label{eq:gauss_new_intermediate}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1524265333
Also from the equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:gradiant"

\end_inset

 and equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:jacobian"

\end_inset

 , we have 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1524188184
\begin_inset Formula 
\begin{equation}
\boldsymbol{g}=2\boldsymbol{J}^{T}r\label{eq:gradian_jacobian}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1524188614
with the Jacobian representation for gradient as shown in equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:gradian_jacobian"

\end_inset

 and Hassian as in equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:hassian_from_jacobian"

\end_inset

, equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:gauss_new_intermediate"

\end_inset

 can be updated as 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1524188310
\begin_inset Formula 
\[
2\boldsymbol{J}^{T}r+\boldsymbol{J}^{T}\boldsymbol{J}(x-x_{t})=0
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1524188436
and 
\begin_inset Formula $x_{t+1}$
\end_inset

 is estimated as
\end_layout

\begin_layout Standard

\change_inserted 215191885 1524110915
\begin_inset Formula 
\begin{equation}
x_{t+1}=x_{t}-(\boldsymbol{J}^{T}\boldsymbol{J})^{-1}\boldsymbol{J}^{T}r\label{eq:guass_newton}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1524112053
Gauss-Newton starts with the initial guess for 
\begin_inset Formula $x_{t}$
\end_inset

and it iteratively sloves for 
\begin_inset Formula $x_{t+1}$
\end_inset

 untill the minimum is approached,  and this method does not need the computatio
n of second derivatives (Hessian matrix) of the of function, which is often
 expensive and sometimes not possible to compute, instead the Hessian is
 approximated with the Jacobian matrix of the function as show in equaition
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:hassian_from_jacobian"

\end_inset

.
 
\end_layout

\begin_layout Standard

\change_deleted 215191885 1521913213
T
\change_inserted 215191885 1524112159
T
\change_unchanged
he modification to Gauss-Newton method is 
\change_inserted 215191885 1521907843
t
\change_deleted 215191885 1521907843
T
\change_unchanged
he Levenberg-Marquardt 
\change_inserted 215191885 1521913445
(LVM)
\begin_inset CommandInset citation
LatexCommand cite
key "Gavin2013TheLM"

\end_inset

 
\change_unchanged
algorithm, 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
x_{t+1}=x_{t}-((\boldsymbol{J}^{T}\boldsymbol{J})+\rho\boldsymbol{I})^{-1}\boldsymbol{J}^{T}r\label{eq:lvm}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
this 
\change_deleted 215191885 1523717970
modification
\change_unchanged
 is a hybrid between the 
\change_inserted 215191885 1521912344
Gauss-
\change_unchanged
Newton
\change_inserted 215191885 1524112828
 and gradient decent
\change_unchanged
 method
\change_inserted 215191885 1524112992
s.
 For small value of
\change_unchanged
 
\begin_inset Formula $\rho\simeq0$
\end_inset


\change_inserted 215191885 1524113023
, the equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:lvm"

\end_inset

 is equivalent to the Gauss-Newton method as represented in the equation
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:guass_newton"

\end_inset

,
\change_unchanged
 
\change_inserted 215191885 1524114660
and for the large value of 
\begin_inset Formula $\rho$
\end_inset

, the identity matrix in equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:lvm"

\end_inset

 dominates the Hessian matrix 
\begin_inset Formula $H$
\end_inset

 and the equation behaves as a gradient decent, similar to equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:gradiant_decent"

\end_inset

 with the 
\change_deleted 215191885 1524113204
and a gradient descent 
\change_unchanged
step size 
\change_inserted 215191885 1524113223
of 
\change_unchanged

\begin_inset Formula $\alpha=1/\rho$
\end_inset


\change_inserted 215191885 1524113240
.

\change_unchanged
 
\change_deleted 215191885 1524113213
for 
\begin_inset Formula $\lambda\longrightarrow\infty$
\end_inset


\change_inserted 215191885 1524113535
 LVM efficiently minimizes the squared error by updating the parameters
 in the steepest-descent direction when the initial guess are far way from
 the solution, and switches to Gauss-Newton method when small updates are
 required for approximation.
 The parameter 
\begin_inset Formula $\rho$
\end_inset

 is called a damping parameter, which was heuristically selected and adjusted
 every iteration based the previous error, and this process is also called
 as the damped least-squares approximation.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1524746946
Further, Marquardt suggested that the convergence can be achieved quicker
 if there is larger movement along the directions where the gradient is
 smaller.
 To achieve this he replaced the identity matrix 
\begin_inset Formula $I$
\end_inset

 in the equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:lvm"

\end_inset

 with diagonal values of 
\begin_inset Formula $J^{T}J$
\end_inset

, with the intuition of scaling each component of the gradient according
 to the curvature.
 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1524114000
\begin_inset Formula 
\[
x_{t+1}=x_{t}-((\boldsymbol{J}^{T}\boldsymbol{J})+\rho(diag(\boldsymbol{J}^{T}\boldsymbol{J})))^{-1}\boldsymbol{J}^{T}r
\]

\end_inset


\change_unchanged

\end_layout

\begin_layout Section

\change_inserted 215191885 1523673072
Visual Odometer and Graph Optimization
\begin_inset CommandInset label
LatexCommand label
name "subsec:Visual-Odometer"

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1523984799
Visual odometer is the process of estimating the camera trajectory from
 the stream of images.
 For every pair of images, we can estimate the depth as discussed in section
 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Estimation-of-the-depth"

\end_inset

, and consequently use this depth for the pose estimation from following
 frames.
 This process of pose and depth estimation is repeated for every new image,
 and depth is continuously propagated and refined as discussed in section
 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Depth-propagation"

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1523984819
This is an nonlinear least square optimization problem, which tries to estimate
 the set of parameters that accurately represents the trajectory of the
 camera and observed 3D points of the object.
 The complexity of this problem can increases quickly since we are dealing
 with multiple features from many pairs of images and each pose estimation
 has a 7 degree of freedom 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:sim3"

\end_inset

.
 One efficient way to handle such a large-scale nonlinear optimization problem
 is to represent them in a graph
\begin_inset CommandInset citation
LatexCommand cite
key "5979949"

\end_inset

.
 There are two many advantages to this approach; Firstly, the graph representati
on helps with the modular representation of the problem, where each node
 is reference image and an edge between the nodes is a relative pose estimated.
 And more importantly, graph representation helps handle other essential
 issues like global optimization and loop closures.
\end_layout

\begin_layout Chapter

\change_inserted 215191885 1521048998
LSD SLAM
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521048998
We are using LSD-SLAM 
\begin_inset CommandInset citation
LatexCommand cite
key "Engel2014"

\end_inset

 as our SfM implementation, it is  a 
\emph on
semi-dense
\emph default
, 
\emph on
direct
\emph default
 method which optimizes the geometry directly on the image intensities.
 LSD-SLAM provides as output a reconstruction of the observed 3D environment
 as a pose-graph of specially designated RGB image keyframes with associated
 semi-dense depth images.
 Direct correspondences are found between every RGB frame and each RGB keyframe
 to estimate the keyframe-to-RGB-frame relative camera pose.
 This is achieved by Levenberg-Marquardt optimization of the photometric
 error between the RGB image pair.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1524196388
\begin_inset Formula 
\begin{equation}
E(\xi)=\underset{i}{\sum}(I_{ref}(p_{i})-I(\pi(p_{i},D_{ref}(p_{i}),\boldsymbol{\xi})))^{2}\label{eq:Photomatric Error}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1524680600
Equation (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Photomatric Error"

\end_inset

) shows the specific image alignment object function and formalizes the
 form for equation (
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Image-alignment"

\end_inset

).
 Here, the warp function relies on the current estimate of depth 
\begin_inset Formula $D_{ref}$
\end_inset

 to determine the relative pose 
\begin_inset Formula $\boldsymbol{\xi}\in sim(3)$
\end_inset

.
 The reference depth 
\begin_inset Formula $D_{ref}$
\end_inset

 is the depth associated with current key frame, these depth values could
 be initialized either with random values or with the depth measurements
 from a RGBD sensor to initiate the process.
 
\begin_inset Formula $\pi$
\end_inset

 is the projection of the three dimensional point cloud to two dimensional
 pixel coordinates and it depends on the camera calibration.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1524197453
We can apply Guass-Newton optimization, as discussed in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Non-linear-Optimization"

\end_inset

, to find optimal 
\begin_inset Formula $\xi$
\end_inset

 which minimizes the equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Photomatric Error"

\end_inset

, to do that we need to compute the Jacobian of the residual as required
 by the equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:guass_newton"

\end_inset

.
 The jacobian of 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Photomatric Error"

\end_inset

 with respect to 
\begin_inset Formula $\xi$
\end_inset

 is
\end_layout

\begin_layout Standard

\change_inserted 215191885 1524199415
\begin_inset Formula 
\begin{equation}
J_{r}=\frac{1}{z^{\prime}}\left(\begin{array}{cc}
\nabla I_{x}f_{x} & \nabla I_{y}f_{y}\end{array}\right)\left(\begin{array}{cccccc}
1 & 0 & -\frac{x^{\prime}}{z^{\prime}} & -\frac{x^{\prime}y^{\prime}}{z^{\prime}} & (z^{\prime}+\frac{x^{\prime2}}{z^{\prime}}) & -y^{\prime}\\
0 & 1 & -\frac{y^{\prime}}{z^{\prime}} & -(z^{\prime}+\frac{y^{\prime2}}{z^{\prime}}) & -\frac{x^{\prime}y^{\prime}}{z^{\prime}} & x^{\prime}
\end{array}\right)\label{eq:jacobian_of_photomatric_error}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1524199848
\begin_inset Formula $x^{\prime}$
\end_inset

,
\begin_inset Formula $y^{\prime}$
\end_inset

and 
\begin_inset Formula $z^{\prime}$
\end_inset

 are the warped three dimensional points before projection
\change_unchanged

\end_layout

\begin_layout Standard

\change_inserted 215191885 1524199858
\begin_inset Formula 
\[
\left(\begin{array}{c}
x^{\prime}\\
y^{\prime}\\
z^{\prime}
\end{array}\right)=\boldsymbol{R}_{\xi}\boldsymbol{K}^{-1}\left(\begin{array}{c}
x\\
y\\
d
\end{array}\right)+\boldsymbol{T}_{\xi}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1524200063
here, 
\begin_inset Formula $d$
\end_inset

 is the reference depth at the pixel 
\begin_inset Formula $(x,y)$
\end_inset

.
 
\begin_inset Formula $f_{x}$
\end_inset

, 
\begin_inset Formula $f_{y}$
\end_inset

 are the focal lengths of the camera and 
\begin_inset Formula $K$
\end_inset

 is the intrinsic matrix of the camera.
 
\begin_inset Formula $\nabla I_{x}$
\end_inset

 and 
\begin_inset Formula $\nabla I_{y}$
\end_inset

 are the image gradients.
 
\change_unchanged

\end_layout

\begin_layout Standard

\change_inserted 215191885 1524196952
For every new frame tracked, the depths associated with the keyframe are
 continuously refined by performing the adaptive baseline stereo 3D reconstructi
on 
\begin_inset CommandInset citation
LatexCommand cite
key "6751290"

\end_inset

 between new tracked frames and the stack of previously tracked frames as
 discussed in the section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Estimation-of-the-depth"

\end_inset

.
 When there is drastic pose change between the tracked frame and keyframe,
 the current tracked frame is promoted as a keyframe and depth map from
 previous keyframe is propagated to the new keyframe and regularized.
 Concurrently, all the keyframes are added as a nodes to a pose graph that
 stores the relative pose between the keyframes as edges/constraints 
\begin_inset CommandInset citation
LatexCommand cite
key "5979949"

\end_inset

.
 The pose graph stores the global trajectory of the camera in the 3D scene
 and an optimization algorithm processes the pose graph to improve the camera
 pose estimates as new image correspondences are found by image matching
 and loop closures.
 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1523987380
The depth computation process involves finding the epipolar lines between
 new tracking frame and reference frames, along which the disparity for
 the pixel is determined.
 This process have two error sources, the error on disparity itself called
 the geometric disparity error and the error which encodes intensity difference
 called the photometric disparity error, the former error accounts for the
 magnitude of the image gradient along the epipolar line , while the later
 one on the angle between the image gradient and the epipolar line.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1523989809
We know from the section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Stereo-corr-and-disparity-est"

\end_inset

 that the depths are estimated with high fidelity for the regions having
 a large gradient change.
 This claim is further backed by the outputs of LSD-SLAM, we can see that
 in the figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:lsd-output-2"

\end_inset

, that the depths are estimated for the rather plane wall, because of the
 presence of posters on wall, which introduce the gradient change.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1523988642
\begin_inset Float figure
wide true
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521759399
\begin_inset Graphics
	filename images/LSD SLAM.png
	width 15cm

\end_inset


\end_layout

\begin_layout Plain Layout

\change_inserted 215191885 1521759399
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1523985358
\begin_inset Argument 1
status open

\begin_layout Plain Layout

\change_inserted 215191885 1523985369
An overview of the LSD SLAM algorithm
\change_unchanged

\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:LSDSLAM-Overview"

\end_inset

 An overview of the LSD SLAM algorithm.
 Each incoming frame is tracked against the current keyframe.
 If it does not satisfy the criteria for new keyframe creation, it is used
 along with previous tracked frames for refinement of the estimated depth
 values of the keyframe.
 Otherwise, the frame is considered a new keyframe and depth estimates from
 the previous keyframe are propagated and used for initialization of the
 new depth estimates.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1523988644
\begin_inset Float figure
wide true
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1523988644
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1523988677
\begin_inset Graphics
	filename images/exp1/scaled_lsd_depth.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout

\change_inserted 215191885 1523988644
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1523988644
\begin_inset CommandInset label
LatexCommand label
name "fig:lsd-output-1"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1523988681
\begin_inset Graphics
	filename images/exp2/scaled_lsd.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout

\change_inserted 215191885 1523988644
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1523988644
\begin_inset CommandInset label
LatexCommand label
name "fig:lsd-output-2"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1523988683
\begin_inset Graphics
	filename images/exp3/scaled_Depth.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout

\change_inserted 215191885 1523988644
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1523988644
\begin_inset CommandInset label
LatexCommand label
name "fig:lsd-output-3"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1523988644
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1523989999
\begin_inset Argument 1
status open

\begin_layout Plain Layout

\change_inserted 215191885 1523988714
Outputs of LSD SLAM
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:LSD_SLAM outputs"

\end_inset

The outputs of the LSD SLAM (a) shows the good depth estimation for corner
 with rich pattern.
 (b) shows the good depth estimation along the edges of door and edges of
 poster.
 (c) shows the good depth estimation along the edges of table and chairs.
\end_layout

\end_inset


\end_layout

\end_inset


\change_unchanged

\end_layout

\begin_layout Section

\change_inserted 215191885 1521070779
Point Cloud Reconstruction
\begin_inset CommandInset label
LatexCommand label
name "subsec:Point-Cloud-Reconstruction"

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521070779
Measured 3D 
\begin_inset Formula $(X,Y,Z)$
\end_inset

 positions of sensed surfaces can be directly computed from the intrinsic
 camera parameters and depth image values.
 Here, the 
\begin_inset Formula $Z$
\end_inset

 coordinate is directly taken as the depth value and the 3D 
\begin_inset Formula $(X,Y)$
\end_inset

 coordinates are computed using the pinhole camera model.
 In a typical pinhole camera model 3D 
\begin_inset Formula $(X,Y,Z)$
\end_inset

 points are projected to 
\begin_inset Formula $(x,y)$
\end_inset

 image locations 
\begin_inset CommandInset citation
LatexCommand cite
key "Ma:2003:IVI:971144"

\end_inset

, e.g., for the image columns the 
\begin_inset Formula $x$
\end_inset

 image coordinate is 
\begin_inset Formula $x=f_{x}\frac{X}{Z}+c_{x}-\delta_{x}$
\end_inset

.
 However, for a depth image, this equation is re-organized to 
\begin_inset Quotes eld
\end_inset

back-project
\begin_inset Quotes erd
\end_inset

 the depth into the 3D scene and recover the 3D 
\begin_inset Formula $(X,Y)$
\end_inset

 coordinates as shown by equation (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:RGBD_reconstruction"

\end_inset

) 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521070779
\begin_inset Formula 
\begin{equation}
\begin{array}{ccc}
X & = & (x+\delta_{x}-o_{x})Z/f_{x}\\
Y & = & (y+\delta_{y}-o_{y})Z/f_{y}\\
Z & = & Z
\end{array}\label{eq:RGBD_reconstruction}
\end{equation}

\end_inset

where 
\begin_inset Formula $Z$
\end_inset

 denotes the sensed depth at image position 
\begin_inset Formula $(x,y)$
\end_inset

, 
\begin_inset Formula $(f_{x},f_{y})$
\end_inset

 denotes the camera focal length (in pixels), 
\begin_inset Formula $(o_{x},o_{y})$
\end_inset

 denotes the pixel coordinate of the image center, i.e., the principal point,
 and 
\begin_inset Formula $(\delta_{x},\delta_{y})$
\end_inset

 denote adjustments of the projected pixel coordinate to correct for camera
 lens distortion.
\end_layout

\begin_layout Section

\change_inserted 215191885 1521070779
Point Cloud Re-Projection
\begin_inset CommandInset label
LatexCommand label
name "subsec:Point-Cloud-Re-Projection"

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521070779
Depth images can be simulated for camera sensor in arbitrary poses by 
\begin_inset Quotes eld
\end_inset

re-projection.
\begin_inset Quotes erd
\end_inset

 For discussion, assume that depth image 
\begin_inset Formula $\mathbf{d}(x,y)$
\end_inset

 has been recorded in the 
\begin_inset Quotes eld
\end_inset

standard
\begin_inset Quotes erd
\end_inset

 camera/optical coordinate system where the origin corresponds to the camera
 focal point, the 
\begin_inset Formula $z$
\end_inset

-axis corresponds to the depth/optical axis extending out into the viewed
 scene, the 
\begin_inset Formula $x$
\end_inset

-axis points towards the right and spans the image columns and the 
\begin_inset Formula $y$
\end_inset

-axis points downward and spans the image rows.
 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521070779
Let 
\begin_inset Formula $\mathbf{R}$
\end_inset

 denote the 3D rotation that rotates the coordinate axes of the standard
 coordinate system to align with the same axes of a second camera having
 arbitrary pose.
 Similarly, Let 
\begin_inset Formula $\mathbf{t}$
\end_inset

 denote the 3D translation vector describing the position of the focal point
 of a second camera having arbitrary pose.
 Using this notation, the re-projection algorithm consists of the following
 three steps:
\end_layout

\begin_layout Enumerate

\change_inserted 215191885 1521070779
Back-project 
\begin_inset Formula $\mathbf{d}(x,y)$
\end_inset

 to create an 
\begin_inset Formula $(X,Y,Z)$
\end_inset

 point cloud (as described in 
\begin_inset Formula $\S$
\end_inset


\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Point-Cloud-Reconstruction"

\end_inset

), 
\end_layout

\begin_layout Enumerate

\change_inserted 215191885 1522689207
Transform, i.e., rotate, translate and scale, each point 
\begin_inset Formula $\mathbf{p}_{i}=[X,Y,Z]^{t}$
\end_inset

 in the point cloud to generate a new point 
\begin_inset Formula $\mathbf{p}_{i}^{'}=[X^{'},Y^{'},Z^{'}]^{t}$
\end_inset

 that lies in a standard optical coordinate system centered on the second
 camera's focal point and having orientation that aligns with corresponding
 
\begin_inset Formula $x,y,z$
\end_inset

-axes using equation (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:coordinate_system_change"

\end_inset

),
\begin_inset Formula 
\begin{equation}
\mathbf{p}_{i}^{'}=\mathbf{R}*s*(\mathbf{p}_{i}-\mathbf{t})\label{eq:coordinate_system_change}
\end{equation}

\end_inset


\end_layout

\begin_layout Enumerate

\change_inserted 215191885 1521070779
Re-project the 
\begin_inset Formula $(X,Y,Z)$
\end_inset

 point cloud using the pinhole camera equations to compute the new depth
 image 
\begin_inset Formula $\mathbf{d}^{'}(x,y)=Z$
\end_inset

 using equation (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:re_projection"

\end_inset

).
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521070779
\begin_inset Formula 
\begin{equation}
\begin{array}{ccc}
x & = & f_{x}(\frac{X^{'}}{Z^{'}})-\delta_{x}+o_{x}\\
y & = & f_{y}(\frac{Y^{'}}{Z^{'}})-\delta_{y}+o_{y}\\
Z & = & Z^{'}
\end{array}\label{eq:re_projection}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521070779
Typically, the re-projected point cloud measurements fall at non-integer
 locations in the new depth image and the values of 
\begin_inset Formula $\mathbf{d}^{'}(x,y)$
\end_inset

 must then be interpolated via bilinear interpolation or some other interpolatio
n scheme (nearest neighbor).
 
\end_layout

\begin_layout Chapter

\change_inserted 215191885 1521071426
Methodology
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521071427
\begin_inset Float figure
wide true
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521071427
\begin_inset Graphics
	filename images/overview_coloum_span.png
	lyxscale 20
	width 6.5in
	height 2.5in

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1521071427
\begin_inset CommandInset label
LatexCommand label
name "fig:The-complete-process"

\end_inset

An overview of the proposed depth fusion algorithm.
\end_layout

\end_inset


\end_layout

\end_inset

The proposed fusion approach applies the semi-dense monocular reconstruction
 approach referred to as Large Scale Direct (LSD) SLAM 
\begin_inset CommandInset citation
LatexCommand cite
key "Engel2014"

\end_inset

.
 The LSD-SLAM algorithm solves the SfM problem using a 
\emph on
direct
\emph default
 method to compute pixel correspondences and a 
\emph on
semi-dense
\emph default
 method for 3D reconstruction.
 We select this approach as it does not require or impose any prior knowledge
 about the scene structure as required by 
\emph on
dense 
\emph default
reconstruction methods and it gives more 3D estimates than 
\emph on
sparse
\emph default
 approaches while having similar computational cost.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521071427
The LSD-SLAM SfM algorithm consists of the following three components:
\end_layout

\begin_layout Enumerate

\change_inserted 215191885 1521071427
A tracking component that estimates the pose of the camera
\end_layout

\begin_layout Enumerate

\change_inserted 215191885 1521071427
A depth map estimation component that estimates semi-dense depth images
 for keyframes
\end_layout

\begin_layout Enumerate

\change_inserted 215191885 1521071427
A map optimization component that seeks to create a 3D map of the environment
 that is self-consistent.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521994424
This work utilizes the first two components to explore fusion of RGBD depth
 images with SfM depth images.
 For this work, the map optimization component (3) is not used.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:The-complete-process"

\end_inset

 depicts an overview of the proposed depth fusion algorithm.
\end_layout

\begin_layout Section

\change_inserted 215191885 1521071427
Time and Spatial Sampling Issues
\begin_inset CommandInset label
LatexCommand label
name "subsec:Time-and-Spatial"

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521071427
As mentioned previously, RGBD sensors measure depth at a rate of 30 frames
 per second (fps) and LSD-SLAM computes depth images only for 
\emph on
keyframes 
\emph default
which is a sparse subset of the measured RGB frames.
 Further, keyframes are not generated uniformly in time but created when
 the SfM algorithm detects criteria required to create a new keyframe.
 This condition is triggered when the current camera pose is too far from
 the most recent keyframe camera pose and when the current frame tracking
 result is 
\begin_inset Quotes eld
\end_inset

good
\begin_inset Quotes erd
\end_inset

 in the sense that the image warping correspondence objective function suggests
 an accurate or low-error result.
 As a result, SfM-estimated depths exist only for those RGB images designated
 as keyframes.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521071427
Further, the spatial distribution of SfM-estimated depths within SfM keyframes
 are localized to only those pixels having 
\begin_inset Quotes eld
\end_inset

good
\begin_inset Quotes erd
\end_inset

 3D reconstruction characteristics.
 In this sense, the quality of the depth estimate depends on accurately
 matching pixels along epipolar lines inscribed in the image.
 The matching performance here is best when there is a significant change
 in the image intensities along the epipolar line.
 Hence, 3D depth reconstruction is limited to those pixels that lie at sharp
 intensity changes, i.e., 
\begin_inset Quotes eld
\end_inset

edge
\begin_inset Quotes erd
\end_inset

 pixels, and further limited to those 
\begin_inset Quotes eld
\end_inset

edge
\begin_inset Quotes erd
\end_inset

 pixels that lie on edges that are roughly perpendicular to the direction
 of the epipolar line (see 
\begin_inset CommandInset citation
LatexCommand cite
key "6751290"

\end_inset

 for details).
 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521071427
The LSD-SLAM algorithm estimates depth at 
\begin_inset Quotes eld
\end_inset

good
\begin_inset Quotes erd
\end_inset

 pixel positions as a 1-dimensional Gaussian distribution specified as a
 mean image 
\begin_inset Formula $\mu_{SfM}(x,y)$
\end_inset

, i.e., the estimated depth image, and a variance image 
\begin_inset Formula $\sigma_{SfM}^{2}(x,y)$
\end_inset

 such that the RGB keyframe pixel at location 
\begin_inset Formula $\mathbf{I}(x,y)$
\end_inset

 is estimated to have depth 
\begin_inset Formula $\mu_{SfM}(x,y)$
\end_inset

 with uncertainties given by 
\begin_inset Formula $\sigma_{SfM}^{2}(x,y)$
\end_inset

.
 In this sense, the keyframe image 
\begin_inset Formula $\mathbf{I}(x,y)$
\end_inset

 augmented with the estimated depth image 
\begin_inset Formula $\mu_{SfM}(x,y)$
\end_inset

 is analogous in format to sensed RGBD image data.
 Yet, the uncertainties for the image 
\begin_inset Formula $\mu_{SfM}(x,y)$
\end_inset

 are given by the image 
\begin_inset Formula $\sigma_{SfM}^{2}(x,y)$
\end_inset

 rather than the experimentally validated uncertainties discussed in 
\begin_inset ERT
status open

\begin_layout Plain Layout

\change_inserted 215191885 1521071427


\backslash
S
\end_layout

\end_inset


\begin_inset space ~
\end_inset

 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:RGBD_Measurement-Noise"

\end_inset

.
\end_layout

\begin_layout Section

\change_inserted 215191885 1521071427
Image Registration Issues
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521071427
Fusing depth measurements requires knowledge of the correspondence between
 the depth measurements generated from the RGBD sensor and the SfM algorithm.
 For SfM keyframes this correspondence is trivial due to the fact that RGBD
 sensors support hardware registration.
 Hardware registration co-locates the RGBD depth image measurements, 
\begin_inset Formula $\mathbf{d}_{rgbd}(x,y)$
\end_inset

, and RGB appearance values, 
\begin_inset Formula $\mathbf{I}(x,y)$
\end_inset

.
 Hence, for hardware-registered RGBD depth images, 
\begin_inset Formula $\mathbf{d}_{rgbd}(x,y)$
\end_inset

 is the measured depth of the surface having RGB pixel 
\begin_inset Formula $\mathbf{I}(x,y)$
\end_inset

.
 Similarly, SfM-estimated depths for an RGB keyframe, 
\begin_inset Formula $\mu d_{SfM}(x,y)$
\end_inset

, are the depths for the surface having RGB pixel 
\begin_inset Formula $\mathbf{I}(x,y)$
\end_inset

.
 Hence fusion is accomplished by fusing the measurements at corresponding
 
\begin_inset Formula $(x,y)$
\end_inset

 locations in the RGBD depth image, 
\begin_inset Formula $\mathbf{d}_{rgbd}(x,y)$
\end_inset

, and the SfM depth image, 
\begin_inset Formula $\mu\mathbf{d}_{SfM}(x,y)$
\end_inset

.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521479929
Depth correspondences for RGB images that are not SfM keyframes must be
 computed from one or more SfM keyframe depth images.
 This article uses the most recent, i.e., closest-in-time, keyframe to generate
 co-registered SfM depth images for arbitrary RGB images.
 To do so, the depth image from the most recent keyframe, 
\begin_inset Formula $\mathbf{d}_{SfM}(x,y)$
\end_inset

, is 
\begin_inset Quotes eld
\end_inset

back-projected
\begin_inset Quotes erd
\end_inset

 to create a 3D point cloud of SfM measurements.
 Using the estimated keyframe-to-camera pose change, the 3D measurements
 are then re-projected into the RGB camera image plane using the 3D projection
 equations for the camera provided via camera calibration.
 The resulting depth image, 
\begin_inset Formula $\widetilde{\mathbf{d}}_{SfM}(x,y)$
\end_inset

 is then co-registered with the RGBD depth image 
\begin_inset Formula $\mathbf{d}_{rgbd}(x,y)$
\end_inset

 and RGB image 
\begin_inset Formula $\mathbf{I}(x,y)$
\end_inset

.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1523721651
However, due to the noise in depth computation and consequent trajectory
 estimation, the depth registration is not always exact, this should be
 addressed as early as possible to avoid the drift in their computations.
 We try to achieve better depth registration by estimating the transformation
 error between the point clouds of sensor depth measurements and LSD depth
 estimation.
 Since the LSD estimations are not to the scale, we also need to estimate
 the scale for better registration of depths.
 The Umeyama's Least-Square estimation 
\begin_inset CommandInset citation
LatexCommand cite
key "Umeyama:1991:LET:105514.105525"

\end_inset

 shows us how to estimate these 
\begin_inset Formula $Sim(3)$
\end_inset

 transformation parameters efficiently.
 In his work, Umeyama shows how to estimates the rotation, translation and
 scale between two point clouds with known correspondence.
 In order to avoid errors, we use only the valid depths in both the point
 cloud for estimation.By using 
\begin_inset Formula $\mathbf{pc}_{rgbd}$
\end_inset

and 
\begin_inset Formula $\mathbf{pc}_{SfM}$
\end_inset

to represent the valid point clouds of sensor measurements and LSD estimation
 respectively, we compute the mean, variances and covariance of both them
 as following.
\change_unchanged

\end_layout

\begin_layout Standard

\change_inserted 215191885 1521484409
\begin_inset Formula 
\[
\mu_{rgbd}=\frac{\sum\mathbf{pc}_{rgbd}}{n}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521484766
\begin_inset Formula 
\[
\mu_{SfM}=\frac{\sum\boldsymbol{pc}_{SfM}}{n}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521484766
\begin_inset Formula 
\[
\sigma_{rgbd}^{2}=\frac{\sum\left\Vert \boldsymbol{pc}_{rgbd}-\mu_{rgbd}\right\Vert ^{2}}{n}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521484795
\begin_inset Formula 
\[
\sigma_{SfM}^{2}=\frac{\sum\left\Vert \boldsymbol{pc}_{SfM}-\mu_{SfM}\right\Vert ^{2}}{n}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521501433
\begin_inset Formula 
\[
\Sigma_{SfM,rgbd}=\frac{\sum(\boldsymbol{pc}_{rgbd}-\mu_{rgbd})(\boldsymbol{pc}_{SfM}-\mu_{SfM})^{T}}{n}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521501595
With singular value decomposition of 
\begin_inset Formula $\Sigma_{SfM,rgbd}$
\end_inset

 as 
\begin_inset Formula $UDV^{T},$
\end_inset

the optimum rotation matrix 
\begin_inset Formula $R$
\end_inset

 which achieves the minimum error between point clouds is given by
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521501637
\begin_inset Formula 
\[
\boldsymbol{R}=\boldsymbol{U}\boldsymbol{S}\boldsymbol{V}^{T}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521507419
\begin_inset Formula 
\[
\boldsymbol{S}=\boldsymbol{I}\begin{cases}
I & det(\Sigma_{SfM,rgbd})\geq0\\
diag(1,1,-1) & det(\Sigma_{SfM,rgbd})<0
\end{cases}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521507451
The scale adjustment 
\begin_inset Formula $c$
\end_inset

 is computed by
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521507699
\begin_inset Formula 
\[
c=\frac{1}{\sigma_{SfM}^{2}}trace(\boldsymbol{D}\boldsymbol{S})
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521507819
With scale and rotation established, the translation parameters are determined
 as 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521508665
\begin_inset Formula 
\[
\boldsymbol{t}=\mu_{rgbd}-c\boldsymbol{R}\mu_{SfM}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521917864
with the 
\begin_inset Formula $Sim(3)$
\end_inset

 parameters we transform LSD point clouds before projecting them to find
 
\begin_inset Formula $\widetilde{\mathbf{d}}_{SfM}(x,y)$
\end_inset

 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521514011
\begin_inset Formula 
\[
\tilde{\boldsymbol{pc_{SfM}}}=c\boldsymbol{R}\boldsymbol{pc}_{SfM}+\boldsymbol{T}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521818294
Using these techniques co-registered SfM depth images can be computed for
 a general RGBD image.
 When RGBD images correspond to SfM keyframes the registration is 
\begin_inset Quotes eld
\end_inset

automatic,
\begin_inset Quotes erd
\end_inset

 i.e., no computation is necessary.
 In all other cases, a co-registered SfM depth image must be computed by
 reconstructing a 3D point cloud from a keyframe and then projecting the
 point cloud into the target RGB camera image using the estimated camera
 calibration and keyframe-to-camera-frame relative pose parameters.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521818294
\begin_inset Float figure
wide true
sideways false
status collapsed

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521818294
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521818294
\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "1in"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521818294
\begin_inset Graphics
	filename images/lsd_rgbd_pc_before_reg.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521818294
\begin_inset Graphics
	filename images/lsd_rgbd_pc_before_reg_2.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\change_inserted 215191885 1521818294
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1521818294

\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521818294
\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "1in"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521818294
\begin_inset Graphics
	filename images/lsd_rgbd_pc_after_reg.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521818294
\begin_inset Graphics
	filename images/lsd_rgbd_pc_after_reg_2.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\change_inserted 215191885 1521818294
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1521818294

\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521818294
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1523985399
\begin_inset Argument 1
status open

\begin_layout Plain Layout

\change_inserted 215191885 1523985420
Depth registration
\change_unchanged

\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:depth registartion"

\end_inset

 (a) shows the two different views of the point clouds before depth registration.
 (b) shows the different views of the point clouds after depth registration.
 The green represents the point cloud from LSD SLAM and purple represents
 the point clouds from RGBD sensors
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section

\change_inserted 215191885 1521853647
Noise removal
\end_layout

\begin_layout Standard

\change_inserted 215191885 1523724556
Noise in the image and other inaccuracies in nonlinear optimization leads
 to erroneous depth computation; these outlier depth values should be detected
 and removed to avoid further drift in pose estimation and consequent accumulati
on of the error in depth estimations.
 Fortunately, these outliers have a particular pattern as shown in the 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:vailing_error_a"

\end_inset

 (a), i.e., if we consider the principle point as an origin, the outlier depths
 form a veiling pattern along the line from the origin to the furthest depth
 value.
 Also, we know that the most distant depth is the valid depth, we can find
 this true depth and eliminate all other noisy depth by tracing along the
 line to the origin.
 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1523994286
We can implement this by finding the line from principle point, 
\begin_inset Formula $(0,0,0)$
\end_inset

, to the furthest depth for every valid depth in depth map.
 From the equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:RGBD_reconstruction"

\end_inset

 which represents the point in three dimensional space, we can obtain the
 line 
\begin_inset Formula $(l_{x},l_{y},l_{z})$
\end_inset

 to principle point 
\begin_inset Formula $(0,0,0)$
\end_inset

 as:
\end_layout

\begin_layout Standard

\change_inserted 215191885 1523723455
\begin_inset Formula 
\begin{equation}
\begin{array}{ccc}
l_{x} & = & ((x+\delta_{x}-c_{x})Z/f_{x})-0\\
l_{y} & = & ((y+\delta_{y}-c_{y})Z/f_{y})-0\\
l_{z} & = & (Z-0)
\end{array}\label{eq:line_to_princlpe_point}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1523725035
the slope for the line is determined by:
\end_layout

\begin_layout Standard

\change_inserted 215191885 1523723478
\begin_inset Formula 
\begin{equation}
m=l_{y}/l_{x}\label{eq:slope_of_line}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1523994355
Since these depths have a veiling pattern in the z-axis, the outlier depths
 tend to have the same slope with small variance, and lower z value then
 the real depth.
 By traversing the line towards the origin, we find and eliminate all the
 outlier depths having the slope in the range of empirically determined
 threshold value.
 The experimental results are shown in the figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:vailing_error b"

\end_inset

, where the depth outliers are marked in black color, and result after the
 outlier elimination is shown if figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:vailing_error c"

\end_inset

.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521854479
\begin_inset Float figure
wide true
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521854805
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521854479
\begin_inset Graphics
	filename images/depth_filter_5.jpg
	lyxscale 10
	height 1.3in

\end_inset


\end_layout

\begin_layout Plain Layout

\change_inserted 215191885 1521854479
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1523722337
\begin_inset CommandInset label
LatexCommand label
name "fig:vailing_error_a"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521854479
\begin_inset Graphics
	filename images/depth_filter_7.jpg
	lyxscale 10
	height 1.3in

\end_inset


\end_layout

\begin_layout Plain Layout

\change_inserted 215191885 1521854479
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1523722346
\begin_inset CommandInset label
LatexCommand label
name "fig:vailing_error b"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521854805
\begin_inset Graphics
	filename images/depth_filter_6.jpg
	lyxscale 10
	height 1.3in

\end_inset


\end_layout

\begin_layout Plain Layout

\change_inserted 215191885 1521854805
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1523722358
\begin_inset CommandInset label
LatexCommand label
name "fig:vailing_error c"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1523985429
\begin_inset Argument 1
status open

\begin_layout Plain Layout

\change_inserted 215191885 1523985456
SFM outlier detection
\change_unchanged

\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:vailing error"

\end_inset

 (a) LSD depths with veiling errors.
 (b) Veiling errors detected (represented in black).
 (c) LSD depths with veiling errors removed.
 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section

\change_inserted 215191885 1521071427
Resolving the Unknown SfM Scale
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521071427
The methods described in previous sections detail how co-registered SfM
 depth measurements are computed for every sensed RGBD frame.
 However, as discussed in previously, SfM depth images intrinsically have
 an unknown scale, 
\begin_inset Formula $\alpha$
\end_inset

, which reflects the fact that the solution for the scene structure is not
 geometrically unique, i.e., the same scene structure can be observed at a
 infinite number of distinct scales.
 Therefore fusion requires the scale of the SfM depth image to fit the scale
 of the real-world scene measured by the RGBD camera.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521071427
Given that the depth measurements for the RGBD depth image are co-registered
 with the SfM estimated depth image the scale parameter can be directly
 estimated by minimizing the sum of the squared depth errors 
\begin_inset CommandInset citation
LatexCommand cite
key "Bishop:2006:PRM:1162264"

\end_inset

 between the SfM depth image and the RGBD depth image.
 Let 
\begin_inset Formula $V$
\end_inset

 denote the set of 
\begin_inset Formula $(x,y)$
\end_inset

 positions that have valid depth measurements for 
\begin_inset Quotes eld
\end_inset

standard
\begin_inset Quotes erd
\end_inset

 fusion as described in 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Fusing-RGBD-Depths"

\end_inset

.
 Equation (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:scale error"

\end_inset

) shows the error function used to compute the unknown scale value and equation
 (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:depth_scale_solution"

\end_inset

) shows the solution 
\begin_inset Formula $\widehat{\alpha}$
\end_inset

 that minimizes this error.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521071427
\begin_inset Formula 
\begin{equation}
e(\alpha)=\sum_{(x,y)\epsilon V}\left\Vert \mathbf{d}_{rgbd}(x,y)-\alpha\mathbf{d}_{SfM}(x,y)\right\Vert ^{2}\label{eq:scale error}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521071427
\begin_inset Formula 
\begin{equation}
\widehat{\alpha}=\sum_{(x,y)\epsilon V}\frac{\mathbf{d}{}_{rgbd}(x,y)}{\mathbf{d}_{SfM}(x,y)}\label{eq:depth_scale_solution}
\end{equation}

\end_inset


\end_layout

\begin_layout Section

\change_inserted 215191885 1521071427
RGBD and SfM Depth Fusion
\begin_inset CommandInset label
LatexCommand label
name "subsec:Fusing-RGBD-Depths"

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521071427
For fusing measurements we consider the structured-light measurement of
 the RGBD sensor to generate a distribution for the unknown true depth of
 the scene surfaces at each RGBD 
\begin_inset Formula $(x,y)$
\end_inset

 pixel in the depth image.
 These measurements are considered to be independent and identically distributed
 to the measurements of the true unknown depth of the scene surfaces from
 the registered SfM estimated depths.
 With these assumptions, solving the depth fusion problem is equivalent
 to estimating the posterior distribution of the true scene depth at each
 
\begin_inset Formula $(x,y)$
\end_inset

 position given the distributions for the RGBD and SfM depth values.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1524330119
Fortunately, previous sections show that Gaussian models are appropriate
 distributions for both the RGBD and SfM depth values and the parameters
 of these models are either known (see 
\begin_inset ERT
status open

\begin_layout Plain Layout

\change_inserted 215191885 1521071427


\backslash
S
\end_layout

\end_inset


\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:RGBD_Measurement-Noise"

\end_inset

) or estimated continuously (see 
\begin_inset ERT
status open

\begin_layout Plain Layout

\change_inserted 215191885 1521071427


\backslash
S
\end_layout

\end_inset


\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Time-and-Spatial"

\end_inset

).
 When both distributions are Gaussian, the posterior distribution can be
 found analytically and is a well-known result used in pattern recognition
 and other prediction frameworks, e.g., the Kalman filter as discussed in
 
\begin_inset CommandInset citation
LatexCommand cite
key "Maybeck79stochasticsmodels"

\end_inset

.
 Specifically, let the Gaussian noise for RGBD depth at position 
\begin_inset Formula $(x,y)$
\end_inset

 be represented as 
\begin_inset Formula $\mathcal{N}(d_{rgbd},\sigma_{rgbd}^{2})$
\end_inset

 and from the section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Stereo-corr-and-disparity-est"

\end_inset

 we have the Gaussian noise for the co-registered SfM depth image at position
 
\begin_inset Formula $(x,y)$
\end_inset

 as 
\begin_inset Formula $\mathcal{N}(d_{obs},\sigma_{d,obs}^{2})$
\end_inset

.
 The posterior distribution on the unknown true depth at position 
\begin_inset Formula $(x,y)$
\end_inset

 is also Gaussian and let 
\begin_inset Formula $\mathcal{N}(\mu_{fused},\sigma_{fused}^{2})$
\end_inset

 denote the mean and variance parameters of this distribution.
 Equations (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:fused_mean"

\end_inset

) and (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:fused_variance"

\end_inset

) provide optimal estimates of the mean and variance of the fused depth
 at position 
\begin_inset Formula $(x,y)$
\end_inset

.
 The best estimate of the fused depth is given by the highest probability
 value in the posterior distribution which is the mean fused image, 
\begin_inset Formula $\mu_{fused}(x,y)$
\end_inset

.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521071427
\begin_inset Formula 
\begin{equation}
\mu_{fused}=\frac{d_{rgbd}\sigma_{d,obs}^{2}+d_{obs}\sigma_{rgbd}^{2}}{\sigma_{d,obs}^{2}+\sigma_{rgbd}^{2}}\label{eq:fused_mean}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521514889
\begin_inset Formula 
\begin{equation}
\sigma_{fused}^{2}=\frac{\sigma_{rgbd}^{2}\sigma_{d,obs}^{2}}{\sigma_{rgbd}^{2}+\sigma_{d,obs}^{2}}\label{eq:fused_variance}
\end{equation}

\end_inset


\end_layout

\begin_layout Chapter

\change_inserted 215191885 1521515010
Results
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521515011
\begin_inset Float figure
wide true
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521515011
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521515011
\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "1in"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521515011
\begin_inset Graphics
	filename images/exp1/intensity_image.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521515011
\begin_inset Graphics
	filename images/exp2/intensity.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521516923
\begin_inset Graphics
	filename images/exp3/intenstiy.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521516923
\begin_inset Graphics
	filename images/in_to_out_with_transformation_error/intensity.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521516923
\begin_inset Graphics
	filename images/indoor_good_scale_and_transform/intensity.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\change_inserted 215191885 1521515011
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1521515011

\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521515011
\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "1in"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521515011
\begin_inset Graphics
	filename images/exp1/sensor_measurments.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521515011
\begin_inset Graphics
	filename images/exp2/sensor_measurments.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521516546
\begin_inset Graphics
	filename images/exp3/sensor_depth.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521516754
\begin_inset Graphics
	filename images/in_to_out_with_transformation_error/sensor_measurment.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521516754
\begin_inset Graphics
	filename images/indoor_good_scale_and_transform/sensor_depth.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\change_inserted 215191885 1521515011
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1521515011

\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521515011
\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "1in"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521515011
\begin_inset Graphics
	filename images/exp1/scaled_lsd_depth.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521515011
\begin_inset Graphics
	filename images/exp2/scaled_lsd.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521516530
\begin_inset Graphics
	filename images/exp3/scaled_Depth.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521516757
\begin_inset Graphics
	filename images/in_to_out_with_transformation_error/scaled_lsd_depth.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521516757
\begin_inset Graphics
	filename images/indoor_good_scale_and_transform/scaled_lsd.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\change_inserted 215191885 1521515011
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1521515011

\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521515011
\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "1in"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521515011
\begin_inset Graphics
	filename images/exp1/fused_depth_sets.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521515011
\begin_inset Graphics
	filename images/exp2/depth_Fusion_set.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521516534
\begin_inset Graphics
	filename images/exp3/sensor_depth_set.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521516764
\begin_inset Graphics
	filename images/in_to_out_with_transformation_error/fused_set.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521516764
\begin_inset Graphics
	filename images/indoor_good_scale_and_transform/fused_set.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\change_inserted 215191885 1521515011
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1521515011

\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521515011
\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "1in"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521515011
\begin_inset Graphics
	filename images/exp1/std_deviation_colormap.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521515011
\begin_inset Graphics
	filename images/exp2/std_deviation_colormap.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521516538
\begin_inset Graphics
	filename images/exp3/std_deviation_colmap.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521516771
\begin_inset Graphics
	filename images/in_to_out_with_transformation_error/std_dev_colMap.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521516771
\begin_inset Graphics
	filename images/indoor_good_scale_and_transform/std_deviation_colormap.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\change_inserted 215191885 1521515011
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1521515011

\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521515011
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1523985468
\begin_inset Argument 1
status open

\begin_layout Plain Layout

\change_inserted 215191885 1523985478
Results
\change_unchanged

\end_layout

\end_inset

Results for three experiments are shown.
 Images shown are organized into separate columns.
 Column (a) shows a grayscale image of the scene (b) shows the sensed RGBD
 depth image (c) shows the SfM-estimated depth image (d) shows the fused
 image and (e) shows the standard deviation for fused depths (in 
\begin_inset Formula $m.$
\end_inset

).
 The fused image has been color-coded as follows: (white) denotes depth
 locations sensed only by the RGBD sensor, (yellow) denotes depth locations
 only sensed via SfM, (red) denotes fused (RGBD+SfM) depth locations and
 (black) denotes depth locations without RGBD or SfM measurements.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1523299565
The experiments were conducted with different setups to test and analyze
 the depth fusion algorithm.
 Experiments recorded RGBD image sequenced from ORBBEC Astra RGBD sensors
 and applied LSD-SLAM to their image streams using their factory-provided
 intrinsic camera calibration parameters.
 Each experiment included approximately 60 seconds of RGBD image data at
 the rate of 30 fps.
 The recorded RGB images were processed offline by the LSD-SLAM algorithm
 to generate SfM depth images.
 Experiments initialize the LSD-SLAM algorithm with the first recorded depth
 image from the RGBD sensor to facilitate the initial scale approximation.
 The output from the LSD-SLAM algorithm consisting of the relative pose
 for every tracked frame and the depth map for each keyframe was then captured
 to disk.
 The fusion algorithm was then run offline on the recorded RGBD image stream
 and LSD-SLAM output files to generate the results shown in this section.
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521514993
\begin_inset Tabular
<lyxtabular version="3" rows="8" columns="4">
<features tabularvalignment="middle">
<column alignment="center" valignment="top" width="0pt">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell multirow="3" alignment="center" valignment="middle" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521514993

\size footnotesize
Experiment
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521514993

\size footnotesize
RGBD-only depths
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521514993

\size footnotesize
SfM-only depths
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521514993

\size footnotesize
Fused depths
\end_layout

\end_inset
</cell>
</row>
<row>
<cell multirow="4" alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521514993

\size footnotesize
(%)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521514993

\size footnotesize
(%)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521514993

\size footnotesize
(%) 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521514993
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521514993
67.7
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521514993
6.7
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521514993
25.5
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521514993
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521514993
55.5
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521514993
10.0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521514993
34.3
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521514993
3
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521514993
68.1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521514993
15.1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521514993
16.6
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521522076
4
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521522089
51.0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521522116
25.7
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521522139
23.1
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521522077
5
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521522195
68.3
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521522214
8.8
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521522249
22.8
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521514993

\size footnotesize
Average
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521522298
62.12
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521522330
13.26
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521522369
24.46
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout

\change_inserted 215191885 1521514993
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1521514993
Ratios of depth measurements to total depths
\begin_inset CommandInset label
LatexCommand label
name "tab:Ratios-of-depth"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521514993
Experiment 1 depicts a indoor office scene at the university.
 This scene includes specular and dark surface structures at close range
 that are not measured by the RGBD sensor.
 Yet, the SfM algorithm estimates depths at a number of locations (on the
 podium) where ther are significant intensity changes.
 These additional depths are evident in the fused results, which includes
 SfM-only depth measurements in regions in the vicinity of image edges.
 The experiment demonstrates that depth fusion can improve depth images
 by obtaining depths from surfaces not measurable by the RGBD sensor.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521519752
Experiment 2 depicts a hallway in the UNCC EPIC building that includes both
 specular surfaces and high intensity illumination from overhead lights.
 The experiment is a second example showing that depth fusion bolsters over
 all depth measurement performance by providing the depths when RGBD sensors
 fail.
 The SfM takes advantage of the patterns on the surface and estimates the
 depths irrespective of the nature of its reflectance properties and color.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521518425
Experiment 3 depicts the UNCC faculty conference hall.
 Here a number of scene structures lie beyond the measurement range of the
 RGBD sensor.
 Yet, the SfM algorithm is able to estimate the depth of these scene structures
 (albeit at high variance) providing depths that would otherwise not be
 possible.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521521717
Experiment 4 depicts indoor to outdoor transition.
 By initializing the LSD SLAM indoor, we get the scale estimated.
 We can see that in outdoor, when the sensor measurements fails because
 of the infrared ray flooding, depth estimates from LSD SLAM are used.
 Often because of IR flooding the sensor measurements gets corrupted, in
 those scenarios we can consider replacing entire corrupted depth sensor
 measurements by LSD depth estimation instead of fusion for better results.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521521903
Experiment 5 depicts indoor with well lit conditions, even though the sensors
 are supposed work completely fine in this scenario, there are rare chances
 of the them failing at steep surfaces.
 During such failures, depth fusion comes handy.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521596751
Table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:Ratios-of-depth"

\end_inset

 quantifies the amount of additional depth information provided via RGBD-SfM
 depth image fusion.
 When we average over the five experiments discussed approximately 62% of
 the fused depths originate from the RGBD sensor alone, approximately 13%
 original from the SfM algorithm alone, and approximately 24% of the fused
 depths results from RGBD-SfM fusion.
 The addition of 13% of novel depth data is a significant contribution.
 Further, fused depths account for roughly 24% of depth image data and the
 measurement error for all of these measurements will be reduced by the
 fusion.
 Variance reduction will be greatest for low-variance SfM depth estimates
 which are typically in textured scene locations close to the camera.
 Yet, we note that by inspection of equation (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:fused_variance"

\end_inset

), it is theoretically impossible for the variance of any fusion result
 to increase.
 Another obvious observation from the Table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:Ratios-of-depth"

\end_inset

 is that the SfM has a largest contribution in Experiment 4 (outdoor scenario)
 since sensor fails outdoors and it has least contribution in Experiment
 5 (well lit indoor) where sensor is at its best.
\end_layout

\begin_layout Chapter

\change_inserted 215191885 1521596763
Conclusion and Futher work
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521596763
The SfM depth estimation complements the RGBD sensor measurements and can
 provide depths when RGBD sensors fail.
 The depth fusion algorithm provides the effective way to augment the RGBD
 depth stream and results in improved depth images.
 The experiments conducted shows these improvements in the experimental
 results for a number of scenarios where RGBD sensors fail including successfull
y capturing depth for out-of-range RGBD depth locations and successfully
 capturing depth measurements from specular and dark objects.
 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521605145
As future work, the proposed depth fusion can be generalized to address
 depth image locations the include measurements having non-Gaussian noise
 distributions, especially for the outdoor scenario the error noise are
 not necessarily Gaussian.
 Also, the depth image gets corrupted by random speckles because of infrared
 ray flooding, these speckles could be wrongly treated as depth value, since
 there values are in valid depth range.
 Fortunately, it is observed that these speckles have particular pattern
 and a prepossessing logic could be implemented to get rid of them.
 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521605101
Also, experimental results show that the scale and trajectory estimation
 of SfM depths are not always accurate.
 Back-propagation of RGBD-SfM fused depth imagery into the SfM algorithm
 can be exploited to improve the camera pose estimation in the camera pose
 graph.
 We know that the fused depths have measurements where SfM fails and also
 they have lower variance than SfM's, propagating them to SfM leads to improveme
nt in estimated depth values, which leads to better trajectory computation
 for future frames.
 
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "references_db"
options "ieeetr"

\end_inset


\end_layout

\end_body
\end_document
