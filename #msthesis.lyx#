#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass UNCC-thesis
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package none
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 0
\use_package mathtools 1
\use_package mhchem 0
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 2
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes true
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\author 215191885 "Akash"
\author 1699939148 "arwillis"
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pagenumbering{roman}
\end_layout

\begin_layout Plain Layout


\backslash
fbmatterchapterformat
\end_layout

\begin_layout Plain Layout

% Doctype should be either dissertation proposal, dissertation, or thesis.
\end_layout

\begin_layout Plain Layout

% If you're getting a master's, specify "thesis" below.
  
\end_layout

\begin_layout Plain Layout

% If you're getting a PhD, specify "dissertation" below.
\end_layout

\begin_layout Plain Layout


\backslash
doctype{thesis}
\end_layout

\begin_layout Plain Layout

%%%%%%%%%%%%%%%%     IMPORTANT! IMPORTANT! IMPORTANT! %%%%%%%%%%%%%%%%
\end_layout

\begin_layout Plain Layout

% The rules below MUST be followed for the abstract page and chapter titles
\end_layout

\begin_layout Plain Layout

% to be correctly formatted.
\end_layout

\begin_layout Plain Layout

%
\end_layout

\begin_layout Plain Layout

% 1.
 Only the first letter of the entire title should be capitalized to allow
 the 
\end_layout

\begin_layout Plain Layout

%    title to appear as required by the graduate school on the Abstract
 page.
\end_layout

\begin_layout Plain Layout

% 2.
 Write chapter titles in ALL CAPS.
\end_layout

\begin_layout Plain Layout

%
\end_layout

\begin_layout Plain Layout


\backslash
title{
\change_deleted 215191885 1521733925
Structure From Motion
\change_inserted 215191885 1521733925
Structure-From-Motion and RGBD Depth Fusion
\change_unchanged
}
\end_layout

\begin_layout Plain Layout


\backslash
author{Akash Chandra Shekar}
\end_layout

\begin_layout Plain Layout


\backslash
degree{Master of Science}
\end_layout

\begin_layout Plain Layout


\backslash
major{Computer Science}
\end_layout

\begin_layout Plain Layout


\backslash
publicationyear{2018}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
advisor{Dr.
 Andrew Willis}
\change_inserted 215191885 1521524005

\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

% Add the full name and title of all your committee members,
\end_layout

\begin_layout Plain Layout

% apart from your advisor, one by one.
  The style file expects
\end_layout

\begin_layout Plain Layout

% 3 to 5 committee members in addition to your advisor.
\end_layout

\begin_layout Plain Layout

\change_inserted 215191885 1521524112

\end_layout

\begin_layout Plain Layout

\change_inserted 215191885 1521524116


\backslash
committeeMember{Dr.
 Min Shin}
\change_unchanged

\end_layout

\begin_layout Plain Layout


\backslash
committeeMember{Dr.
 
\change_deleted 215191885 1521523590
Min Shin
\change_inserted 215191885 1521523595
Srinivas Akella
\change_unchanged
}
\end_layout

\begin_layout Plain Layout


\backslash
committeeMember{Dr.
 
\change_deleted 215191885 1521523646
Jianping Fan
\change_inserted 215191885 1521523647
Hamed Tabkhi
\change_unchanged
}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

% Generate the preliminary title page and copyright page.
\end_layout

\begin_layout Plain Layout


\backslash
maketitlepage
\end_layout

\begin_layout Plain Layout


\backslash
makecopyright
\end_layout

\end_inset


\end_layout

\begin_layout Abstract

\change_deleted 215191885 1521734264
The Structure from 
\change_deleted 1699939148 1515786609
m
\change_deleted 215191885 1521734264
Motion (SfM) systems use knowledge of a camera's image formation model and
 a time-sequence of images from the camera to estimate camera motion and
 3D scene structure.
\change_deleted 1699939148 1515786717
involves two aspects, one is to track the camera
\change_deleted 215191885 1521734264
 Formulations of this problem decompose the problem into two sub-problems:
 
\change_deleted 1699939148 1515786757
trajectory by 
\change_deleted 215191885 1521734264
 (1) solving for
\change_deleted 1699939148 1515786772
ing
\change_deleted 215191885 1521734264
e 
\change_deleted 1699939148 1515786851
nonlinear
\change_deleted 215191885 1521734264
the camera trajectory 
\change_deleted 1699939148 1515786856
 equation
\change_deleted 215191885 1521734264
 and 
\change_deleted 1699939148 1515786861
other is to
\change_deleted 215191885 1521734264
solving for 
\change_deleted 1699939148 1515786868
 estimate
\change_deleted 215191885 1521734264
 the depth of 3D scene points observed by the camera in multiple images.
 Solutions are provided by 
\change_deleted 1699939148 1515786913
by solving
\change_deleted 215191885 1521734264
 finding 
\change_deleted 1699939148 1515786917
Stereo 
\change_deleted 215191885 1521734264
correspondences between 
\change_deleted 1699939148 1515786923
of sub-set of 
\change_deleted 215191885 1521734264

\begin_inset Formula $(x,y)$
\end_inset

 locations
\change_deleted 1699939148 1515787163
pixels
\change_deleted 215191885 1521734264
 in images take from different poses taken at distinct times in the camera
 trajectory.
 
\change_deleted 1699939148 1515786976
The t
\change_deleted 215191885 1521734264
Traditional SfM
\change_deleted 1699939148 1515786982
Structure From Motion
\change_deleted 215191885 1521734264
 
\change_deleted 1699939148 1515786986
functions by 
\change_deleted 215191885 1521734264
find
\change_deleted 1699939148 1515786988
ing
\change_deleted 215191885 1521734264
 these correspondences 
\change_deleted 1699939148 1515786994
between pair of images,
\change_deleted 215191885 1521734264
 by extracting and tracking 
\change_deleted 1699939148 1515786999
the 
\change_deleted 215191885 1521734264
features
\change_deleted 1699939148 1515787197
 like
\change_deleted 215191885 1521734264
, e.g., corner points, 
\change_deleted 1699939148 1515787229
lines etc
\change_deleted 215191885 1521734264
match local regions within images.
 However, these 
\change_deleted 1699939148 1515787268
features
\change_deleted 215191885 1521734264
approaches 
\change_deleted 1699939148 1515787017
do not necessarily 
\change_deleted 215191885 1521734264
provide very sparse geometric descriptions of the scene structure and omit
 much of
\change_deleted 1699939148 1515787046
all
\change_deleted 215191885 1521734264
 the geometric information 
\change_deleted 1699939148 1515787052
regarding
\change_deleted 215191885 1521734264
in the image.
 
\change_deleted 1699939148 1515787057
The 
\change_deleted 215191885 1521734264
Other 
\change_deleted 1699939148 1515787062
more accurate method
\change_deleted 215191885 1521734264
, semi-dense, approaches 
\change_deleted 1699939148 1515787072
is to 
\change_deleted 215191885 1521734264
attempt to find large collections of correspondences by solving for pixel-level
\change_deleted 1699939148 1515787101
use the pixel intensity itself
\change_deleted 215191885 1521734264
 correspondences which provide more geometric information about the scene.
 Unfortunately, unlike features, pixel values may are susceptible to viewpoint,
 illumination, and other image formation phenomena which requires semi-dense
 approaches to
\change_deleted 1699939148 1515787356
to minimize
\change_deleted 215191885 1521734264
 consider 
\change_deleted 1699939148 1515787363
the P
\change_deleted 215191885 1521734264
photometric 
\change_deleted 1699939148 1515787370
and
\change_deleted 215191885 1521734264
correction 
\change_deleted 1699939148 1515787380
Geometric errors, by defining the robust error functions.
\change_deleted 215191885 1521734264
in order to robustly determine correspondences.
 
\change_inserted 215191885 1521734267

\end_layout

\begin_layout Abstract

\change_inserted 215191885 1521734449
This article describes a technique to augment a typical RGBD sensor by integrati
ng depth estimates obtained via Structure-from-Motion (SfM) with depth measureme
nts from an RGBD sensor.
 Limitations in the RGBD depth sensing technology prevent capturing depth
 measurements in four important contexts: (1) distant surfaces (>5m), (2)
 dark surfaces, (3) brightly lit indoor scenes and (4) sunlit outdoor scenes.
 SfM technology computes depth via multi-view reconstruction from the RGB
 image sequence alone.
 As such, SfM depth estimates do not suffer the same limitations and may
 be computed in all four of the previously listed circumstances.
 This work describes a novel fusion of RGBD depth data and SfM-estimated
 depths to generate an improved depth stream that may be processed by one
 of many important downstream applications such as robot localization, robot
 mapping, robot navigation, object tracking, pose estimation, and object
 recognition.This approach is demonstrated on sequences of images that transition
 from indoor scenes, where the RGBD depth sensor can function, to outdoor
 scenes, where the RGBD depth sensor fails.
\change_unchanged

\end_layout

\begin_layout Acknowledgements
If you decide to have a acknowledgements page, your acknowledgement text
 would go here.
\end_layout

\begin_layout Acknowledgements
The Acknowledgement page should be brief, simple, and free of sentimentality
 or trivia.
 It is customary to recognize the role of the advisor, the other members
 of the advisory committee, and only those organizations or individuals
 who actually aided in the project.
 Further, you should acknowledge any outside source of financial assistance,
 such as GASP grants, contracts, or fellowships.
\end_layout

\begin_layout Dedication
If you decide to have a dedication page, your dedication text would go here.
\end_layout

\begin_layout Dedication
The Dedication page, if used, pays a special tribute to a person(s) who
 has given extraordinary encouragement or support to one's academic career.
\end_layout

\begin_layout Introduction
If you decide to have an introduction page, your introduction text would
 go here.
 
\end_layout

\begin_layout Introduction
Depending on the discipline or the requirements of the student's advisory
 committee, an Introduction may be included as a preliminary page.
\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Standard
\begin_inset FloatList figure

\end_inset


\end_layout

\begin_layout Standard
\begin_inset FloatList table

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newpage
\end_layout

\begin_layout Plain Layout


\backslash
renewcommand{
\backslash
nomname}{LIST OF ABBREVIATIONS}
\end_layout

\begin_layout Plain Layout

% uncomment line below to title your nomenclature list as LIST OF SYMBOLS
\end_layout

\begin_layout Plain Layout

%
\backslash
renewcommand{
\backslash
nomname}{LIST OF SYMBOLS}
\end_layout

\begin_layout Plain Layout

%
\end_layout

\begin_layout Plain Layout

% NOTE: IF YOU USE A LIST OF ABBREVIATIONS / LIST OF SYMBOLS and are using
 command-line LaTeX (not LyX) 
\end_layout

\begin_layout Plain Layout

% YOU MUST COMPILE THE NOMENCLATURE INDEX
\end_layout

\begin_layout Plain Layout

% example:
\end_layout

\begin_layout Plain Layout

% bash$> pdflatex msthesis.tex
\end_layout

\begin_layout Plain Layout

% bash$> makeindex msthesis.nlo -s nomencl.ist -o msthesis.nls
\end_layout

\begin_layout Plain Layout

% bash$> pdflatex msthesis.tex
\end_layout

\begin_layout Plain Layout

%
\end_layout

\begin_layout Plain Layout


\backslash
addcontentsline{toc}{chapter}{
\backslash
nomname}
\end_layout

\end_inset


\begin_inset CommandInset nomencl_print
LatexCommand printnomenclature
set_width "auto"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "ECE"
description "An acronym for Electrical and Computer Engineering."

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newpage
\end_layout

\begin_layout Plain Layout


\backslash
setcounter{page}{1}
\end_layout

\begin_layout Plain Layout


\backslash
pagenumbering{arabic}
\end_layout

\begin_layout Plain Layout

% 2 inch top spacing for new chapters
\end_layout

\begin_layout Plain Layout


\backslash
bodychapterformat
\end_layout

\end_inset


\end_layout

\begin_layout Chapter
INTRODUCTION
\change_inserted 215191885 1521736201

\end_layout

\begin_layout Standard

\change_inserted 215191885 1521736202
RGBD sensors are a relatively new class of image sensors.
 Their key novel feature is the ability to simultaneously capture color
 
\begin_inset Quotes eld
\end_inset

RGB
\begin_inset Quotes erd
\end_inset

 images of the scene and depth 
\begin_inset Quotes eld
\end_inset

D
\begin_inset Quotes erd
\end_inset

 images of scene; hence the term 
\begin_inset Quotes eld
\end_inset

RGBD.
\begin_inset Quotes erd
\end_inset

 RGB images are capture using a conventional visible light camera that incorpora
tes a lens that focuses light rays from scene locations onto distinct light-sens
itive pixels of image sensor.
 RGBD integrate the three devices: (1) an IR projector, (2) and IR camera
 and (3) a RGB camera in a rigid relative geometry to create a single sensor
 that captures color-attributed 
\begin_inset Formula $(X,Y,Z)$
\end_inset

 surface data at ranges up to ~6m with frame rates up to 30 Hz.
 RGBD sensors have a wide range of applications which include mapping, localizat
ion, pose estimation, and object recognition.
 They have become popular for their ease-of-use and low cost in comparison
 with the other visual sensor technologies such as LIDAR, and have been
 incorporated into consumer products like mobile phones, gaming consoles,
 and automobiles 
\begin_inset CommandInset citation
LatexCommand cite
key "litomisky2012consumer"

\end_inset

.
\end_layout

\begin_layout Section

\change_inserted 215191885 1521736202
RGBD Sensing Technology and Limitations
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521736202
Depth image formation is accomplished using structured light technology
 to measure the geometric position of viewed surfaces.
 This is accomplished by illuminating scene surfaces with an infrared (IR)
 projector having a known pattern and then using an IR camera to capture
 the projected pattern 
\begin_inset CommandInset citation
LatexCommand cite
key "Zhang:2012:MKS:2225053.2225203"

\end_inset

.
 Deformation of the projected pattern over scene object is analyzed and
 used to triangulate the depth of scene surfaces with respect to the camera's
 optical axis.
 The IR projector operates outside the visible light frequencies and, as
 such, does not interfere with the captured RGB stream pixel values.
 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521736202
Despite the popularity of RGBD sensors, their utility for generic depth
 measurement is limited in several ways due to shortcomings associated with
 structured light depth estimation.
 One significant shortcoming is that RGBD sensors often fail to provide
 meaningful depth values in sunlit outdoor scenes.
 Here the IR radiation from sunlight interferes with the projected pattern
 causing the depth estimation process to fail.
 This phenomenon also occurs in sunlit indoor scenes.
 RGBD sensors also fail to collect measurements from surfaces having specific
 reflectance properties.
 This includes the following three reflectance contexts: (1) 
\begin_inset Quotes eld
\end_inset

dark
\begin_inset Quotes erd
\end_inset

 surfaces, i.e., surfaces having a low reflectance, (2) specular, i.e., mirror-like,
 surfaces and (3) transparent surfaces
\series bold
 
\series default

\begin_inset CommandInset citation
LatexCommand cite
key "8211432"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Kadambi2014"

\end_inset

.
\end_layout

\begin_layout Section

\change_inserted 215191885 1521736202
The Structure from Motion 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521736202
The Structure from Motion (SfM) algorithm leverages ideas originally drawn
 from photogrammetry to estimate the three-dimensional structure of a scene
 from a time series of RGB images from a moving single camera.
 This is achieved by calibrating the camera 
\begin_inset CommandInset citation
LatexCommand cite
key "Zhang:2000:FNT:357014.357025"

\end_inset

 to develop a highly-accurate model to describe how 3D positions are projected
 into camera images.
 Using this image formation model, the SfM algorithm matches together pixels
 in separate images that correspond to projections of the same 3D location
 as the camera moves in the scene.
 Using the camera projection model and the assumption that matched pixels
 are measurements of the same 3D world coordinates, the SfM algorithm solves
 for both the pose of the camera within the global coordinate system and
 the set of 3D surface positions provided by matched image pixels 
\begin_inset CommandInset citation
LatexCommand cite
key "6751290"

\end_inset

.
 The SfM problem is non-linear in the unknowns and is typically solved in
 a two-stage sequence.
 Stage 1 solves for the relative pose of the cameras at the instant the
 images were recorded.
 Stage 2 conditions on the estimated camera pose values and solves for the
 3D scene structure.
 Both stages use correspondences between pixels from different images to
 solve the non-linear equations in the unknown variables.
 The camera pose tracking problem of Stage 1 is often solved by finding
 a map that transforms pixels from the original 
\begin_inset Formula $(x,y)$
\end_inset

 coordinate field to new coordinate positions 
\begin_inset Formula $(x',y')$
\end_inset

 such that both locations correspond to images of the same 3D scene point.
 The multi-view 3D surface reconstruction of Stage 2 is often solved using
 the bundle adjustment algorithm 
\begin_inset CommandInset citation
LatexCommand cite
key "Triggs:1999:BAM:646271.685629"

\end_inset

.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521737049
While scene reconstruction via SfM produces depth images in contexts where
 depth cameras fail, this modality for depth estimation also has several
 shortcomings.
 Specifically, the theoretical formulation of the SfM problem shows that
 the scale of the estimated 3D structure cannot be known without prior or
 outside information.
 This complicates both the mathematical and computational SfM solutions.
 SfM also presumes that viewed surfaces are static, i.e., they do not move,
 and when this assumption is violated reconstructed surfaces are highly
 inaccurate.
\end_layout

\begin_layout Section

\change_inserted 215191885 1521757796
Contribution
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521757798
\begin_inset Float figure
wide true
sideways false
status collapsed

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521757798
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521757798
\begin_inset Graphics
	filename /home/akash/southeastcon/2018_SeCon_RGBDSfM_Fusion/images/teaser_rgb.png
	lyxscale 10
	height 1.3in

\end_inset


\end_layout

\begin_layout Plain Layout

\change_inserted 215191885 1521757798
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1521757798

\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521757798
\begin_inset Graphics
	filename /home/akash/southeastcon/2018_SeCon_RGBDSfM_Fusion/images/teaser_rgbd.png
	lyxscale 10
	height 1.3in

\end_inset


\end_layout

\begin_layout Plain Layout

\change_inserted 215191885 1521757798
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1521757798

\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521757798
\begin_inset Graphics
	filename /home/akash/southeastcon/2018_SeCon_RGBDSfM_Fusion/images/teaser_sfm.png
	lyxscale 10
	height 1.3in

\end_inset


\end_layout

\begin_layout Plain Layout

\change_inserted 215191885 1521757798
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1521757798

\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521757798
\begin_inset Graphics
	filename /home/akash/southeastcon/2018_SeCon_RGBDSfM_Fusion/images/teaser_fused.png
	lyxscale 10
	height 1.3in

\end_inset


\end_layout

\begin_layout Plain Layout

\change_inserted 215191885 1521757798
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1521757798

\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1521757798
\begin_inset CommandInset label
LatexCommand label
name "fig:gussian fussion"

\end_inset

An overview of the proposed RGBD and SfM fusion algorithm (a) shows a grayscale
 image of the scene (b) shows the sensed RGBD depth image (c) shows the
 SfM-estimated depth image and (d) shows the fused image.
 The fused image has been color-coded as follows: (white) denotes depth
 locations sensed only by the RGBD sensor, (yellow) denotes depth locations
 only sensed via SfM, (red) denotes fused (RGBD+SfM) depth locations and
 (black) denotes depth locations without RGBD or SfM measurements.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521737050
This article seeks to leverage the strengths of RGBD-derived and SfM-derived
 depth measurements by fusing these measurements into an improved depth
 image that provides depth measurements in contexts where at least one of
 the two depth estimation approaches succeeds.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:gussian fussion"

\end_inset

 shows an RGBD-SfM fusion result for an indoor scene and how the fusion
 result (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:gussian fussion"

\end_inset

(c,d)) captures more scene geometry than either approach independently.
 Our proposed method to fuse RGBD and SfM depth imagery includes consideration
 of the RGBD sensor depth image noise model, the SfM algorithm depth noise
 model and also copes with the inherent unknown scale and scale-drift problems
 intrinsic to SfM.
 To our knowledge these technical issues have not been discussed elsewhere
 in the literature.
\end_layout

\begin_layout Standard

\change_deleted 215191885 1521736635
Structure from Motion (SfM) is 
\change_deleted 1699939148 1515787416
a
\change_deleted 215191885 1521736635
the photogrammetric process of estimating the three-dimensional structure
 of a scene from a set of two-dimensional images, this is achieved by tracking
 the motion of the camera
\change_deleted 1699939148 1515787425
s
\change_deleted 215191885 1521736635
 
\change_deleted 1699939148 1515787428
corresponding
\change_deleted 215191885 1521736635
generating 
\change_deleted 1699939148 1515787431
to 
\change_deleted 215191885 1521736635
these images and correlating information within the time-sequence of images.
 SfM has many applications in the field of robotics including augmented
 reality
\change_deleted 1699939148 1515787468
 and
\change_deleted 215191885 1521736635
, geoscience 
\change_deleted 1699939148 1515787471
etc
\change_deleted 215191885 1521736635
and 
\series bold
\emph on
[put more here]
\series default
\emph default
.
 In 
\change_deleted 1699939148 1515787485
R
\change_deleted 215191885 1521736635
robotics, SfM is 
\change_deleted 1699939148 1515787489
mainly 
\change_deleted 215191885 1521736635
applied 
\change_deleted 1699939148 1515787498
to
\change_deleted 215191885 1521736635
as one approach for
\change_deleted 1699939148 1515787504
 implement the
\change_deleted 215191885 1521736635
 visual odometry, the process of estimating 
\change_deleted 1699939148 1515787514
where 
\change_deleted 215191885 1521736635
the egomotion of an robot 
\change_deleted 1699939148 1515787571
is estimated 
\change_deleted 215191885 1521736635
using only the inputs of cameras attached to it.
 In the field of augmented reality, SfM used to estimate the depth maps
 of the scene, which are later used to implement basic physical interaction
 with the environment [depth map not defined, basic physical interaction
 vague].
 
\end_layout

\begin_layout Standard

\change_deleted 215191885 1521736635
\begin_inset Note Comment
status open

\begin_layout Plain Layout

\change_inserted 1699939148 1515787697
Maybe explain 3D reconstruction via triangulation
\change_unchanged

\end_layout

\end_inset

The core of the SfM involves solving the trigonometry [??? unclear what
 trig?] on set of images from camera with unknown calibration to extract
 the depth of an object [3d reconstruction explanation perhaps].
 The main requirement to do this is to find a correspondence between pair
 of images, traditionally this was done by feature extraction and matching
\begin_inset CommandInset citation
LatexCommand cite
key "klein07parallel"

\end_inset

.
 The feature extraction is the process of selecting the key points in the
 image which are unique and distinguishable and represent them in a efficient
 descriptor,which are later used for matching in the other image.
 There are many possible choices for features and descriptors, like SIFT,
 SURF, ORB etc.
 However in more recent implementations
\begin_inset CommandInset citation
LatexCommand cite
key "Engel2014"

\end_inset

, instead of features, the image intensities are used directly for matching.
 Once the correspondence are established, the Pose of the cameras are estimated
 by imposing the Epipolar constraint, now with the estimated camera pose
 the three-dimensional structure (depth) is computed.
 There mainly two variants of SfM, 
\change_deleted 1699939148 1515787706
I
\change_deleted 215191885 1516181838
incremental and 
\change_deleted 1699939148 1515787709
G
\change_deleted 215191885 1516181838
global
\change_deleted 1699939148 1515787713
 SfM
\change_deleted 215191885 1521736635
.
 Incremental SfM
\begin_inset CommandInset citation
LatexCommand cite
key "Snavely:2006:PTE:1141911.1141964"

\end_inset

 begins by first estimating the 3D structure and camera poses of just two
 cameras based on their relative pose.
 Then additional cameras are added on incrementally and 3D structure is
 refined as new parts of the scene are observed.
 On the other hand 
\change_deleted 1699939148 1515787728
G
\change_deleted 215191885 1521736635
global SfM
\begin_inset CommandInset citation
LatexCommand cite
key "Wilson2014"

\end_inset

 considers the entire problem at once
\change_deleted 1699939148 1515787735
,
\change_deleted 215191885 1521736635
.
 Here the objective is 
\change_deleted 1699939148 1515787745
it tries 
\change_deleted 215191885 1521736635
to estimate the global camera poses and 3D structure by removing outliers
 and by applying an averaging scheme [cite ???].
 [why is this discussed? are their results different? is one of interest
 to you?] Over the years many approaches have be suggested to tackle this
 problem, one of them is the Parallel Tracking and Mapping (PTAM)
\begin_inset CommandInset citation
LatexCommand cite
key "klein07parallel"

\end_inset

, it is a feature based system, here the tracking and mapping are split
 into two separate tasks, processed in parallel threads on dual core computer
 [global or local?].
 The map is represented by a collection of point features located in a world
 coordinate frame 
\change_deleted 1699939148 1515787826
W
\change_deleted 215191885 1516599029

\begin_inset Formula $W$
\end_inset

.
 These points feature represents a locally planar textured patch in the
 world, each point has coordinates in world frame, an unit patch normal
 and a reference to the patch source pixels.
 The map also contains 
\change_deleted 1699939148 1515787809
N
\change_deleted 215191885 1516599029

\begin_inset Formula $N$
\end_inset

 [use math font for variables] key frames, each key frame has an associated
 camera-center coordinate frame and the transformation between the frames
 and also stores a four level pyramid of gray-scale 8bpp (bits-per-pixel)
 images.
 The tracking is a two-stage process done from coarse-to-fine,when the new
 image is acquired, an initial coarse search searches only for 50 map points
 which appear at the highest levels of the current frame’s image pyramid,
 and this search is performed (with sub-pixel refinement) over a large search
 radius.
 A new pose is then calculated from these measurements.
 After this, up to 1000 of the remaining potentially visible image patches
 are re-projected into the image, and now the patch search is performed
 over a far tighter search region.
 Sub-pixel refinement is performed only on a high-level subset of patches.
 The final frame pose is calculated from both coarse and fine sets of image
 measurements together.The pose update is computed iteratively by minimizing
 a robust objective function of the re-projection error:
\end_layout

\begin_layout Standard

\change_deleted 215191885 1516599029
\begin_inset Formula 
\[
\mu^{\prime}=\underset{\mu}{argmin}\underset{j\in s}{\sum}Obj\left(\frac{\left\Vert e_{j}\right\Vert }{\sigma_{j}},\sigma_{T}\right)
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516599029
Where 
\begin_inset Formula $e_{j}$
\end_inset

is the re-projection error vector:
\end_layout

\begin_layout Standard

\change_deleted 215191885 1516599029
\begin_inset Formula 
\[
e_{j}=\left(\begin{array}{c}
\hat{u_{j}}\\
\hat{v_{j}}
\end{array}\right)-CamProj(exp(\mu)E_{CWp_{j}})
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516599029
\begin_inset Formula $Obj(.,\sigma_{T})$
\end_inset

 is the Tukey bi-weight objective function and 
\begin_inset Formula $\sigma_{T}$
\end_inset

 a robust (median-based) estimate of the distribution’s standard deviation
 derived from all the residuals.
 [why are you talking about PTAM? is it related to your thesis? what is
 your research here?]
\end_layout

\begin_layout Standard

\change_deleted 215191885 1516599029
Associated with the the key-frame in the map is a set 
\begin_inset Formula $S_{i}$
\end_inset

 of image measurements..
 For example, the 
\begin_inset Formula $jth$
\end_inset

 map point measured in key-frame 
\begin_inset Formula $i$
\end_inset

 would have been found at 
\begin_inset Formula $(\hat{u}_{ji},\hat{v}_{ji})^{T}$
\end_inset

 with standard deviation of 
\begin_inset Formula $\sigma_{ji}$
\end_inset

 pixels.
 Writing the current state of the map as 
\begin_inset Formula $\left\{ E_{k_{1}W}....E_{k_{N}w}\right\} $
\end_inset

 and 
\begin_inset Formula $\left\{ p_{1}...p_{M}\right\} $
\end_inset

, each image measurement also has an associated re-projection error 
\begin_inset Formula $e_{ji}$
\end_inset

.
 Bundle adjustment is applied to iteratively adjust the map so as to minimize
 the robust objective function:
\end_layout

\begin_layout Standard

\change_deleted 215191885 1521736635
\begin_inset Formula 
\[
\left\{ \left\{ \mu_{2..}\mu_{N}\right\} ,\left\{ p_{1}^{\prime}..p_{M}^{\prime}\right\} \right\} =\underset{\{\{\mu\},\{p\}\}}{argmin}\stackrel[i=1]{N}{\sum}\underset{j\in s}{\sum}Obj\left(\frac{\left\Vert e_{ji}\right\Vert }{\sigma_{ji}},\sigma_{T}\right)
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1521736635
LSD SLAM 
\begin_inset CommandInset citation
LatexCommand cite
key "Engel2014"

\end_inset

 on the other hand is the direct method [you have not defined sparse methods,
 dense methods, direct methods, or indirect methods do this in a review
 of approaches above – similar to that from Direct Sparse Odometry article
 (https://arxiv.org/pdf/1607.02565.pdf) from TUM], this circumvent the drawback
 of PTAM, which is a feature base method and only the information that conforms
 to the feature type can be used.
 LSD LSD SLAM on the other hand optimizes the geometry directly on the image
 intensities, which enables using all information in the image.In addition
 to higher accuracy and robustness in particular in environments with little
 key-points, this provides substantially more information about the geometry
 of the environment.Images are aligned by Gauss -Newton minimization of the
 photometric error.
\end_layout

\begin_layout Standard

\change_deleted 215191885 1521736635
\begin_inset Formula 
\[
E(\xi)=\underset{i}{\sum}(I_{ref}(P_{i})-I(\omega(p_{i},D_{ref}(p_{i}),\xi)))^{2}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1521736635
Where 
\begin_inset Formula $D_{ref}$
\end_inset

 is the estimated depth of reference frame,
\begin_inset Formula $\xi\in se(3)$
\end_inset

 is Lie-algebra representation of rigid body motion and 
\begin_inset Formula $\omega$
\end_inset

 is the affine wrap function.
 The above error function gives the maximum-likelihood estimator for 
\begin_inset Formula $\xi$
\end_inset

 assuming i.i.d.
 Gaussian residuals.
 
\begin_inset Formula $\delta\xi^{(n)}$
\end_inset

 is computed for each iteration by solving for the minimum of Gauss-Newton
 second-order approximation of 
\change_deleted 1699939148 1515788038
E
\change_deleted 215191885 1521736635

\begin_inset Formula $E$
\end_inset

: [no description of image warping needed here, briefly discuss image warping
 for direct methods as a methods for image pixel matching see Baker2004
 http://www.ncorr.com/download/publications/bakerunify.pdf]
\end_layout

\begin_layout Standard

\change_deleted 215191885 1521736635
\begin_inset Formula 
\[
\delta\xi^{(n)}=-(J^{T}J)^{-1}J^{T}r(\xi^{(n)})
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1521736635
with 
\end_layout

\begin_layout Standard

\change_deleted 215191885 1521736635
\begin_inset Formula 
\[
J=\frac{\partial r(\epsilon\circ\delta\xi^{(n)})}{\partial\epsilon}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1521736635
The new estimate is then obtained by multiplication with the computed update
\end_layout

\begin_layout Standard

\change_deleted 215191885 1521736635
\begin_inset Formula 
\[
\xi^{(n+1)}=\delta\xi^{(n)}\circ\xi^{(n)}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1521736635
The overall system is composed of three major components, tracking, depth
 map estimation and map optimization.
\end_layout

\begin_layout Standard

\change_deleted 215191885 1521736635
The tracking component continuously estimates the rigid body pose with respect
 to the current keyframe, using the pose of the previous frame as initialization.
LSD slam tracks new frame by minimizing the variance-normalized photometric
 error
\end_layout

\begin_layout Standard

\change_deleted 215191885 1521736635
\begin_inset Formula 
\[
E_{p}(\xi_{ji})=\underset{p\in\Omega_{D_{i}}}{\sum}\left\Vert \frac{r_{p}^{2}(p,\xi_{ji})}{\sigma_{r_{p}(p,\xi_{ji})}^{2}}\right\Vert _{\delta}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1521736635
with
\begin_inset Formula 
\[
r_{p}(p,\xi_{ji})\coloneqq I_{i}(p)-I_{j}(\omega(p,D_{i}(p),\xi_{ji}))
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1521736635
\begin_inset Formula 
\[
\sigma_{r_{p}(p,\xi_{ji})}^{2}\coloneqq2\sigma_{I}^{2}+\left(\frac{\partial r_{p}(p,\xi_{ji})}{\partial D_{i}(p)}\right)^{2}V_{i}(p)
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1521736635
where 
\begin_inset Formula $\left\Vert .\right\Vert $
\end_inset

 is the Huber norm
\end_layout

\begin_layout Standard

\change_deleted 215191885 1521736635
\begin_inset Formula 
\[
\left\Vert r^{2}\right\Vert _{\delta}\coloneqq\begin{cases}
\frac{r^{2}}{2\delta}if\left\Vert r\right\Vert \leq\delta & ,\left\Vert r\right\Vert -\frac{\delta}{2}\end{cases}otherwise
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1521736635
applied to the normalized residual.
 The depth map estimation component uses tracked frames to either refine
 or replace the current key-frame.
 Depth is refined by filtering over many per-pixel, small-baseline stereo
 comparisons coupled with interleaved spatial regularization.
 If the camera has moved too far, a new key-frame is initialized by projecting
 points from existing, close-by key-frames into it.
 Each key-frame is scaled such that its mean inverse depth is one, which
 enables more small-baseline stereo comparisons.
 For every new key-frame added, the possibility of loop closure is checked
 by performing the reciprocal tracking check.
 The map optimization component is responsible for the updating depth map
 into global map, it detect loop closure and scale drift by estimating similarit
y transform (sim(3)) to close by existing key-frames.The global map is represente
d as a pose graph consisting of key-frames as vertices's with 3D similarity
 transforms as edges, elegantly incorporating changing scale of the environment
 and allowing to detect and correct accumulated drift.
 Each key-frame consists of a camera image, an inverse depth map and variance
 of the inverse depth.Edges between key-frames contain their relative alignment
 as similarity transform, as well as corresponding covariance matrix.
 The map, consisting of a set of key-frames and tracked sim(3)-constraints,
 is continuously optimized in the background using pose graph optimization.
 The error function that is minimized is defined by (W defining the world
 frame) 
\end_layout

\begin_layout Standard

\change_deleted 215191885 1521736635
\begin_inset Formula 
\[
E(\xi_{W1}....\xi_{Wn})\coloneqq\underset{(\xi_{ji},\Sigma_{ji})\in\varepsilon}{\sum}(\xi_{ji}\circ\xi_{Wi}^{-1}\circ\xi_{Wj})\Sigma_{ji}^{-1}(\xi_{ji}\circ\xi_{Wi}^{-1}\circ\xi_{Wj})
\]

\end_inset


\change_inserted 215191885 1521031169

\end_layout

\begin_layout Chapter

\change_inserted 215191885 1521031399
RELATED WORK AND BACKGROUND INFORMATION
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521031399
This article proposes fusion of SfM-estimated depths with depths captured
 from an RGBD image sensor.
 This section is dedicated to discussing the relevant aspects of the SfM
 algorithm and the sensed RGBD measurements necessary to explain the proposed
 fusion method.
 Specifically, this section reviews the theoretical details of the SfM algorithm
, methods for processing depth images including computing depth images for
 arbitrary camera poses, and details existing knowledge regarding the sensor
 measurement noise for RGBD depth measurements
\end_layout

\begin_layout Standard

\change_deleted 215191885 1521031758
Methodology
\change_unchanged

\end_layout

\begin_layout Standard

\change_deleted 215191885 1521031840
Structure from motions (SfM) is the process of triangulating the three-dimension
al structure from two-dimensional images, along with estimating the motion
 of camera (visual odometer), hence it is some time called as Visual SLAM.

\change_unchanged
 
\change_inserted 215191885 1521995641

\end_layout

\begin_layout Section

\change_inserted 215191885 1522008547
Camera Calibration
\begin_inset CommandInset label
LatexCommand label
name "sec:Camera-Calibration"

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1522000457
The SfM is a photogrammetric process and it relies on the camera parameters,
 these parameters are called intrinsic parameter since they are unique to
 each camera and are not influenced by any external factors.
 These intrinsic properties are composed of the focal lengths 
\begin_inset Formula $(fx,fy)$
\end_inset

 and principal point offset 
\begin_inset Formula $(x_{0},y_{0})$
\end_inset

 of camera is both vertical and horizontal direction along with the skew
 factor 
\begin_inset Formula $s$
\end_inset

.
 These properties are usually represented in a self-contained matrix 
\begin_inset Formula $k$
\end_inset

 called the intrinsic matrix.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1522001989
\begin_inset Formula 
\begin{equation}
k=\left[\begin{array}{ccc}
fx & s & x_{0}\\
0 & fy & y_{0}\\
0 & 0 & 1
\end{array}\right]\label{eq:intrensic matrix}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1522002046
The image generated by a typical camera usually have a distortions, and
 since the estimation of intrinsic parameters relies on these images, those
 distortions should be fixed.
 There are mainly of two types of distortions called the radial distortion
 and tangential distortion.
 The radial distortion effects image by curving the string lines, Its effect
 is more as we move away from the center of image.
 This distortion can be corrected by Brown's distortion model
\begin_inset CommandInset citation
LatexCommand cite
key "s16060807"

\end_inset

 as following.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521999851
\begin_inset Formula 
\[
x_{corrected}=x(1+k_{1}r^{2}+k_{2}r^{4}+k_{3}r^{6})
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521999915
\begin_inset Formula 
\[
y_{corrected}=y(1+k_{1}r^{2}+k_{2}r^{4}+k_{3}r^{6})
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1522001273
where 
\begin_inset Formula $(x,y)$
\end_inset

 are distorted image intensities and 
\begin_inset Formula $(x_{corrected},y_{corrected})$
\end_inset

 are corrected image intensities, with 
\begin_inset Formula $k_{x}$
\end_inset

is the distortion coefficient of the camera to be determined and r is the
 distance of pixel from the principal point.
 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1522001472
Tangential distortion are due to misalignment of lens with imaging plane,
 as a consequence some areas in image may look nearer than expected.
 This distortion can be corrected by The Brown–Conrady model as following
 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1522001383
\begin_inset Formula 
\[
x_{corrected}=x+[2p_{1}xy+p_{2}(r^{2}+2x^{2})]
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1522001478
\begin_inset Formula 
\[
y_{corrected}=y+[p_{1}(r^{2}+2y^{2})+2p_{2}xy]
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1522001570
In total there are 5 parameters, known as distortion coefficients given
 by:
\end_layout

\begin_layout Standard

\change_inserted 215191885 1522001576
\begin_inset Formula 
\begin{equation}
D_{coff}=(k_{1},k_{2},p_{1},p_{2},k_{3})\label{eq:distortion coeff}
\end{equation}

\end_inset


\change_unchanged

\end_layout

\begin_layout Standard

\change_inserted 215191885 1522008604
All the essential parameters of camera from 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:intrensic matrix"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:distortion coeff"

\end_inset

 are determined for a given camera by traditional method 
\begin_inset CommandInset citation
LatexCommand cite
key "Zhang2014,Zhang:2000:FNT:357014.357025"

\end_inset

 using the checkerboard pattern.
 There are many standard library implementations of this method and we are
 using the OpenCV library 
\begin_inset CommandInset citation
LatexCommand cite
key "5534797"

\end_inset

 to do this.
\end_layout

\begin_layout Section

\change_inserted 215191885 1521044107
Image alignment
\begin_inset CommandInset label
LatexCommand label
name "sec:Image-alignment"

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521917682
The image alignment is the process of transforming (wraping) one image with
 respect to another image, with a goal to minimizing the total difference
 between the intensities 
\begin_inset CommandInset citation
LatexCommand cite
key "Baker:2004:LYU:964568.964604"

\end_inset

.
\begin_inset Formula 
\begin{equation}
\underset{x}{\sum}\left[I(\omega(x))-T(x)\right]^{2}\label{eq:image alignment}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521917709
In eq 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:image alignment"

\end_inset

 the wrap function 
\begin_inset Formula $\omega$
\end_inset

 deforms image I along the image T in an attempt to minimize the difference
 between them.
 The wrap function which achieve this goal is estimated incremental by performin
g gradient descent.In order to achieve this, we need to find the correspondence
 between images.
\change_unchanged

\end_layout

\begin_layout Subsection

\change_inserted 215191885 1521651161
Solving for Image Pixel Correspondences
\begin_inset CommandInset label
LatexCommand label
name "subsec:Solving-for-Image"

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521039811
There are generically two different approaches for finding corresponding
 observations of the same 3D surface location in multiple images referred
 to as 
\emph on
direct 
\emph default
and 
\emph on
indirect 
\emph default

\begin_inset CommandInset citation
LatexCommand cite
key "7898369"

\end_inset


\emph on
.
 
\emph default
In this discussion, we refer to the correspondence problem as a source-to-target
 matching problem.
 Let 
\begin_inset Formula $\mathbf{I}_{t}(x,y)$
\end_inset

 denote an image recorded at time 
\begin_inset Formula $t$
\end_inset

 and 
\begin_inset Formula $\mathbf{I}_{t+\Delta t}(x,y)$
\end_inset

 denote a subsequent image measured at time 
\begin_inset Formula $t+\Delta t$
\end_inset

.
 The correspondence problem seeks to find a map that transforms pixels from
 the original 
\begin_inset Formula $(x,y)$
\end_inset

 coordinate field of 
\begin_inset Formula $\mathbf{I}_{t}$
\end_inset

 to new coordinate positions 
\begin_inset Formula $(x',y')$
\end_inset

 in 
\begin_inset Formula $\mathbf{I}_{t+\Delta t}$
\end_inset

 such that 
\begin_inset Formula $\mathbf{I}_{t}(x,y)$
\end_inset

 and 
\begin_inset Formula $\mathbf{I}_{t+\Delta t}(x',y')$
\end_inset

 correspond to images of the same 3D scene point.
 
\end_layout

\begin_layout Subsubsection*

\change_inserted 215191885 1521039811
Indirect Methods
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521039811
Indirect methods compute this mapping by detecting special 
\begin_inset Formula $(x,y)$
\end_inset

 locations referred to as features locations with a purpose-built feature
 detection algorithm, e.g., the Harris corner detector 
\begin_inset CommandInset citation
LatexCommand cite
key "Harris88acombined"

\end_inset

.
 A description of the image patch in the vicinity of each detected 
\begin_inset Formula $(x,y)$
\end_inset

 location is computed using some feature descriptor algorithm, e.g., Lowe's
 SIFT descriptor 
\begin_inset CommandInset citation
LatexCommand cite
key "Lowe2004"

\end_inset

.
 Feature descriptors seek to provide a vector of values from the image patch
 data that is invariant to the image variations that occur during camera
 motion.
 These include but are not limited to the following effects: illumination
 variation, affine and/or projective invariance, photometric invariance
 (brightness constancy), and scale invariance.
 Popular feature descriptors often prioritize scale and affine invariance
 as their strengths.
 The invariance property allows for correspondences to be computed by finding
 the mapping from the feature descriptor set calculated from image 
\begin_inset Formula $\mathbf{I}_{t}(x,y)$
\end_inset

 to the feature descriptor set calculated from image 
\begin_inset Formula $\mathbf{I}_{t+\Delta t}(x,y)$
\end_inset

.
\end_layout

\begin_layout Subsubsection*

\change_inserted 215191885 1521039811
Direct Methods
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521043355
Direct methods on the other hand typically iteratively solve for a set of
 transformation parameters that best align a pair of images by the minimization
 of pixel-wise errors.
 An image warping function, 
\begin_inset Formula $\omega(\mathbf{x})$
\end_inset

, maps a pixel location, 
\begin_inset Formula $\mathbf{x}=[x,y]^{t}$
\end_inset

, in the original coordinate field to new coordinate positions, 
\begin_inset Formula $\mathbf{x}'$
\end_inset

, such that both locations correspond to images.
 A classical solution to this problem is given by the Lucas-Kanade-Tomassi
 (LKT) camera tracking algorithm 
\begin_inset CommandInset citation
LatexCommand cite
key "Baker:2004:LYU:964568.964604"

\end_inset

.
 
\end_layout

\begin_layout Section

\change_inserted 215191885 1521738907
SfM Depth Reconstruction
\end_layout

\begin_layout Standard

\change_inserted 215191885 1522008579
The SfM algorithm uses a time sequence of images from a moving camera to
 recover the 3D geometry of objects viewed by the camera.
 While this problem can be solved without a calibrated camera, reconstruction
 accuracy will adversely affected.
 This work assumes that the camera calibration 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Camera-Calibration"

\end_inset

 parameters are known.
 The SfM algorithm can be broken down into two key steps:
\end_layout

\begin_layout Enumerate

\change_inserted 215191885 1521040943
estimation of the camera pose, i.e., position and orientation, at the time
 each image was recorded,
\end_layout

\begin_layout Enumerate

\change_inserted 215191885 1521040943
estimation of the 3D structure of the scene.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521040943
As previously mentioned, typical SfM systems solve (1) by computing a map
 that associates pixels from the original 
\begin_inset Formula $(x,y)$
\end_inset

 coordinate field to new coordinate positions 
\begin_inset Formula $(x',y')$
\end_inset

 such that both locations correspond to images of the same 3D scene point
 and (2) via multi-view 3D surface reconstruction algorithm, e.g., bundle
 adjustment 
\begin_inset CommandInset citation
LatexCommand cite
key "Triggs:1999:BAM:646271.685629"

\end_inset

.
 In the following sections we provide an overview of aspects of the SfM
 algorithm necessary for the development of the proposed RGBD-SfM depth
 fusion algorithm.
\end_layout

\begin_layout Subsection

\change_inserted 215191885 1521739070
Estimation of the Camera Trajectory
\change_unchanged

\end_layout

\begin_layout Subsubsection*
Rigid Body Motion
\change_inserted 215191885 1521044093

\begin_inset CommandInset label
LatexCommand label
name "subsec:Rigid-Body-Motion"

\end_inset


\change_unchanged

\end_layout

\begin_layout Standard

\change_deleted 215191885 1521032805
One of the goals of SfM it to estimate the
\change_inserted 215191885 1521032806
The
\change_unchanged
 Camera trajectory
\change_inserted 215191885 1521847011
 
\change_deleted 215191885 1521032811
, which 
\change_unchanged
is a rigid body motion.
 We need an efficient model to represent and compute this rigid body motion.
 The camera position is represented by a 3D Vector in an Euclidean 
\change_deleted 1699939148 1515788126
S
\change_unchanged
space, this camera position is chosen to represent 
\change_inserted 215191885 1521032965
origin of 
\change_unchanged
the world frame and specify the translation and rotation of the scene relative
 to 
\change_deleted 215191885 1521032976
that
\change_inserted 215191885 1521032976
this
\change_unchanged
 frame.
 The rigid body motion itself is composed of a rotation and translation.
 
\end_layout

\begin_layout Standard
Traditionally, rotation is represented by a 
\begin_inset Formula $3\times3$
\end_inset

 special orthogonal matrix called rotational matrix.
 Special Orthogonal matrix SO(3) are the matrix which satisfy 
\begin_inset Formula $R^{T}R=RR^{T}=I$
\end_inset

 and have a determinant of 
\begin_inset Formula $+1$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
SO(3)=\{R\in\mathbb{R}^{3\times3}\mid R^{T}R=I,det(R)=+1\}
\]

\end_inset


\end_layout

\begin_layout Standard
The rotation transformation of the coordinates 
\begin_inset Formula $X_{c}$
\end_inset

of a point p relative to frame C to its coordinates 
\begin_inset Formula $X_{w}$
\end_inset

 relative to frame W is 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
X_{w}=R_{wc}X_{c}
\]

\end_inset


\end_layout

\begin_layout Standard
Also, because the rotational matrix are orthogonal, we have
\begin_inset Formula $R^{-1}=R^{T}$
\end_inset

, on this line the inverse transformation of coordinates are 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
X_{c}=R_{wc}^{-1}X_{w}=R_{wc}^{T}X_{w}
\]

\end_inset


\end_layout

\begin_layout Standard
The continuous rotation of a camera is described as a trajectory 
\begin_inset Formula $R(t):t\rightarrow SO(3)$
\end_inset

 in the space 
\begin_inset Formula $SO(3)$
\end_inset

.When the starting time is not t = 0, the relative motion between time 
\begin_inset Formula $t_{2}$
\end_inset

 and time 
\begin_inset Formula $t_{1}$
\end_inset

 will be denoted as 
\begin_inset Formula $R(t_{2},t_{1})$
\end_inset

.
 The composition law of the rotation group implies
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
R(t_{2,}t_{0})=R(t_{2,}t_{1})\times R(t_{1,}t_{0}),\vee t_{0}<t_{1}<t_{2}\in R
\]

\end_inset


\end_layout

\begin_layout Standard
On the other hand the translation is represented by a 
\begin_inset Formula $T\in R^{3}$
\end_inset

,
\begin_inset Formula $1\times3$
\end_inset

 vector which adds the translation values in each dimension.With this, the
 complete rigid body motion is represented by 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
X_{w}=R_{wc}X_{c}+T_{wc}\label{eq:1}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
However, the above equation is not linear but affine.
 We may convert this to linear by using homogeneous coordinates, where we
 append 1 for 
\begin_inset Formula $1\times3$
\end_inset

 vector and make it a 
\begin_inset Formula $1\times4$
\end_inset

 vector,
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\bar{X}=\left[\begin{array}{c}
X\\
1
\end{array}\right]=\left[\begin{array}{c}
X_{1}\\
X_{2}\\
X_{3}\\
1
\end{array}\right]\in\mathbb{R}^{4}
\]

\end_inset


\end_layout

\begin_layout Standard
With this new notation for point, we can rewrite the transformation from
 equation 6 as following
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\bar{X}_{w}=\left[\begin{array}{c}
X_{w}\\
1
\end{array}\right]=\left[\begin{array}{cc}
R_{wc} & T_{wc}\\
0 & 1
\end{array}\right]\left[\begin{array}{c}
X_{c}\\
1
\end{array}\right]=\bar{g}_{wc}\bar{X}_{c}\label{eq:2}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where the 
\begin_inset Formula $4\times4$
\end_inset

 matrix 
\begin_inset Formula $\bar{g}_{wc}\in R^{4\times4}$
\end_inset

is called the homogeneous representation of the rigid-body motion.
\end_layout

\begin_layout Standard
The set of all possible configurations of a rigid body can then be described
 by the space of rigid-body motions or special Euclidean transformations
 called SE(3)
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
SE(3)=\{\bar{g}=\left[\begin{array}{cc}
R & T\\
0 & 1
\end{array}\right]\mid R\in SO(3),T\in R^{3}\}\subset\mathbb{R}^{4\times4}\label{eq:se3}
\end{equation}

\end_inset


\change_inserted 215191885 1516636715

\end_layout

\begin_layout Standard

\change_inserted 215191885 1521824363
Finally, one other dimension we could consider is the scale, this can represente
d by a Similarity transformation 
\begin_inset Formula $Sim(3)$
\end_inset

, which is the composition of a rotation, translation and a uniform rescaling,
 and hence this representation has 7 degree of freedom, 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1516637515
\begin_inset Formula 
\begin{equation}
Sim(3)=\{\bar{g}=\left[\begin{array}{cc}
R & T\\
0 & s^{-1}
\end{array}\right]\mid R\in SO(3),T\in R^{3},s\in R\}\subset\mathbb{R}^{4\times4}\label{eq:sim3}
\end{equation}

\end_inset


\change_unchanged

\end_layout

\begin_layout Subsubsection*
Exponential Map
\end_layout

\begin_layout Standard
The special orthogonal group in three dimensions can be represented by a
 
\begin_inset Formula $3\times3$
\end_inset

 rotation matrix 
\begin_inset Formula $R\in SO(3)$
\end_inset

, which must satisfy the constraint 
\begin_inset Formula $R^{T}R=I$
\end_inset

, this implies that the 
\begin_inset Formula $SO(3)$
\end_inset

 transformations leaves the quantity 
\begin_inset Formula $x^{2}+y^{2}+z^{2}$
\end_inset

 invariant.
 The group 
\begin_inset Formula $SO(3)$
\end_inset

 has 9 parameters, but the invariance of the length produces six independent
 conditions, leaving three free parameters, Hence, the dimension of the
 space of rotation matrices 
\begin_inset Formula $SO(3)$
\end_inset

 
\change_deleted 215191885 1521848714
should
\change_inserted 215191885 1521848714
can
\change_unchanged
 be only three, and six parameters out of the nine are in fact redundant.We
 can use this to have better representation of Rigid body motion.
\change_inserted 215191885 1516614036

\end_layout

\begin_layout Standard

\change_inserted 215191885 1516618836
The 
\begin_inset Formula $SO(3)$
\end_inset

, 
\begin_inset Formula $SE(3)$
\end_inset

 and 
\begin_inset Formula $SIM(3)$
\end_inset

are categorized under the special group called the Lie group, which represents
 the smooth differentiable manifolds.
 Every Lie group has a tangent space at identity called the Lie algebra,
 which is a vector space used to study the infinitesimal transformations.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1516618136
For rotation group 
\begin_inset Formula $SO(3)$
\end_inset

 its lie algebra 
\begin_inset Formula $so(3)$
\end_inset

 is represented by
\end_layout

\begin_layout Standard

\change_inserted 215191885 1516619360
\begin_inset Formula 
\[
so(3)\doteq\left\{ \hat{\omega}\in\mathbb{R}^{3\times3}\mid\omega\in\mathbb{R}^{3}\right\} 
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1516619394
where 
\begin_inset Formula $\hat{\omega}$
\end_inset

is a skew symmetric matrix for the vector 
\begin_inset Formula $\omega$
\end_inset

 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1516618683
The map from the space 
\begin_inset Formula $so(3)$
\end_inset

 to 
\begin_inset Formula $SO(3)$
\end_inset

 is called the exponential map.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1516618685
\begin_inset Formula 
\[
exp:so(3)\rightarrow SO(3);\hat{\omega}\mapsto e^{\hat{\omega}}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1516618599
\begin_inset Formula 
\begin{equation}
R(t)=e^{\hat{\omega}t}\label{exp map for so(3)}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1516619295
The equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "exp map for so(3)"

\end_inset

 represents a rotation around the axis 
\begin_inset Formula $\omega\in\mathbb{R}^{3}$
\end_inset

 by an angle of 
\begin_inset Formula $t$
\end_inset

 radians.
 And the inverse mapping is obtained by logarithm of SO(3)
\end_layout

\begin_layout Standard

\change_inserted 215191885 1516618286
\begin_inset Formula 
\begin{equation}
log:SO(3)\rightarrow so(3);log(R)\mapsto\hat{\omega}\label{eq:inv_SO}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1516618390
We can extend this to full rigid body motion which also involves the translation
 along with the rotation, for rotation group 
\begin_inset Formula $SE(3)$
\end_inset

 its lie algebra 
\begin_inset Formula $se(3)$
\end_inset

 is represented by .
\end_layout

\begin_layout Standard

\change_inserted 215191885 1516618391
\begin_inset Formula 
\[
se(3)\doteq\left\{ \hat{\xi}=\left[\begin{array}{cc}
\hat{\omega} & \upsilon\\
0 & 0
\end{array}\right]\mid\hat{\omega}\in so(3),\upsilon\in\mathbb{R}^{3}\right\} \subset\mathbb{R}^{4\times4}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1516618477
with 
\begin_inset Formula $\upsilon(t)=\dot{T}(t)-\hat{\omega}(t)T(t)\in\mathbb{R}^{3}$
\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1516618546
Similarly, the exponential map from the space 
\begin_inset Formula $se(3)$
\end_inset

 to 
\begin_inset Formula $SE(3)$
\end_inset

is given by
\end_layout

\begin_layout Standard

\change_inserted 215191885 1516618478
\begin_inset Formula 
\[
exp:se(3)\rightarrow SE(3);\hat{\xi}\mapsto e^{\hat{\xi}}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521849596
and on the same lines of 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:inv_SO"

\end_inset

 inverse to the exponential map is defined by logarithm 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1516639609
\begin_inset Formula 
\[
log:SE(3)\rightarrow se(3);log(g)\mapsto\hat{\xi}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618781
We know that the continuous rotational motion represented by 
\begin_inset Formula $R(t):R\in SO(3)$
\end_inset

, must satisfy the following constraint
\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618781
\begin_inset Formula 
\[
R(t)R^{T}(t)=I
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618781
Differentiating the above equation with respect to time t gives
\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618781
\begin_inset Formula 
\[
\dot{R}(t)R^{T}(t)+R(t)\dot{R}^{T}(t)=0
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618781
\begin_inset Formula 
\[
\dot{R}(t)R^{T}(t)=-(\dot{R}(t)R^{T}(t))^{T}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618781
This shows that the matrix 
\begin_inset Formula $\dot{R}(t)R^{T}(t)\in\mathbb{R}^{3\times3}$
\end_inset

is a skew-symmetric matrix.This implies that there must exist a vector, say
 
\begin_inset Formula $\omega(t)\in\mathbb{R}^{3}$
\end_inset

,such that
\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618781
\begin_inset Formula 
\begin{equation}
\dot{R}(t)R^{T}(t)=\hat{\omega}(t)\label{eq:3-1}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618781
Multiplying both sides by 
\begin_inset Formula $R(t)$
\end_inset

 on the right yields
\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618781
\begin_inset Formula 
\begin{equation}
\dot{R}(t)=\hat{\omega}(t)R(t)\label{eq:3}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618781
In above equation, if 
\begin_inset Formula $R(t_{0})=I$
\end_inset

 for for 
\begin_inset Formula $t=t_{0}$
\end_inset

, we have 
\begin_inset Formula $\dot{R}(t_{0})=\hat{\omega}(t_{0})$
\end_inset

.
 Hence, around the identity matrix I, a skew-symmetric matrix gives a first-
 order approximation to a rotation matrix:
\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618781
\begin_inset Formula 
\[
R(t_{0}+dt)\thickapprox I+\hat{\omega}(t_{0})dt
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618781
This space of all skew-symmetric matrix represents the tangent space of
 the rotation group 
\begin_inset Formula $SO(3)$
\end_inset

 and it is the lie algebra 
\begin_inset Formula $so(3)$
\end_inset

 of the corresponding lie group.
\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618781
\begin_inset Formula 
\[
so(3)\doteq\left\{ \hat{\omega}\in\mathbb{R}^{3\times3}\mid\omega\in\mathbb{R}^{3}\right\} 
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618781
Once we have so(3), we need a way to map SO(3) to so(3).
 
\change_deleted 1699939148 1515788165
,
\change_deleted 215191885 1516618781
It is obvious that the solution for 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:3"

\end_inset

 is the matrix exponential 
\begin_inset Formula $e^{\hat{\omega}t}$
\end_inset

, where
\change_inserted 1699939148 1515788319
 
\change_deleted 215191885 1521776300
[THIS IS TOO SIMILAR to source work elsewhere, bordering on plagiarism –
 REMOVE!, you should not be going into this much detail for the differential
 properties of rotations as this is not something you are comfortable with,
 write down the most important relationships, i.e., the differential of the
 rotation and why you need it for SfM, you will want to define sim3 transforms
 and how they are distinct from se3 transforms then leave this topic]
\change_unchanged

\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
\begin_inset Formula 
\[
e^{\hat{\omega}t}=I+\hat{\omega}t+\frac{(\hat{\omega}t)^{2}}{2!}+\cdots+\frac{(\hat{\omega}t)^{n}}{n!}+\cdots
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
Hence, we have 
\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
\begin_inset Formula 
\begin{equation}
R(t)=e^{\hat{\omega}t}\label{eq:4}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
The above equation represents a rotation around the axis 
\begin_inset Formula $\omega\in\mathbb{R}^{3}$
\end_inset

 by an angle of 
\begin_inset Formula $t$
\end_inset

 radians.This map from the space 
\begin_inset Formula $so(3)$
\end_inset

 to 
\begin_inset Formula $SO(3)$
\end_inset

 is called the exponential map.
\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
\begin_inset Formula 
\[
exp:so(3)\rightarrow SO(3);\hat{\omega}\mapsto e^{\hat{\omega}}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
And the inverse mapping is obtained by logarithm of SO(3)
\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
\begin_inset Formula 
\[
log:SO(3)\rightarrow so(3);log(R)\mapsto\hat{\omega}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
We can extend this to full rigid body motion which also involves the translation
 along with the rotation.From 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:2"

\end_inset

, the continuous rigid body trajectory on SE(3) is given by
\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
\begin_inset Formula 
\[
g(t)=\left[\begin{array}{cc}
R(t) & T(t)\\
0 & 1
\end{array}\right]\in\mathbb{R}^{4\times4}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
With above representation, we have
\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
\begin_inset Formula 
\[
\dot{g}(t)g^{-1}(t)=\left[\begin{array}{cc}
\dot{R}(t)R^{T}(t) & \dot{T}(t)-\dot{R}(t)R^{T}(t)T(t)\\
0 & 0
\end{array}\right]\in\mathbb{R}^{4\times4}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
with 
\begin_inset Formula $\upsilon(t)=\dot{T}(t)-\hat{\omega}(t)T(t)\in\mathbb{R}^{3}$
\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:3-1"

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
\begin_inset Formula 
\begin{equation}
\hat{\xi(t)}=\left[\begin{array}{cc}
\hat{\omega}(t) & \upsilon(t)\\
0 & 0
\end{array}\right]\in\mathbb{R}^{4\times4}\label{eq:6}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
then we have
\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
\begin_inset Formula 
\[
\dot{g}(t)=(\dot{g}(t)g^{-1}(t))g(t)=\hat{\xi(t)}g(t)
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
where 
\begin_inset Formula $\hat{\xi}$
\end_inset

 is called the twist and can be used to approximate g(t) locally
\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
\begin_inset Formula 
\[
g(t+dt)\approx g(t)+\hat{\xi(t)}g(t)d(t)=(I+\hat{\xi(t)}dt)g(t)
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
Also the twist represent the tangent space (or Lie algebra) of the matrix
 group SE(3).
\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
\begin_inset Formula 
\[
se(3)\doteq\left\{ \hat{\xi}=\left[\begin{array}{cc}
\hat{\omega} & \upsilon\\
0 & 0
\end{array}\right]\mid\hat{\omega}\in so(3),\upsilon\in\mathbb{R}^{3}\right\} \subset\mathbb{R}^{4\times4}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
Similar 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:4"

\end_inset

.
 with the initial condition of 
\begin_inset Formula $g(0)=1,$
\end_inset

we have
\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
\begin_inset Formula 
\[
g(t)=e^{\hat{\xi}t}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
where
\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
\begin_inset Formula 
\[
e^{\hat{\xi}t}=I+\hat{\xi}t+\frac{(\hat{\xi}t)^{2}}{2!}+\cdots+\frac{(\hat{\xi}t)^{n}}{n!}+\cdots
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
This defines the the exponential map from the space 
\begin_inset Formula $se(3)$
\end_inset

 to 
\begin_inset Formula $SE(3)$
\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
\begin_inset Formula 
\[
exp:se(3)\rightarrow SE(3);\hat{\xi}\mapsto e^{\hat{\xi}}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
and as before inverse to the exponential map is defined by logarithm 
\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
\begin_inset Formula 
\[
log:SE(3)\rightarrow se(3);log(g)\mapsto\hat{\xi}
\]

\end_inset


\change_inserted 215191885 1517225203

\end_layout

\begin_layout Subsubsection*

\change_inserted 215191885 1521739083
Visual Odometer
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521739096
With this efficient representation for rigid body transformation eq 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:sim3"

\end_inset

,the trajectory of camera is determined by the process of direct image alignment
 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Image-alignment"

\end_inset

, i.e we can incrementally find the relative pose 
\begin_inset Formula $\xi\in SIM(3)$
\end_inset

that minimize the photometric error between image pixels.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521045043
\begin_inset Formula 
\begin{equation}
\widehat{\xi}=\min_{\xi}\underset{\mathbf{x}}{\sum}\left(\mathbf{I}(\omega(\mathbf{x},\xi))-\mathbf{T}(\mathbf{x})\right)^{2}\label{eq:Direct image alignment}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521045080
In equation (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Direct image alignment"

\end_inset

), the warp function, 
\begin_inset Formula $\omega(\mathbf{x},\xi)$
\end_inset

, maps pixel locations, 
\begin_inset Formula $\mathbf{x}$
\end_inset

, in the template to pixel locations in the image 
\begin_inset Formula $\mathbf{I}(\mathbf{x})$
\end_inset

 using the warp transformation parameters 
\begin_inset Formula $\xi$
\end_inset

.
 For direct correspondence estimation, 
\begin_inset Formula $\xi$
\end_inset

 is a pose transformation of the viewing camera represented as a unknown
 6x1 vector.
 We then seek the camera pose transformation parameters, 
\begin_inset Formula $\widehat{\xi}$
\end_inset

, that minimize equation which provides the camera pose change that best
 explain the differences in these two of the same scene.
 Given two images and the camera pose change between them, one can take
 the information in one image, and through the warp function and map these
 values into the viewpoint of the other image establishing a correspondence.
 In this case the theoretical difference between expected and observed values
 for the image pair is zero if the camera pose change is known exactly and
 sensor noise and other outside influences are ignored.
 
\end_layout

\begin_layout Subsection

\change_inserted 215191885 1521739054
Estimation of the 3D structure
\change_unchanged

\end_layout

\begin_layout Subsubsection*
Camera model
\end_layout

\begin_layout Standard
The 2D image is formed by capturing the light energy (irradiance) for every
 pixel, this process can be mathematically represented by thin lens camera
 model, which describes the relationship between the three-dimensional coordinat
ors to its projection onto the image plane.
 The thin lens model is represented by a optical axis and a perpendicular
 plane called the focal plane.The thin lens itself is characterized by its
 focal length and diameter, the focal length is the distance from optic
 center to where all the ray intersect the optic axis, while the point of
 intersection itself is called the focus of the lens.
 One of the important properties to consider is that the rays entering the
 lens through optic center are undeflected while the rest of the rays are.
 With this model the irradiance on the image plane is obtained by the integratio
n of all the energy emitted from region of space contained in the cone determine
d by the geometry of the lens.
 
\end_layout

\begin_layout Standard

\change_deleted 1699939148 1515788341
\begin_inset Graphics
	filename images/thin_lens_Cam.png
	width 8cm
	height 6cm

\end_inset


\change_inserted 1699939148 1515788341

\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 1699939148 1515788341
\begin_inset Graphics
	filename images/thin_lens_Cam.png
	lyxscale 25
	width 8cm
	height 6cm

\end_inset


\change_unchanged

\end_layout

\begin_layout Plain Layout

\change_inserted 1699939148 1515788341
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1516609076
Thin lens camera model 
\change_deleted 215191885 1516609014
Put caption here explain variables and what I'm looking at
\change_unchanged

\end_layout

\end_inset


\change_unchanged

\end_layout

\end_inset


\change_unchanged

\end_layout

\begin_layout Standard
Using similar triangles, from above figure, we obtain the following fundamental
 equation of the thin lens
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\frac{1}{Z}+\frac{1}{z}=\frac{1}{f}
\]

\end_inset


\end_layout

\begin_layout Standard
For the simplification of calculation, we consider a Ideal camera model
 called the pinhole camera model, here the aperture of a thin lens is assumed
 to decreased to zero, all rays are forced to go through the optical center
 o, and therefore they remain undeflected.Consequently,as the aperture of
 the cone decreases to zero, the only points that contribute to the irradiance
 at the image point 
\begin_inset Formula $x=\left[x,y\right]$
\end_inset

 are on a line through the center 0 of the lens.
 If a point p has coordinates 
\begin_inset Formula $X=\left[X,Y,Z\right]$
\end_inset

 relative to a reference frame centered at the optical center 0, with its
 z-axis being the optical axis (of the lens), then it is immediate to see
 from similar triangles in Figure that the coordinates of p and its image
 x are related by the so-called ideal perspective projection.
\end_layout

\begin_layout Standard

\change_deleted 1699939148 1515788385
\begin_inset Graphics
	filename images/pinhole_cam.png
	width 8cm
	height 6cm

\end_inset


\change_inserted 1699939148 1515788385

\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 1699939148 1515788385
\begin_inset Graphics
	filename images/pinhole_cam.png
	width 8cm
	height 6cm

\end_inset


\change_unchanged

\end_layout

\begin_layout Plain Layout

\change_inserted 1699939148 1515788385
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1516609093
Pin-hole camera model 
\change_deleted 215191885 1516609091
Caption here
\change_unchanged

\end_layout

\end_inset


\change_unchanged

\end_layout

\end_inset


\change_unchanged

\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
x=-f\frac{X}{Z}\label{eq:pinhole camera x}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
y=-f\frac{Y}{Z}\label{eq:pinhole camera y}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1521852700
[write projection as a matrix transform similar to ROS]
\change_unchanged
This mapping of 3D point to 2D is called projection and is represented by
 
\begin_inset Formula $\pi$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\pi:R^{3}\rightarrow R^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
This is also written as 
\begin_inset Formula $x=\pi(X)$
\end_inset

.
 
\end_layout

\begin_layout Standard
The negative sign in the 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:pinhole camera x"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:pinhole camera y"

\end_inset

 makes the object appear upside down on the image plan, we can handle this
 by moving the image plane to front of optic center to 
\begin_inset Formula $\{z=-f\}$
\end_inset

 this will make 
\begin_inset Formula $(x,y)\rightarrow(-x,-y).$
\end_inset

There for the equation 2 and 3 changes to 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
x=f\frac{X}{Z}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
y=f\frac{Y}{Z}
\]

\end_inset


\end_layout

\begin_layout Standard
This can be represented in matrix form as 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
x=\left[\begin{array}{c}
x\\
y
\end{array}\right]=\frac{f}{Z}\left[\begin{array}{c}
X\\
Y
\end{array}\right]
\]

\end_inset

 In homogeneous coordinates, this relationship can be modified as
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
Z\left[\begin{array}{c}
x\\
y\\
1
\end{array}\right]=\left[\begin{array}{cccc}
f & 0 & 0 & 0\\
0 & f & 0 & 0\\
0 & 0 & 1 & 0
\end{array}\right]\left[\begin{array}{c}
X\\
Y\\
Z\\
1
\end{array}\right]
\]

\end_inset


\end_layout

\begin_layout Standard
The above equation can be decomposed into
\begin_inset Formula 
\[
\left[\begin{array}{cccc}
f & 0 & 0 & 0\\
0 & f & 0 & 0\\
0 & 0 & 1 & 0
\end{array}\right]=\left[\begin{array}{ccc}
f & 0 & 0\\
0 & f & 0\\
0 & 0 & 1
\end{array}\right]\left[\begin{array}{cccc}
1 & 0 & 0 & 0\\
0 & 1 & 0 & 0\\
0 & 0 & 1 & 0
\end{array}\right]
\]

\end_inset


\end_layout

\begin_layout Standard
with
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
K_{f}=\left[\begin{array}{cccc}
f & 0 & 0 & 0\\
0 & f & 0 & 0\\
0 & 0 & 1 & 0
\end{array}\right]\in R^{3\times3},\Pi_{0}=\left[\begin{array}{cccc}
1 & 0 & 0 & 0\\
0 & 1 & 0 & 0\\
0 & 0 & 1 & 0
\end{array}\right]\in\mathbb{R}{}^{3\times4}
\]

\end_inset


\end_layout

\begin_layout Standard
The matrix 
\begin_inset Formula $\Pi_{0}$
\end_inset

is a standard projection matrix.
\end_layout

\begin_layout Standard
With the rigid body transformation from 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:2"

\end_inset

, we can represent the overall geometric model for an ideal camera as below
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\lambda\left[\begin{array}{c}
x^{\prime}\\
y^{\prime}\\
1
\end{array}\right]=\left[\begin{array}{ccc}
f & 0 & 0\\
0 & f & 0\\
0 & 0 & 1
\end{array}\right]\left[\begin{array}{cccc}
1 & 0 & 0 & 0\\
0 & 1 & 0 & 0\\
0 & 0 & 1 & 0
\end{array}\right]\left[\begin{array}{cc}
R & T\\
0 & 1
\end{array}\right]\left[\begin{array}{c}
X_{0}\\
Y_{0}\\
Z_{0}\\
1
\end{array}\right]\label{eq:ideal camera model}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Where 
\begin_inset Formula $\lambda$
\end_inset

 is the unknown scale factor.
\end_layout

\begin_layout Standard
However, the above equation represents the ideal model where the retinal
 frame centered at the optical center with one axis aligned with the optical
 axis.But in practice, this does not true and the origin of the image coordinate
 frame typically in the upper-left corner of the image.we need to address
 this relationship between the retinal plane coordinate frame and the pixel
 array in our camera model.
 This can be represented by a special matrix as following
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
K_{s}=\left[\begin{array}{ccc}
s_{x} & s_{\theta} & o_{x}\\
0 & s_{y} & o_{y}\\
0 & 0 & 1
\end{array}\right]\in\mathbb{R}^{3\times3}
\]

\end_inset


\end_layout

\begin_layout Standard
with these new parameters the we can represent the interstice parameters
 of camera as following
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
K=K_{s}K_{f}=\left[\begin{array}{ccc}
s_{x} & s_{\theta} & o_{x}\\
0 & s_{y} & o_{y}\\
0 & 0 & 1
\end{array}\right]\left[\begin{array}{ccc}
f & 0 & 0\\
0 & f & 0\\
0 & 0 & 1
\end{array}\right]=\left[\begin{array}{ccc}
fs_{x} & s_{\theta} & o_{x}\\
0 & fs_{y} & o_{y}\\
0 & 0 & 1
\end{array}\right]
\]

\end_inset


\end_layout

\begin_layout Standard
where
\end_layout

\begin_layout Itemize
\begin_inset Formula $o_{x}$
\end_inset

: x-coordinate of the principal point in pixels, 
\end_layout

\begin_layout Itemize
\begin_inset Formula $o_{y}$
\end_inset

: y-coordinate of the principal point in pixels, 
\end_layout

\begin_layout Itemize
\begin_inset Formula $fs_{x}$
\end_inset

= 
\begin_inset Formula $\alpha_{x}$
\end_inset

 : size of unit length in horizontal pixels, 
\end_layout

\begin_layout Itemize
\begin_inset Formula $fs_{y}$
\end_inset

= 
\begin_inset Formula $\alpha_{y}$
\end_inset

 : size of unit length in vertical pixels, 
\end_layout

\begin_layout Itemize
\begin_inset Formula $\frac{\alpha_{x}}{\alpha_{y}}$
\end_inset

: aspect ratio 
\begin_inset Formula $\sigma$
\end_inset

, 
\end_layout

\begin_layout Itemize
\begin_inset Formula $fs_{\theta}$
\end_inset

: skew of the pixel, often close to zero.
\end_layout

\begin_layout Standard
Now, the ideal camera model 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:ideal camera model"

\end_inset

can be updated as 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\lambda\left[\begin{array}{c}
x^{\prime}\\
y^{\prime}\\
1
\end{array}\right]=\left[\begin{array}{ccc}
s_{x} & s_{\theta} & o_{x}\\
0 & s_{y} & o_{y}\\
0 & 0 & 1
\end{array}\right]\left[\begin{array}{cccc}
1 & 0 & 0 & 0\\
0 & 1 & 0 & 0\\
0 & 0 & 1 & 0
\end{array}\right]\left[\begin{array}{cc}
R & T\\
0 & 1
\end{array}\right]\left[\begin{array}{c}
X_{0}\\
Y_{0}\\
Z_{0}\\
1
\end{array}\right]\label{eq:camera model with internsic parameter}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
In the matrix notation,
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\lambda x=K\Pi_{0}gX_{0}\label{eq:camera model with internsic parameter matrix rep}
\end{equation}

\end_inset

 
\end_layout

\begin_layout Standard
To summarize, 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:camera model with internsic parameter matrix rep"

\end_inset

 represents the projection of three-dimensional coordinates 
\begin_inset Formula $X_{0}$
\end_inset

 by camera with orientation (extrinsic parameters)
\begin_inset Formula $g$
\end_inset

 and intrinsic parameters K, onto two-dimensional coordinate 
\begin_inset Formula $x$
\end_inset

 with known scale 
\begin_inset Formula $\lambda$
\end_inset


\end_layout

\begin_layout Standard
In addition to above linear distortion, if a camera has a wide field of
 view, there will be a significant distortion along radial directions called
 radial distortion.
 Such a distortion can be models by 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
x=x_{d}(1+a_{1}r^{2}+a_{2}r^{4})
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
y=y_{d}(1+a_{1}r^{2}+a_{2}r^{4})
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $(x_{d},y_{d})$
\end_inset

 are the coordinates of the distorted points, 
\begin_inset Formula $a_{1},a_{2}$
\end_inset

are the coefficients of radial distortion and 
\begin_inset Formula $r$
\end_inset

 is the radius of the radial distortion.
\change_inserted 215191885 1517237472

\end_layout

\begin_layout Subsubsection*
Epipolar geometry
\change_inserted 215191885 1521650966

\begin_inset CommandInset label
LatexCommand label
name "subsec:Epipolar-geometry"

\end_inset


\change_unchanged

\end_layout

\begin_layout Standard
Consider two images of the same point p from two camera position with relative
 pose 
\begin_inset Formula $(R,T)$
\end_inset

, where 
\begin_inset Formula $R\in SO(3)$
\end_inset

 is the relative orientation and 
\begin_inset Formula $T\in\mathbb{R}^{3}$
\end_inset

is the relative position,then if 
\begin_inset Formula $X_{1},X_{2}\in\mathbb{R}^{3}$
\end_inset

 are the 3-D coordinates of a point p relative to the two camera frames,
 by the rigid-body transformation we have
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
X_{2}=RX_{1}+T
\]

\end_inset

Now,let 
\begin_inset Formula $x_{1},x_{2}\in\mathbb{R}^{3}$
\end_inset

be the homogeneous coordinates of the projection of the same point p in
 the two image planes with respective unknown scales of 
\begin_inset Formula $\lambda_{1}$
\end_inset

and 
\begin_inset Formula $\lambda_{2}$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\lambda_{2}x_{2}=R\lambda_{1}x_{1}+T
\]

\end_inset


\end_layout

\begin_layout Standard
By multiplying both the side by 
\begin_inset Formula $\hat{T}$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\lambda_{2}\hat{T}x_{2}=\hat{T}R\lambda_{1}x_{1}
\]

\end_inset


\end_layout

\begin_layout Standard
By multiplying both the side by 
\begin_inset Formula $x_{2}$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
0=x_{2}^{T}\hat{T}R\lambda_{1}x_{1}\label{eq:epipolar constraint}
\end{equation}

\end_inset


\change_inserted 215191885 1521916612

\end_layout

\begin_layout Standard

\change_inserted 215191885 1521916613
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521916613
\begin_inset Graphics
	filename images/epipolar_geo.png
	width 8cm
	height 6cm

\end_inset


\end_layout

\begin_layout Plain Layout

\change_inserted 215191885 1521916613
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1521916622
Epipolar geometry
\end_layout

\end_inset


\end_layout

\end_inset


\change_unchanged

\end_layout

\begin_layout Standard

\change_deleted 215191885 1516609323
\begin_inset Graphics
	filename images/epipolar_geo.png
	width 8cm
	height 6cm

\end_inset


\change_unchanged

\end_layout

\begin_layout Standard
This is the epipolar constraint and the matrix 
\begin_inset Formula $E=\hat{T}R$
\end_inset

 is called the essential matrix.It encodes the relative pose between the
 two cameras.
 Geometrically, it impose that the The vector connecting the first camera
 center 
\begin_inset Formula $o_{1}$
\end_inset

 and the point p, the vector connecting 
\begin_inset Formula $o_{2}$
\end_inset

 and p, and the vector connecting the two optical centers 
\begin_inset Formula $o_{1}$
\end_inset

 and 
\begin_inset Formula $o_{2}$
\end_inset

 clearly form a triangle.
 Therefore, the three vectors lie on the same plane.Hence the their triple
 product which measures the volume of the parallelepiped is zero.
 
\change_inserted 215191885 1521739288

\end_layout

\begin_layout Subsubsection*

\change_inserted 215191885 1521739289
Stereo correspondence and Disparity Estimation
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521739289
We can estimate the depth of object from a pair of images captured by the
 same camera with a small transnational change but the same orientation.
 The amount of distance that the object have shifted from first image to
 second is called the disparity
\begin_inset CommandInset citation
LatexCommand cite
key "Schmidt2002DenseDM"

\end_inset

 and this information is useful in computing the depth of the object itself.
 With the epiplore constraint 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Epipolar-geometry"

\end_inset

 and known translation between the pair of images, we can efficiently compute
 disparity of image, this approach is called the fixed baseline disparity
 mapping since the baseline between pair of image are known and they are
 fixed .
 This process starts with finding the correspondence between images.
 Stereo correspondence is the problem of finding which part of one image
 correspond to which parts of another image, where the difference are due
 to the movement of the camera.
 As discussed in 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Solving-for-Image"

\end_inset

 the correspondence can be either dense or sparse.
 In dense correspondence image intensities are used, where as in sparse
 only image feature are used.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521916660
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521916660
\begin_inset Graphics
	filename images/disparity_2.png
	width 8cm
	height 6cm

\end_inset


\end_layout

\begin_layout Plain Layout

\change_inserted 215191885 1521916660
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1521916731
Disparity Computation
\begin_inset CommandInset label
LatexCommand label
name "fig:Disparity-Computation"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521739289
Let 
\begin_inset Formula $x_{L}$
\end_inset

 and 
\begin_inset Formula $x_{R}$
\end_inset

 be the one of the established correspondences in the image pair 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Disparity-Computation"

\end_inset

 and 
\begin_inset Formula $P=(x_{p},y_{p},z_{p})$
\end_inset

 be the 3D location of object, from the epiploar constraint we know that
 these corresponding image points and 3D position of the object forms a
 triangle as depicted in the image 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Disparity-Computation"

\end_inset

 Since the focal length 
\begin_inset Formula $f$
\end_inset

 of the camera and baseline 
\begin_inset Formula $B=2l$
\end_inset

 are known, we can compute the disparity 
\begin_inset Formula $d$
\end_inset

 and consequently the depth of object 
\begin_inset Formula $z_{p}$
\end_inset

 with properties of similarity triangles as fallowing.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521739289
\begin_inset Formula 
\[
d=x_{L}-x_{R}=f\left(\frac{x_{p}+l}{z_{p}}-\frac{x_{p}-l}{z_{p}}\right)
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521739289
\begin_inset Formula 
\[
d=\frac{2fl}{z_{p}}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521739289
\begin_inset Formula 
\[
z_{p}=\frac{fB}{d}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521741278
However, the problem with the fixed baseline disparity computation is that
 the error grows quadratically with depth, as a result the nearer depths
 have better accuracy but not the further depths.This can addressed efficiently
 by using variable baseline/resolution instead of fixed baseline
\begin_inset CommandInset citation
LatexCommand cite
key "4587671"

\end_inset

, where the baselines and focal length are selected with proportion to the
 depth, which helps us archive the constant accuracy over all the depths.
 Further performance and accuracy improvements could be achieved by using
 probabilistic approach of adaptive baseline
\begin_inset CommandInset citation
LatexCommand cite
key "6751290"

\end_inset

 which explicitly takes advantage of the fact that in a video, small- baseline
 frames are available before large-baseline frames.
 In this case, for each pixel a suitable frame is selected for disparity
 search.
\end_layout

\begin_layout Subsection

\change_inserted 215191885 1521741278
Solving for Scene Geometry
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521741278
There are generically two different approaches for 3D reconstruction referred
 to as 
\emph on
sparse
\emph default
 and 
\emph on
dense
\emph default
.
 Sparse methods reconstruct the 3D scene geometry only for a select subset
 of the entire image data 
\begin_inset CommandInset citation
LatexCommand cite
key "klein07parallel"

\end_inset

.
 This subset is often corner locations or locations marked by some type
 of feature extraction, e.g., SIFT or SURF.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Lowe2004,Bay:2008:SRF:1370312.1370556"

\end_inset

 This results in a sparse description of the 3D scene in terms of a point
 cloud.
 In contrast, dense methods 
\begin_inset CommandInset citation
LatexCommand cite
key "6696650"

\end_inset

 reconstruct as many 3D geometric locations as possible and seek to provide
 a complete description of the 3D scene.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521741278
Sparse reconstructions often benefit from having a lower computational cost
 but provide few 3D measurements.
 Dense reconstructions have higher computational cost but provide a much
 more complete description of the 3D scene.
 Dense reconstruction techniques have seen much recent interest, although
 a highly accurate, dense, and real-time SfM approach has remained elusive.
 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521741278
A third class of algorithms, referred to as 
\emph on
semi-dense
\emph default
 algorithms 
\begin_inset CommandInset citation
LatexCommand cite
key "6751290"

\end_inset

, seeks to strike a compromise between the sparse and dense methods.
 The reconstruction techniques used are most similar to dense methods, however,
 only a subset of all image pixels are reconstructed.
 These approaches leverage the high accuracy of dense reconstruction techniques,
 but are sparse enough to allow for real-time operation.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521741278
Here, reconstruction is limited to those pixels which possess high intensity
 gradient values.
 These regions often correspond to scene geometries such as edges, corners,
 and curves and to other areas of the scene that are highly textured.
 The thought here is that regions of the image that possess large changes
 in intensity convey more information than regions that possess less, thus
 semi-dense reconstructions provide a compressed version of the total scene
\change_unchanged

\end_layout

\begin_layout Subsubsection
The eight-point linear algorithm
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521741584
One of the simple closed-form algorithm which estimates camera trajectory
 and 3D depths from pair of images is The eight-point algorithm, 
\change_deleted 215191885 1521741600
With epipolar constrain between two images, we should be able to retrieve
 the relative pose of the cameras.
 The eight-point linear algorithm is a simple closed-form algorithm
\change_unchanged
, it consists of two steps: First a matrix E is recovered from a number
 of epipolar constraints; then relative translation and orientation are
 extracted from E.
\end_layout

\begin_layout Standard
The entries of E are denoted by 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
E=\left[\begin{array}{ccc}
e_{11} & e_{12} & e_{13}\\
e_{21} & e_{22} & e_{23}\\
e_{31} & e_{32} & e_{33}
\end{array}\right]\in\mathbb{R}^{3\times3}
\]

\end_inset


\end_layout

\begin_layout Standard
The matrix E is reshaped into vector 
\begin_inset Formula $E\in\mathbb{R}^{9}$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
E=\left[e_{11},e_{21},e_{31},e_{12},e_{22},e_{32},e_{13},e_{23},e_{33}\right]^{T}
\]

\end_inset


\end_layout

\begin_layout Standard
and for 
\begin_inset Formula $x_{1}=\left[x_{1,}y_{1},z_{1}\right]^{T}\in\mathbb{R}^{3}$
\end_inset

and 
\begin_inset Formula $x_{2}=\left[x_{2,}y_{2},z_{2}\right]^{T}\in\mathbb{R}^{3}$
\end_inset

 define 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
a=[x_{1}x_{2},x_{1}y_{2},x_{1}z_{2},y_{1}x_{2},y_{1}y_{2},y_{1}z_{2},z_{1}x_{2},z_{1}y_{2},z_{1}z_{2}]\in\mathbb{R}^{9}
\]

\end_inset


\end_layout

\begin_layout Standard
With these new notations, we can rewrite the 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:epipolar constraint"

\end_inset

 as below
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
a^{T}E=0
\]

\end_inset


\end_layout

\begin_layout Standard
this representation emphasizes the linear dependence of the epipolar constraint
 on the elements of the essential matrix.
\end_layout

\begin_layout Standard
Now,with a set of corresponding image points 
\begin_inset Formula $(x_{1}^{j},x_{2}^{j}),j=1,2,....,n$
\end_inset

 we can define a matrix 
\begin_inset Formula $\chi\in\mathbb{R}^{n\times9}$
\end_inset

 associated with these measurements to be
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\chi=\left[a^{1},a^{2},...,a^{n}\right]^{T}
\]

\end_inset


\end_layout

\begin_layout Standard
In the absence of noise, the vector E satisfies
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\chi E=0
\]

\end_inset


\end_layout

\begin_layout Standard
In order to obtain the unique solution, the rank of the matrix 
\begin_inset Formula $\chi\in\mathbb{R}^{n\times9}$
\end_inset

 needs to be exactly eight.
 This should be the case when we have 
\begin_inset Formula $n\geq8$
\end_inset

 "ideal" corresponding points, hence the name The eight-point linear algorithm.
\end_layout

\begin_layout Standard
Because of errors in correspondences, we try to find the E that minimizes
 the least-squares error function 
\begin_inset Formula $\left\Vert \chi E\right\Vert ^{2}$
\end_inset

.We do this by choosing eigenvector of 
\begin_inset Formula $\chi^{T}\chi$
\end_inset

 that corresponds to its smallest eigenvalue.
\end_layout

\begin_layout Standard
Once we have E, we need to extract the pose (
\begin_inset Formula $R\in SO(3)$
\end_inset

 and 
\begin_inset Formula $T\in\mathbb{R}^{3}$
\end_inset

) from E, we know that 
\begin_inset CommandInset citation
LatexCommand cite
key "41368"

\end_inset

a nonzero matrix 
\begin_inset Formula $E\in\mathbb{R}^{3}$
\end_inset

 is an essential matrix if and only if E has a singular value decomposition
 
\begin_inset Formula $(SVD)E=U\varSigma V^{T}$
\end_inset

with 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\varSigma=diag\left\{ \sigma,\sigma,0\right\} 
\]

\end_inset


\end_layout

\begin_layout Standard
for some 
\begin_inset Formula $\sigma>0$
\end_inset

 and 
\begin_inset Formula $U,V,\in SO(3)$
\end_inset


\end_layout

\begin_layout Standard
With this we can obtain two relative pose
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
(\hat{T}_{1},R_{1})=(UR_{Z}(+\frac{\pi}{2})\Sigma U^{T},UR_{Z}^{T}(+\frac{\pi}{2})V^{T})
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
(\hat{T}_{2},R_{2})=(UR_{Z}(-\frac{\pi}{2})\Sigma U^{T},UR_{Z}^{T}(-\frac{\pi}{2})V^{T})
\]

\end_inset


\end_layout

\begin_layout Standard
Among these one with that gives the meaningful (positive) depth are selected
 as the valid pose.
\end_layout

\begin_layout Standard
With 
\begin_inset Formula $R_{Z}(\pm\frac{\pi}{2})=\left[\begin{array}{ccc}
0 & \pm1 & 0\\
\pm1 & 0 & 0\\
0 & 0 & 1
\end{array}\right]$
\end_inset


\end_layout

\begin_layout Paragraph
Structure Reconstruction
\end_layout

\begin_layout Standard
One remaining thing to find is the position of points in three-dimension
 by recovering their depths relative to each camera frame.With the estimated
 pose (Translation is T is defined up to the scale 
\begin_inset Formula $\gamma$
\end_inset

) and point correspondence, we have
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\lambda_{2}^{j}x_{2}^{j}=\lambda_{1}^{j}Rx_{1}^{j}+\gamma T
\]

\end_inset

 for 
\begin_inset Formula $j=1,2,3...,n$
\end_inset


\end_layout

\begin_layout Standard
where, 
\begin_inset Formula $\lambda_{1}$
\end_inset

and 
\begin_inset Formula $\lambda_{2}$
\end_inset

are depths with respect to the first and second camera frames, respectively.
 One of the depths is redundant, if 
\begin_inset Formula $\lambda_{1}$
\end_inset

 is known, we can estimate 
\begin_inset Formula $\lambda_{2}$
\end_inset

 as a function of 
\begin_inset Formula $(R,T)$
\end_inset

.Hence we can eliminate, say, 
\begin_inset Formula $\lambda_{2}$
\end_inset

 from the above equation by multiplying both sides by 
\begin_inset Formula $\hat{x_{2}}$
\end_inset

 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
0=\lambda_{1}^{j}x_{2}^{j}Rx_{1}^{j}+\gamma x_{2}^{j}T
\]

\end_inset


\end_layout

\begin_layout Standard
This is represented as linear equations
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
M^{j}\bar{\lambda^{j}}=\left[\hat{x_{2}^{j}}Rx_{1}^{j},\hat{x_{2}^{j}}T\right]\left[\begin{array}{c}
\lambda_{1}^{j}\\
\gamma
\end{array}\right]=0
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $M^{j}=\left[\hat{x_{2}^{j}}Rx_{1}^{j},\hat{x_{2}^{j}}T\right]\in\mathbb{R}^{3\times2}$
\end_inset

and 
\begin_inset Formula $\bar{\lambda^{j}}=\left[\begin{array}{c}
\lambda_{1}^{j}\\
\gamma
\end{array}\right]\in\mathbb{R}^{2}$
\end_inset

 for 
\begin_inset Formula $j=1,2,3...,n$
\end_inset


\end_layout

\begin_layout Standard
since all n equations above share the same 
\begin_inset Formula $\gamma$
\end_inset

; we define a vector 
\begin_inset Formula $\vec{\lambda}=\left[\lambda_{1}^{1},\lambda_{1}^{2},...,\lambda_{1}^{2},\gamma\right]$
\end_inset

 and a matrix 
\begin_inset Formula $M\in\mathbb{R}^{3n\times(n+1)}$
\end_inset

as
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
M=\left[\begin{array}{cccccc}
\hat{x}_{2}^{1}Rx_{1}^{1} & 0 & 0 & 0 & 0 & \hat{x}_{2}^{1}T\\
0 & \hat{x}_{2}^{2}Rx_{1}^{2} & 0 & 0 & 0 & \hat{x}_{2}^{2}T\\
0 & 0 & \ddots & 0 & 0 & \vdots\\
0 & 0 & 0 & \hat{x}_{2}^{n-1}Rx_{1}^{n-1} & 0 & \hat{x}_{2}^{n-1}T\\
0 & 0 & 0 & 0 & \hat{x}_{2}^{n}Rx_{1}^{n} & \hat{x}_{2}^{n}T
\end{array}\right]
\]

\end_inset


\end_layout

\begin_layout Standard
Then the equation
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
M\overrightarrow{\lambda}=0
\]

\end_inset


\end_layout

\begin_layout Standard
determines all the unknown depths up to a single universal scale.
 The linear least-squares estimate of 
\begin_inset Formula $\overrightarrow{\lambda}$
\end_inset

 is simply the eigenvector of 
\begin_inset Formula $M^{T}M$
\end_inset

s that corresponds to its smallest eigenvalue.
\change_inserted 215191885 1521047989

\end_layout

\begin_layout Standard

\end_layout

\begin_layout Section

\change_inserted 215191885 1521070857
RGBD Measurement Noise
\begin_inset CommandInset label
LatexCommand label
name "subsec:RGBD_Measurement-Noise"

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521070857
The proposed fusion algorithm relies on experimental studies of accuracy
 and noise for RGBD sensor measurement, e.g., the Kinect sensor.
 Research in 
\begin_inset CommandInset citation
LatexCommand cite
key "7925286"

\end_inset

 shows that a Gaussian noise model provides a good fit to observed measurement
 errors on planar targets where the distribution parameters are mean 
\begin_inset Formula $0$
\end_inset

 and standard deviation
\begin_inset Formula $\sigma_{Z}=\frac{m}{2f_{x}b}Z^{2}$
\end_inset

 for depth measurements where 
\begin_inset Formula $\frac{m}{f_{x}b}=-2.85e^{-3}$
\end_inset

 is the linearized slope for the normalized disparity empirically found
 in 
\begin_inset CommandInset citation
LatexCommand cite
key "7925286"

\end_inset

.
 Since 3D the coordinates for 
\begin_inset Formula $(X,Y)$
\end_inset

 are a function of both the pixel location and the depth, their distributions
 are also known as shown below:
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521070857
\begin_inset Formula 
\begin{equation}
\begin{array}{ccc}
\sigma_{X} & = & \frac{x_{im}-c_{x}+\delta_{x}}{f_{x}}\sigma_{Z}=\frac{x_{im}-c_{x}+\delta_{x}}{f_{x}}(1.425e^{-3})Z^{2}\\
\sigma_{Y} & = & \frac{y_{im}-c_{y}+\delta_{y}}{f_{y}}\sigma_{Z}=\frac{y_{im}-c_{y}+\delta_{y}}{f_{y}}(1.425e^{-3})Z^{2}\\
\sigma_{Z} & = & \frac{m}{f_{x}b}Z^{2}\sigma_{d'}=(1.425e^{-3})Z^{2}
\end{array}\label{eq:noise_models}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521070857
These equations indicate that 3D coordinate measurement uncertainty increases
 as a quadratic function of the depth for all three coordinate values.
 However, the quadratic coefficient for the 
\begin_inset Formula $(X,Y)$
\end_inset

 coordinate standard deviation is at most half that in the depth direction,
 i.e., 
\begin_inset Formula $(\sigma_{X},\sigma_{Y})\approx0.5\sigma_{Z}$
\end_inset

 at the image periphery where 
\begin_inset Formula $\frac{x-c_{x}}{f}\approx0.5$
\end_inset

, and this value is significantly smaller for pixels close to the optical
 axis.
\change_unchanged

\end_layout

\begin_layout Section
Non linear Optimization
\end_layout

\begin_layout Standard
In practice, because of the noise in image correspondence and other errors
 we cannot measure the actual coordinates but only their noisy versions,
 say
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\widetilde{x}_{1}^{j}=x_{1}^{j}+\omega_{1}^{j}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\widetilde{x}_{2}^{j}=x_{2}^{j}+\omega_{2}^{j}
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $x_{1}^{j}$
\end_inset

and 
\begin_inset Formula $x_{2}^{j}$
\end_inset

are the ideal image coordinates and 
\begin_inset Formula $\omega_{1}^{j}=\left[\omega_{11}^{j},\omega_{12}^{j},0\right]^{T}$
\end_inset

and 
\begin_inset Formula $\omega_{2}^{j}=\left[\omega_{21}^{j},\omega_{22}^{j},0\right]^{T}$
\end_inset

are localization errors (called residuals) in the correspondence.Therefore,
 we need a way to optimize the parameters 
\begin_inset Formula $(x,R,T)$
\end_inset

 that minimize this errors.
\end_layout

\begin_layout Standard
One of the minimalistic approach to optimality is to minimize the squared
 2-norm of residuals, if we choose the first camera frame as the reference
 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\phi(x,R,T,\lambda)=\stackrel[j=1]{n}{\sum}\left\Vert \omega_{1}^{j}\right\Vert +\left\Vert \omega_{2}^{j}\right\Vert ^{2}=\stackrel[j=1]{n}{\sum}\left\Vert \widetilde{x}_{1}^{j}-x_{1}^{j}\right\Vert ^{2}+\left\Vert \widetilde{x}_{2}^{j}-\pi(R\lambda_{1}^{j}x_{1}^{j}+T)\right\Vert ^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
The above error is often called the 
\begin_inset Quotes eld
\end_inset

re-projection error
\begin_inset Quotes erd
\end_inset

, since 
\begin_inset Formula $x_{1}^{j}$
\end_inset

 and 
\begin_inset Formula $x_{2}^{j}$
\end_inset

are the recovered 3-D points projected back onto the image planes.
 This process of minimizing the above expression for the unknowns 
\begin_inset Formula $(R,T,x_{1},\lambda)$
\end_inset

 is known as bundle adjustment
\begin_inset CommandInset citation
LatexCommand cite
key "Triggs:1999:BAM:646271.685629"

\end_inset

.
\change_inserted 215191885 1521909532

\end_layout

\begin_layout Standard

\change_inserted 215191885 1521910863
One of the simplest ways to minimize this squared error is the gradient
 descent method.
 It is a first-order optimization method which aims to determine a local
 minimum of a non-convex cost function by iteratively stepping in the direction
 in which the energy decreases most.
 Hence this method is also called as the steepest descent method.
 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521910909
\begin_inset Formula 
\[
x_{t+1}=x_{t}-\alpha g
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521910964
with step size 
\begin_inset Formula $\alpha$
\end_inset

 and gradient g
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521910909
\begin_inset Formula 
\[
g_{j}=2\underset{i}{\sum}r_{i}\frac{\partial r_{i}}{\partial x_{i}}
\]

\end_inset


\change_unchanged

\end_layout

\begin_layout Standard

\change_deleted 215191885 1521909550
One of the many ways to minimize this squared error is the
\change_inserted 215191885 1521913152
The problem with the gradient descent is that it is only first order approximati
on we can achieve a better results with second order approximation and it
 is done by an efficient approach called the
\change_unchanged
 Gauss-Newton method.
 Gauss-Newton is a iterative method for finding the value of the variables
 which minimizes the sum of squares
\change_inserted 215191885 1521911027
, this method achieves this by assuming that the least squares function
 is locally quadratic and tries to find the the minimum of the quadratic.
\change_deleted 215191885 1521909702
,
\change_inserted 215191885 1521909725
 I
\change_deleted 215191885 1521909725
i
\change_unchanged
t starts with the initial guess and this method does not need the second
 derivatives (Hessian matrix) of the of function, which is often expensive
 and sometimes not possible to compute, instead the Hessian is approximated
 with the Jacobian matrix of the function.
 
\end_layout

\begin_layout Standard
For the least-square function of form
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\underset{x}{min}\underset{i}{\sum}r_{i}(x)^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
Gauss-Newton method iteratively solves 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
x_{t+1}=x_{t}-H^{-1}g
\]

\end_inset


\end_layout

\begin_layout Standard
with gradient g
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
g_{j}=2\underset{i}{\sum}r_{i}\frac{\partial r_{i}}{\partial x_{i}}
\]

\end_inset


\end_layout

\begin_layout Standard
and the Hessian H
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
H_{jk}=2\underset{i}{\sum}r_{i}(\frac{\partial r_{i}}{\partial x_{j}}\frac{\partial r_{i}}{\partial x_{k}}+r_{i}\frac{\partial^{2}r_{i}}{\partial x_{i}\partial x_{k}})
\]

\end_inset


\end_layout

\begin_layout Standard
by dropping the second order term we can approximate the Hessian matrix
 with Jacobian matrix 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
H_{jk}=2\underset{i}{\sum}J_{ij}J_{ik}
\]

\end_inset


\end_layout

\begin_layout Standard
with
\begin_inset Formula 
\[
J_{ij}=\frac{\partial r_{i}}{\partial x_{j}}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521913214
Finaly, t
\change_deleted 215191885 1521913213
T
\change_unchanged
he modification to Gauss-Newton method is 
\change_inserted 215191885 1521907843
t
\change_deleted 215191885 1521907843
T
\change_unchanged
he Levenberg-Marquardt 
\change_inserted 215191885 1521913445
(LVM)
\begin_inset CommandInset citation
LatexCommand cite
key "Gavin2013TheLM"

\end_inset

 
\change_unchanged
algorithm, 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
x_{t+1}=x_{t}-(H+\lambda I)^{-1}g
\]

\end_inset


\end_layout

\begin_layout Standard
this modification is a hybrid between the 
\change_inserted 215191885 1521912344
Gauss-
\change_unchanged
Newton method 
\begin_inset Formula $(\lambda=0)$
\end_inset

 and a gradient descent step size 
\begin_inset Formula $1/\lambda$
\end_inset

 for 
\begin_inset Formula $\lambda\longrightarrow\infty$
\end_inset


\change_inserted 215191885 1521913240
.
 i.e LVM efficiently minimizes the squared error by updating the parameters
 in the steepest-descent direction when the initial guess are far way from
 the solution, and switchs to Gauss-Newton method when small updates are
 required for approximation.
\end_layout

\begin_layout Section

\change_inserted 215191885 1521822765
Graph Optimization
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521839416
SfM is a non-linear least square optimization problem which tries to estimate
 the set of parameters that accurately represents the trajectory of the
 camera and observed 3D points of the object, the complexity of this problem
 increases quickly since we are dealing with multiple features from many
 pairs of images and each pose estimation has a 7 degree of freedom 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:sim3"

\end_inset

.
 One efficient way to handle such a large scale non-linear optimization
 problem is to represent them in a graph
\begin_inset CommandInset citation
LatexCommand cite
key "5979949"

\end_inset

.
 There are two many advantages to this approach, Firstly, the graph representati
on helps with the modular representation of the problem, where each nodes
 is reference image and edge between the nodes are relative pose estimated.
 And more importantly, graph representation helps handle other important
 problems like global optimization and loop closures.
\end_layout

\begin_layout Chapter

\change_inserted 215191885 1521048998
LSD SLAM
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521048998
We are using LSD-SLAM 
\begin_inset CommandInset citation
LatexCommand cite
key "Engel2014"

\end_inset

 as our SfM implementation, it is  a 
\emph on
semi-dense
\emph default
, 
\emph on
direct
\emph default
 method which optimizes the geometry directly on the image intensities.
 LSD-SLAM provides as output a reconstruction of the observed 3D environment
 as a pose-graph of specially designated RGB image keyframes with associated
 semi-dense depth images.
 Direct correspondences are found between every RGB frame and each RGB keyframe
 to estimate the keyframe-to-RGB-frame relative camera pose.
 This is achieved by Levenberg-Marquardt optimization of the photometric
 error between the RGB image pair.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521048998
\begin_inset Formula 
\begin{equation}
E(\xi)=\underset{i}{\sum}(I_{ref}(P_{i})-I(\omega(p_{i},D_{ref}(p_{i}),\xi)))^{2}\label{eq:Photomatric Error}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521074363
Equation (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Photomatric Error"

\end_inset

) shows the specific image alignment object function and formalizes the
 form for equation (
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Image-alignment"

\end_inset

).
 Here, the warp function relies on the current estimate of depth 
\begin_inset Formula $D_{ref}$
\end_inset

 to determine the relative pose 
\begin_inset Formula $\xi\in sim(3)$
\end_inset

.
 The reference depth 
\begin_inset Formula $D_{ref}$
\end_inset

 is the depth associated with current key frame, these depth values could
 be initialized either with random values or with the depth measurements
 from a RGBD sensor to initiate the process.
 For every new frame tracked, the depths associated with the keyframe are
 continuously refined by performing the adaptive baseline stereo 3D reconstructi
on 
\begin_inset CommandInset citation
LatexCommand cite
key "6751290"

\end_inset

 between new tracked frames and the stack of previously tracked frames.
 When there is drastic pose change between the tracked frame and keyframe,
 the current tracked frame is promoted as a keyframe and depth map from
 previous keyframe is propagated to the new keyframe and regularized.
 Concurrently, all the keyframes are added as a nodes to a pose graph that
 stores the relative pose between the keyframes as edges/constraints 
\begin_inset CommandInset citation
LatexCommand cite
key "5979949"

\end_inset

.
 The pose graph stores the global trajectory of the camera in the 3D scene
 and an optimization algorithm processes the pose graph to improve the camera
 pose estimates as new image correspondences are found by image matching
 and loop closures.
 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521917616
The depth computation process involves finding the epiploar lines between
 new tracking frame and reference frames, along which the disparity for
 the pixel is determined.
 This process have two error sources, the error on disparity itself called
 the geometric disparity error and the error which encodes intensity difference
 called the photometric disparity error, The former error accounts for the
 magnitude of the image gradient along the epipolar line , while the later
 one on the angle between the image gradient and the epipolar line.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521759399
\begin_inset Float figure
wide true
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521759399
\begin_inset Graphics
	filename /home/akash/backup/2018_SeCon_RGBDSfM_Fusion/images/LSD SLAM.png
	width 15cm

\end_inset


\end_layout

\begin_layout Plain Layout

\change_inserted 215191885 1521759399
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1521917601
\begin_inset CommandInset label
LatexCommand label
name "fig:LSDSLAM-Overview"

\end_inset

 An overview of the LSD SLAM algorithm.
 Each incoming frame is tracked against the current keyframe.
 If it does not satisfy the criteria for new keyframe creation, it is used
 along with previous tracked frames for refinement of the estimated depth
 values of the keyframe.
 Otherwise, the frame is considered a new keyframe and depth estimates from
 the previous keyframe are propagated and used for initialization of the
 new depth estimates.
\end_layout

\end_inset


\end_layout

\end_inset


\change_unchanged

\end_layout

\begin_layout Section

\change_inserted 215191885 1521070779
Point Cloud Reconstruction
\begin_inset CommandInset label
LatexCommand label
name "subsec:Point-Cloud-Reconstruction"

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521070779
Measured 3D 
\begin_inset Formula $(X,Y,Z)$
\end_inset

 positions of sensed surfaces can be directly computed from the intrinsic
 camera parameters and depth image values.
 Here, the 
\begin_inset Formula $Z$
\end_inset

 coordinate is directly taken as the depth value and the 3D 
\begin_inset Formula $(X,Y)$
\end_inset

 coordinates are computed using the pinhole camera model.
 In a typical pinhole camera model 3D 
\begin_inset Formula $(X,Y,Z)$
\end_inset

 points are projected to 
\begin_inset Formula $(x,y)$
\end_inset

 image locations 
\begin_inset CommandInset citation
LatexCommand cite
key "Ma:2003:IVI:971144"

\end_inset

, e.g., for the image columns the 
\begin_inset Formula $x$
\end_inset

 image coordinate is 
\begin_inset Formula $x=f_{x}\frac{X}{Z}+c_{x}-\delta_{x}$
\end_inset

.
 However, for a depth image, this equation is re-organized to 
\begin_inset Quotes eld
\end_inset

back-project
\begin_inset Quotes erd
\end_inset

 the depth into the 3D scene and recover the 3D 
\begin_inset Formula $(X,Y)$
\end_inset

 coordinates as shown by equation (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:RGBD_reconstruction"

\end_inset

) 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521070779
\begin_inset Formula 
\begin{equation}
\begin{array}{ccc}
X & = & (x+\delta_{x}-c_{x})Z/f_{x}\\
Y & = & (y+\delta_{y}-c_{y})Z/f_{y}\\
Z & = & Z
\end{array}\label{eq:RGBD_reconstruction}
\end{equation}

\end_inset

where 
\begin_inset Formula $Z$
\end_inset

 denotes the sensed depth at image position 
\begin_inset Formula $(x,y)$
\end_inset

, 
\begin_inset Formula $(f_{x},f_{y})$
\end_inset

 denotes the camera focal length (in pixels), 
\begin_inset Formula $(c_{x},c_{x})$
\end_inset

 denotes the pixel coordinate of the image center, i.e., the principal point,
 and 
\begin_inset Formula $(\delta_{x},\delta_{y})$
\end_inset

 denote adjustments of the projected pixel coordinate to correct for camera
 lens distortion.
\end_layout

\begin_layout Section

\change_inserted 215191885 1521070779
Point Cloud Re-Projection
\begin_inset CommandInset label
LatexCommand label
name "subsec:Point-Cloud-Re-Projection"

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521070779
Depth images can be simulated for camera sensor in arbitrary poses by 
\begin_inset Quotes eld
\end_inset

re-projection.
\begin_inset Quotes erd
\end_inset

 For discussion, assume that depth image 
\begin_inset Formula $\mathbf{d}(x,y)$
\end_inset

 has been recorded in the 
\begin_inset Quotes eld
\end_inset

standard
\begin_inset Quotes erd
\end_inset

 camera/optical coordinate system where the origin corresponds to the camera
 focal point, the 
\begin_inset Formula $z$
\end_inset

-axis corresponds to the depth/optical axis extending out into the viewed
 scene, the 
\begin_inset Formula $x$
\end_inset

-axis points towards the right and spans the image columns and the 
\begin_inset Formula $y$
\end_inset

-axis points downward and spans the image rows.
 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521070779
Let 
\begin_inset Formula $\mathbf{R}$
\end_inset

 denote the 3D rotation that rotates the coordinate axes of the standard
 coordinate system to align with the same axes of a second camera having
 arbitrary pose.
 Similarly, Let 
\begin_inset Formula $\mathbf{t}$
\end_inset

 denote the 3D translation vector describing the position of the focal point
 of a second camera having arbitrary pose.
 Using this notation, the re-projection algorithm consists of the following
 three steps:
\end_layout

\begin_layout Enumerate

\change_inserted 215191885 1521070779
Back-project 
\begin_inset Formula $\mathbf{d}(x,y)$
\end_inset

 to create an 
\begin_inset Formula $(X,Y,Z)$
\end_inset

 point cloud (as described in 
\begin_inset Formula $\S$
\end_inset


\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Point-Cloud-Reconstruction"

\end_inset

), 
\end_layout

\begin_layout Enumerate

\change_inserted 215191885 1521070779
Transform, i.e., rotate and translate, each point 
\begin_inset Formula $\mathbf{p}_{i}=[X,Y,Z]^{t}$
\end_inset

 in the point cloud to generate a new point 
\begin_inset Formula $\mathbf{p}_{i}^{'}=[X^{'},Y^{'},Z^{'}]^{t}$
\end_inset

 that lies in a standard optical coordinate system centered on the second
 camera's focal point and having orientation that aligns with corresponding
 
\begin_inset Formula $x,y,z$
\end_inset

-axes using equation (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:coordinate_system_change"

\end_inset

),
\begin_inset Formula 
\begin{equation}
\mathbf{p}_{i}^{'}=\mathbf{R}(\mathbf{p}_{i}-\mathbf{t})\label{eq:coordinate_system_change}
\end{equation}

\end_inset


\end_layout

\begin_layout Enumerate

\change_inserted 215191885 1521070779
Re-project the 
\begin_inset Formula $(X,Y,Z)$
\end_inset

 point cloud using the pinhole camera equations to compute the new depth
 image 
\begin_inset Formula $\mathbf{d}^{'}(x,y)=Z$
\end_inset

 using equation (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:re_projection"

\end_inset

).
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521070779
\begin_inset Formula 
\begin{equation}
\begin{array}{ccc}
x & = & f_{x}(\frac{X^{'}}{Z^{'}})-\delta_{x}+c_{x}\\
y & = & f_{y}(\frac{Y^{'}}{Z^{'}})-\delta_{y}+c_{y}\\
Z & = & Z^{'}
\end{array}\label{eq:re_projection}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521070779
Typically, the re-projected point cloud measurements fall at non-integer
 locations in the new depth image and the values of 
\begin_inset Formula $\mathbf{d}^{'}(x,y)$
\end_inset

 must then be interpolated via bilinear interpolation or some other interpolatio
n scheme (nearest neighbor).
 
\end_layout

\begin_layout Chapter

\change_inserted 215191885 1521071426
Methodology
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521071427
\begin_inset Float figure
wide true
sideways false
status collapsed

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521071427
\begin_inset Graphics
	filename /home/akash/southeastcon/2018_SeCon_RGBDSfM_Fusion/images/overview_coloum_span.png
	lyxscale 20
	width 7in
	height 2.5in
	keepAspectRatio

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1521071427
\begin_inset CommandInset label
LatexCommand label
name "fig:The-complete-process"

\end_inset

An overview of the proposed depth fusion algorithm.
\end_layout

\end_inset


\end_layout

\end_inset

The proposed fusion approach applies the semi-dense monocular reconstruction
 approach referred to as Large Scale Direct (LSD) SLAM 
\begin_inset CommandInset citation
LatexCommand cite
key "Engel2014"

\end_inset

.
 The LSD-SLAM algorithm solves the SfM problem using a 
\emph on
direct
\emph default
 method to compute pixel correspondences and a 
\emph on
semi-dense
\emph default
 method for 3D reconstruction.
 We select this approach as it does not require or impose any prior knowledge
 about the scene structure as required by 
\emph on
dense 
\emph default
reconstruction methods and it gives more 3D estimates than 
\emph on
sparse
\emph default
 approaches while having similar computational cost.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521071427
The LSD-SLAM SfM algorithm consists of the following three components:
\end_layout

\begin_layout Enumerate

\change_inserted 215191885 1521071427
A tracking component that estimates the pose of the camera
\end_layout

\begin_layout Enumerate

\change_inserted 215191885 1521071427
A depth map estimation component that estimates semi-dense depth images
 for keyframes
\end_layout

\begin_layout Enumerate

\change_inserted 215191885 1521071427
A map optimization component that seeks to create a 3D map of the environment
 that is self-consistent.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521994424
This work utilizes the first two components to explore fusion of RGBD depth
 images with SfM depth images.
 For this work, the map optimization component (3) is not used.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:The-complete-process"

\end_inset

 depicts an overview of the proposed depth fusion algorithm.
\end_layout

\begin_layout Section

\change_inserted 215191885 1521071427
Time and Spatial Sampling Issues
\begin_inset CommandInset label
LatexCommand label
name "subsec:Time-and-Spatial"

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521071427
As mentioned previously, RGBD sensors measure depth at a rate of 30 frames
 per second (fps) and LSD-SLAM computes depth images only for 
\emph on
keyframes 
\emph default
which is a sparse subset of the measured RGB frames.
 Further, keyframes are not generated uniformly in time but created when
 the SfM algorithm detects criteria required to create a new keyframe.
 This condition is triggered when the current camera pose is too far from
 the most recent keyframe camera pose and when the current frame tracking
 result is 
\begin_inset Quotes eld
\end_inset

good
\begin_inset Quotes erd
\end_inset

 in the sense that the image warping correspondence objective function suggests
 an accurate or low-error result.
 As a result, SfM-estimated depths exist only for those RGB images designated
 as keyframes.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521071427
Further, the spatial distribution of SfM-estimated depths within SfM keyframes
 are localized to only those pixels having 
\begin_inset Quotes eld
\end_inset

good
\begin_inset Quotes erd
\end_inset

 3D reconstruction characteristics.
 In this sense, the quality of the depth estimate depends on accurately
 matching pixels along epipolar lines inscribed in the image.
 The matching performance here is best when there is a significant change
 in the image intensities along the epipolar line.
 Hence, 3D depth reconstruction is limited to those pixels that lie at sharp
 intensity changes, i.e., 
\begin_inset Quotes eld
\end_inset

edge
\begin_inset Quotes erd
\end_inset

 pixels, and further limited to those 
\begin_inset Quotes eld
\end_inset

edge
\begin_inset Quotes erd
\end_inset

 pixels that lie on edges that are roughly perpendicular to the direction
 of the epipolar line (see 
\begin_inset CommandInset citation
LatexCommand cite
key "6751290"

\end_inset

 for details).
 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521071427
The LSD-SLAM algorithm estimates depth at 
\begin_inset Quotes eld
\end_inset

good
\begin_inset Quotes erd
\end_inset

 pixel positions as a 1-dimensional Gaussian distribution specified as a
 mean image 
\begin_inset Formula $\mu_{SfM}(x,y)$
\end_inset

, i.e., the estimated depth image, and a variance image 
\begin_inset Formula $\sigma_{SfM}^{2}(x,y)$
\end_inset

 such that the RGB keyframe pixel at location 
\begin_inset Formula $\mathbf{I}(x,y)$
\end_inset

 is estimated to have depth 
\begin_inset Formula $\mu_{SfM}(x,y)$
\end_inset

 with uncertainties given by 
\begin_inset Formula $\sigma_{SfM}^{2}(x,y)$
\end_inset

.
 In this sense, the keyframe image 
\begin_inset Formula $\mathbf{I}(x,y)$
\end_inset

 augmented with the estimated depth image 
\begin_inset Formula $\mu_{SfM}(x,y)$
\end_inset

 is analogous in format to sensed RGBD image data.
 Yet, the uncertainties for the image 
\begin_inset Formula $\mu_{SfM}(x,y)$
\end_inset

 are given by the image 
\begin_inset Formula $\sigma_{SfM}^{2}(x,y)$
\end_inset

 rather than the experimentally validated uncertainties discussed in 
\begin_inset ERT
status open

\begin_layout Plain Layout

\change_inserted 215191885 1521071427


\backslash
S
\end_layout

\end_inset


\begin_inset space ~
\end_inset

 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:RGBD_Measurement-Noise"

\end_inset

.
\end_layout

\begin_layout Section

\change_inserted 215191885 1521071427
Image Registration Issues
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521071427
Fusing depth measurements requires knowledge of the correspondence between
 the depth measurements generated from the RGBD sensor and the SfM algorithm.
 For SfM keyframes this correspondence is trivial due to the fact that RGBD
 sensors support hardware registration.
 Hardware registration co-locates the RGBD depth image measurements, 
\begin_inset Formula $\mathbf{d}_{rgbd}(x,y)$
\end_inset

, and RGB appearance values, 
\begin_inset Formula $\mathbf{I}(x,y)$
\end_inset

.
 Hence, for hardware-registered RGBD depth images, 
\begin_inset Formula $\mathbf{d}_{rgbd}(x,y)$
\end_inset

 is the measured depth of the surface having RGB pixel 
\begin_inset Formula $\mathbf{I}(x,y)$
\end_inset

.
 Similarly, SfM-estimated depths for an RGB keyframe, 
\begin_inset Formula $\mu d_{SfM}(x,y)$
\end_inset

, are the depths for the surface having RGB pixel 
\begin_inset Formula $\mathbf{I}(x,y)$
\end_inset

.
 Hence fusion is accomplished by fusing the measurements at corresponding
 
\begin_inset Formula $(x,y)$
\end_inset

 locations in the RGBD depth image, 
\begin_inset Formula $\mathbf{d}_{rgbd}(x,y)$
\end_inset

, and the SfM depth image, 
\begin_inset Formula $\mu\mathbf{d}_{SfM}(x,y)$
\end_inset

.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521479929
Depth correspondences for RGB images that are not SfM keyframes must be
 computed from one or more SfM keyframe depth images.
 This article uses the most recent, i.e., closest-in-time, keyframe to generate
 co-registered SfM depth images for arbitrary RGB images.
 To do so, the depth image from the most recent keyframe, 
\begin_inset Formula $\mathbf{d}_{SfM}(x,y)$
\end_inset

, is 
\begin_inset Quotes eld
\end_inset

back-projected
\begin_inset Quotes erd
\end_inset

 to create a 3D point cloud of SfM measurements.
 Using the estimated keyframe-to-camera pose change, the 3D measurements
 are then re-projected into the RGB camera image plane using the 3D projection
 equations for the camera provided via camera calibration.
 The resulting depth image, 
\begin_inset Formula $\widetilde{\mathbf{d}}_{SfM}(x,y)$
\end_inset

 is then co-registered with the RGBD depth image 
\begin_inset Formula $\mathbf{d}_{rgbd}(x,y)$
\end_inset

 and RGB image 
\begin_inset Formula $\mathbf{I}(x,y)$
\end_inset

.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521917849
However, due to the noise in depth computation and consequent trajectory
 estimation, the depth registration is not always correct, this should be
 addressed as early as possible to avoid the drift in their computations.
 We try to achieve better depth registration by estimating the transformation
 error between the point clouds of sensor depth measurements and LSD depth
 estimation.
 Since the LSD estimations are not to the scale, we also need to estimate
 the scale for better registration of depths.
 The Umeyama's Least-Square estimation 
\begin_inset CommandInset citation
LatexCommand cite
key "Umeyama:1991:LET:105514.105525"

\end_inset

 shows us how to estimate these 
\begin_inset Formula $Sim(3)$
\end_inset

 transformation parameters efficiently.
 In his work, Umeyama shows how to estimates the rotation, translation and
 scale between two point clouds with known correspondence.
 In order to avoid errors, we use only the valid depths in both the point
 cloud for estimation.By using 
\begin_inset Formula $\mathbf{pc}_{rgbd}$
\end_inset

and 
\begin_inset Formula $\mathbf{pc}_{SfM}$
\end_inset

to represent the valid point clouds of sensor measurements and LSD estimation
 respectively, we compute the mean, variances and covariance of both them
 as following.
\change_unchanged

\end_layout

\begin_layout Standard

\change_inserted 215191885 1521484409
\begin_inset Formula 
\[
\mu_{rgbd}=\frac{\sum\mathbf{pc}_{rgbd}}{n}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521484766
\begin_inset Formula 
\[
\mu_{SfM}=\frac{\sum\boldsymbol{pc}_{SfM}}{n}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521484766
\begin_inset Formula 
\[
\sigma_{rgbd}^{2}=\frac{\sum\left\Vert \boldsymbol{pc}_{rgbd}-\mu_{rgbd}\right\Vert ^{2}}{n}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521484795
\begin_inset Formula 
\[
\sigma_{SfM}^{2}=\frac{\sum\left\Vert \boldsymbol{pc}_{SfM}-\mu_{SfM}\right\Vert ^{2}}{n}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521501433
\begin_inset Formula 
\[
\Sigma_{SfM,rgbd}=\frac{\sum(\boldsymbol{pc}_{rgbd}-\mu_{rgbd})(\boldsymbol{pc}_{SfM}-\mu_{SfM})^{T}}{n}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521501595
With singular value decomposition of 
\begin_inset Formula $\Sigma_{SfM,rgbd}$
\end_inset

 as 
\begin_inset Formula $UDV^{T},$
\end_inset

the optimum rotation matrix 
\begin_inset Formula $R$
\end_inset

 which achieves the minimum error between point clouds is given by
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521501637
\begin_inset Formula 
\[
R=USV^{T}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521507419
\begin_inset Formula 
\[
S=I\begin{cases}
I & det(\Sigma_{SfM,rgbd})\geq0\\
diag(1,1,-1) & det(\Sigma_{SfM,rgbd})<0
\end{cases}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521507451
The scale adjustment 
\begin_inset Formula $c$
\end_inset

 is computed by
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521507699
\begin_inset Formula 
\[
c=\frac{1}{\sigma_{SfM}^{2}}trace(DS)
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521507819
With scale and rotation established, the translation parameters are determined
 as 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521508665
\begin_inset Formula 
\[
t=\mu_{rgbd}-cR\mu_{SfM}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521917864
with the 
\begin_inset Formula $Sim(3)$
\end_inset

 parameters we transform LSD point clouds before projecting them to find
 
\begin_inset Formula $\widetilde{\mathbf{d}}_{SfM}(x,y)$
\end_inset

 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521514011
\begin_inset Formula 
\[
\tilde{\boldsymbol{pc_{SfM}}}=cR\boldsymbol{pc}_{SfM}+t
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521818294
Using these techniques co-registered SfM depth images can be computed for
 a general RGBD image.
 When RGBD images correspond to SfM keyframes the registration is 
\begin_inset Quotes eld
\end_inset

automatic,
\begin_inset Quotes erd
\end_inset

 i.e., no computation is necessary.
 In all other cases, a co-registered SfM depth image must be computed by
 reconstructing a 3D point cloud from a keyframe and then projecting the
 point cloud into the target RGB camera image using the estimated camera
 calibration and keyframe-to-camera-frame relative pose parameters.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521818294
\begin_inset Float figure
wide true
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521818294
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521818294
\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "1in"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521818294
\begin_inset Graphics
	filename images/lsd_rgbd_pc_before_reg.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521818294
\begin_inset Graphics
	filename images/lsd_rgbd_pc_before_reg_2.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\change_inserted 215191885 1521818294
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1521818294

\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521818294
\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "1in"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521818294
\begin_inset Graphics
	filename images/lsd_rgbd_pc_after_reg.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521818294
\begin_inset Graphics
	filename images/lsd_rgbd_pc_after_reg_2.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\change_inserted 215191885 1521818294
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1521818294

\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521818294
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1521818564
\begin_inset CommandInset label
LatexCommand label
name "fig:depth registartion"

\end_inset

 (a)Shows the two different views of the point clouds before depth registration
 (b)Shows the different views of the point clouds after depth registration.
 The green represents the point cloud from LSD SLAM and purple represents
 the point clouds from RGBD sensors
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section

\change_inserted 215191885 1521853647
Noise removal
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521917875
Noise in non linear optimization leads to erroneous depth computation, these
 depths values should be detected and removed in-order to avoid the to drift
 in pose estimation and consequent accretion of the error in depth estimations.
 Fortunately, these noisy depths have a particular pattern as shown in the
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:vailing error"

\end_inset

 (a), i.e.
 if we consider the optic center as the origin, the noisy depth form a veiling
 pattern along the line from origin to furthest depth value.
 Also we know that the furthest depth is the valid depth , we can find this
 valid depth and eliminate all other noisy depth by tracing the line to
 origin.
 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521854479
\begin_inset Float figure
wide true
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521854805
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521854479
\begin_inset Graphics
	filename images/depth_filter_5.jpg
	lyxscale 10
	height 1.3in

\end_inset


\end_layout

\begin_layout Plain Layout

\change_inserted 215191885 1521854479
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1521854479

\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521854479
\begin_inset Graphics
	filename images/depth_filter_7.jpg
	lyxscale 10
	height 1.3in

\end_inset


\end_layout

\begin_layout Plain Layout

\change_inserted 215191885 1521854479
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1521854479

\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521854805
\begin_inset Graphics
	filename images/depth_filter_6.jpg
	lyxscale 10
	height 1.3in

\end_inset


\end_layout

\begin_layout Plain Layout

\change_inserted 215191885 1521854805
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1521854805

\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1521917885
\begin_inset CommandInset label
LatexCommand label
name "fig:vailing error"

\end_inset

 (a) LSD depths with veiling errors (b) Veiling errors detected (represented
 in black) (c) LSD depths with veiling errors removed 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section

\change_inserted 215191885 1521071427
Resolving the Unknown SfM Scale
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521071427
The methods described in previous sections detail how co-registered SfM
 depth measurements are computed for every sensed RGBD frame.
 However, as discussed in previously, SfM depth images intrinsically have
 an unknown scale, 
\begin_inset Formula $\alpha$
\end_inset

, which reflects the fact that the solution for the scene structure is not
 geometrically unique, i.e., the same scene structure can be observed at a
 infinite number of distinct scales.
 Therefore fusion requires the scale of the SfM depth image to fit the scale
 of the real-world scene measured by the RGBD camera.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521071427
Given that the depth measurements for the RGBD depth image are co-registered
 with the SfM estimated depth image the scale parameter can be directly
 estimated by minimizing the sum of the squared depth errors 
\begin_inset CommandInset citation
LatexCommand cite
key "Bishop:2006:PRM:1162264"

\end_inset

 between the SfM depth image and the RGBD depth image.
 Let 
\begin_inset Formula $V$
\end_inset

 denote the set of 
\begin_inset Formula $(x,y)$
\end_inset

 positions that have valid depth measurements for 
\begin_inset Quotes eld
\end_inset

standard
\begin_inset Quotes erd
\end_inset

 fusion as described in 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Fusing-RGBD-Depths"

\end_inset

.
 Equation (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:scale error"

\end_inset

) shows the error function used to compute the unknown scale value and equation
 (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:depth_scale_solution"

\end_inset

) shows the solution 
\begin_inset Formula $\widehat{\alpha}$
\end_inset

 that minimizes this error.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521071427
\begin_inset Formula 
\begin{equation}
e(\alpha)=\sum_{(x,y)\epsilon V}\left\Vert \mathbf{d}_{rgbd}(x,y)-\alpha\mathbf{d}_{SfM}(x,y)\right\Vert ^{2}\label{eq:scale error}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521071427
\begin_inset Formula 
\begin{equation}
\widehat{\alpha}=\sum_{(x,y)\epsilon V}\frac{\mathbf{d}{}_{rgbd}(x,y)}{\mathbf{d}_{SfM}(x,y)}\label{eq:depth_scale_solution}
\end{equation}

\end_inset


\end_layout

\begin_layout Section

\change_inserted 215191885 1521071427
RGBD and SfM Depth Fusion
\begin_inset CommandInset label
LatexCommand label
name "subsec:Fusing-RGBD-Depths"

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521071427
For fusing measurements we consider the structured-light measurement of
 the RGBD sensor to generate a distribution for the unknown true depth of
 the scene surfaces at each RGBD 
\begin_inset Formula $(x,y)$
\end_inset

 pixel in the depth image.
 These measurements are considered to be independent and identically distributed
 to the measurements of the true unknown depth of the scene surfaces from
 the registered SfM estimated depths.
 With these assumptions, solving the depth fusion problem is equivalent
 to estimating the posterior distribution of the true scene depth at each
 
\begin_inset Formula $(x,y)$
\end_inset

 position given the distributions for the RGBD and SfM depth values.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521071427
Fortunately, previous sections show that Gaussian models are appropriate
 distributions for both the RGBD and SfM depth values and the parameters
 of these models are either known (see 
\begin_inset ERT
status open

\begin_layout Plain Layout

\change_inserted 215191885 1521071427


\backslash
S
\end_layout

\end_inset


\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:RGBD_Measurement-Noise"

\end_inset

) or estimated continuously (see 
\begin_inset ERT
status open

\begin_layout Plain Layout

\change_inserted 215191885 1521071427


\backslash
S
\end_layout

\end_inset


\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Time-and-Spatial"

\end_inset

).
 When both distributions are Gaussian, the posterior distribution can be
 found analytically and is a well-known result used in pattern recognition
 and other prediction frameworks, e.g., the Kalman filter as discussed in
 
\begin_inset CommandInset citation
LatexCommand cite
key "Maybeck79stochasticsmodels"

\end_inset

.
 Specifically, let the Gaussian noise for RGBD depth at position 
\begin_inset Formula $(x,y)$
\end_inset

 be represented as 
\begin_inset Formula $\mathcal{N}(\mathbf{d}_{rgbd},\sigma_{rgbd}^{2})$
\end_inset

 and the Gaussian noise for the co-registered SfM depth image at position
 
\begin_inset Formula $(x,y)$
\end_inset

 be 
\begin_inset Formula $\mathcal{N}(\mu_{SfM},\sigma_{SfM}^{2})$
\end_inset

.
 The posterior distribution on the unknown true depth at position 
\begin_inset Formula $(x,y)$
\end_inset

 is also Gaussian and let 
\begin_inset Formula $\mathcal{N}(\mu_{fused},\sigma_{fused}^{2})$
\end_inset

 denote the mean and variance parameters of this distribution.
 Equations (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:fused_mean"

\end_inset

) and (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:fused_variance"

\end_inset

) provide optimal estimates of the mean and variance of the fused depth
 at position 
\begin_inset Formula $(x,y)$
\end_inset

.
 The best estimate of the fused depth is given by the highest probability
 value in the posterior distribution which is the mean fused image, 
\begin_inset Formula $\mu_{fused}(x,y)$
\end_inset

.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521071427
\begin_inset Formula 
\begin{equation}
\mu_{fused}=\frac{\mathbf{d}_{rgbd}\sigma_{SfM}^{2}+\mu_{SfM}\sigma_{rgbd}^{2}}{\sigma_{SfM}^{2}+\sigma_{rgbd}^{2}}\label{eq:fused_mean}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521514889
\begin_inset Formula 
\begin{equation}
\sigma_{fused}^{2}=\frac{\sigma_{rgbd}^{2}\sigma_{SfM}^{2}}{\sigma_{rgbd}^{2}+\sigma_{SfM}^{2}}\label{eq:fused_variance}
\end{equation}

\end_inset


\end_layout

\begin_layout Chapter

\change_inserted 215191885 1521515010
Results
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521515011
\begin_inset Float figure
wide true
sideways false
status collapsed

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521515011
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521515011
\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "1in"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521515011
\begin_inset Graphics
	filename images/exp1/intensity_image.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521515011
\begin_inset Graphics
	filename images/exp2/intensity.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521516923
\begin_inset Graphics
	filename images/exp3/intenstiy.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521516923
\begin_inset Graphics
	filename images/in_to_out_with_transformation_error/intensity.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521516923
\begin_inset Graphics
	filename images/indoor_good_scale_and_transform/intensity.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\change_inserted 215191885 1521515011
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1521515011

\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521515011
\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "1in"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521515011
\begin_inset Graphics
	filename images/exp1/sensor_measurments.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521515011
\begin_inset Graphics
	filename images/exp2/sensor_measurments.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521516546
\begin_inset Graphics
	filename images/exp3/sensor_depth.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521516754
\begin_inset Graphics
	filename images/in_to_out_with_transformation_error/sensor_measurment.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521516754
\begin_inset Graphics
	filename images/indoor_good_scale_and_transform/sensor_depth.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\change_inserted 215191885 1521515011
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1521515011

\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521515011
\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "1in"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521515011
\begin_inset Graphics
	filename images/exp1/scaled_lsd_depth.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521515011
\begin_inset Graphics
	filename images/exp2/scaled_lsd.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521516530
\begin_inset Graphics
	filename images/exp3/scaled_Depth.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521516757
\begin_inset Graphics
	filename images/in_to_out_with_transformation_error/scaled_lsd_depth.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521516757
\begin_inset Graphics
	filename images/indoor_good_scale_and_transform/scaled_lsd.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\change_inserted 215191885 1521515011
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1521515011

\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521515011
\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "1in"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521515011
\begin_inset Graphics
	filename images/exp1/fused_depth_sets.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521515011
\begin_inset Graphics
	filename images/exp2/depth_Fusion_set.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521516534
\begin_inset Graphics
	filename images/exp3/sensor_depth_set.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521516764
\begin_inset Graphics
	filename images/in_to_out_with_transformation_error/fused_set.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521516764
\begin_inset Graphics
	filename images/indoor_good_scale_and_transform/fused_set.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\change_inserted 215191885 1521515011
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1521515011

\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521515011
\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "1in"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521515011
\begin_inset Graphics
	filename images/exp1/std_deviation_colormap.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521515011
\begin_inset Graphics
	filename images/exp2/std_deviation_colormap.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521516538
\begin_inset Graphics
	filename images/exp3/std_deviation_colmap.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521516771
\begin_inset Graphics
	filename images/in_to_out_with_transformation_error/std_dev_colMap.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521516771
\begin_inset Graphics
	filename images/indoor_good_scale_and_transform/std_deviation_colormap.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\change_inserted 215191885 1521515011
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1521515011

\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521515011
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1521515011
Results for three experiments are shown.
 Images shown are organized into separate columns.
 Column (a) shows a grayscale image of the scene (b) shows the sensed RGBD
 depth image (c) shows the SfM-estimated depth image (d) shows the fused
 image and (e) shows the standard deviation for fused depths (in 
\begin_inset Formula $m.$
\end_inset

).
 The fused image has been color-coded as follows: (white) denotes depth
 locations sensed only by the RGBD sensor, (yellow) denotes depth locations
 only sensed via SfM, (red) denotes fused (RGBD+SfM) depth locations and
 (black) denotes depth locations without RGBD or SfM measurements.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521514993
We conducted experiments with different setups to test and analyze the depth
 fusion algorithm.
 Experiments recorded RGBD image sequenced from ORBBEC Astra RGBD sensors
 and applied LSD-SLAM to their image streams using their factory-provided
 intrinsic camera calibration parameters.
 Each experiment included approximately 60 seconds of RGBD image data at
 the rate of 30 fps.
 Experimentally recorded RGB images were processed offline by the LSD-SLAM
 algorithm to generate SfM depth images.
 Experiments initialize the LSD-SLAM algorithm with the first recorded depth
 image from the RGBD sensor to facilitate the initial scale approximation.
 The output from the LSD-SLAM algorithm consisting of the relative pose
 for every tracked frame and the depth map for each keyframe was then captured
 to disk.
 The fusion algorithm was then run offline on the recorded RGBD image stream
 and LSD-SLAM output files to generate the results shown in this section.
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521514993
\begin_inset Tabular
<lyxtabular version="3" rows="8" columns="4">
<features tabularvalignment="middle">
<column alignment="center" valignment="top" width="0pt">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell multirow="3" alignment="center" valignment="middle" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521514993

\size footnotesize
Experiment
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521514993

\size footnotesize
RGBD-only depths
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521514993

\size footnotesize
SfM-only depths
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521514993

\size footnotesize
Fused depths
\end_layout

\end_inset
</cell>
</row>
<row>
<cell multirow="4" alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521514993

\size footnotesize
(%)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521514993

\size footnotesize
(%)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521514993

\size footnotesize
(%) 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521514993
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521514993
67.7
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521514993
6.7
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521514993
25.5
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521514993
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521514993
55.5
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521514993
10.0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521514993
34.3
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521514993
3
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521514993
68.1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521514993
15.1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521514993
16.6
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521522076
4
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521522089
51.0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521522116
25.7
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521522139
23.1
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521522077
5
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521522195
68.3
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521522214
8.8
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521522249
22.8
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521514993

\size footnotesize
Average
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521522298
62.12
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521522330
13.26
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521522369
24.46
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout

\change_inserted 215191885 1521514993
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1521514993
Ratios of depth measurements to total depths
\begin_inset CommandInset label
LatexCommand label
name "tab:Ratios-of-depth"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521514993
Experiment 1 depicts a indoor office scene at the university.
 This scene includes specular and dark surface structures at close range
 that are not measured by the RGBD sensor.
 Yet, the SfM algorithm estimates depths at a number of locations (on the
 podium) where ther are significant intensity changes.
 These additional depths are evident in the fused results, which includes
 SfM-only depth measurements in regions in the vicinity of image edges.
 The experiment demonstrates that depth fusion can improve depth images
 by obtaining depths from surfaces not measurable by the RGBD sensor.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521519752
Experiment 2 depicts a hallway in the UNCC EPIC building that includes both
 specular surfaces and high intensity illumination from overhead lights.
 The experiment is a second example showing that depth fusion bolsters over
 all depth measurement performance by providing the depths when RGBD sensors
 fail.
 The SfM takes advantage of the patterns on the surface and estimates the
 depths irrespective of the nature of its reflectance properties and color.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521518425
Experiment 3 depicts the UNCC faculty conference hall.
 Here a number of scene structures lie beyond the measurement range of the
 RGBD sensor.
 Yet, the SfM algorithm is able to estimate the depth of these scene structures
 (albeit at high variance) providing depths that would otherwise not be
 possible.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521521717
Experiment 4 depicts indoor to outdoor transition.
 By initializing the LSD SLAM indoor, we get the scale estimated.
 We can see that in outdoor, when the sensor measurements fails because
 of the infrared ray flooding, depth estimates from LSD SLAM are used.
 Often because of IR flooding the sensor measurements gets corrupted, in
 those scenarios we can consider replacing entire corrupted depth sensor
 measurements by LSD depth estimation instead of fusion for better results.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521521903
Experiment 5 depicts indoor with well lit conditions, even though the sensors
 are supposed work completely fine in this scenario, there are rare chances
 of the them failing at steep surfaces.
 During such failures, depth fusion comes handy.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521596751
Table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:Ratios-of-depth"

\end_inset

 quantifies the amount of additional depth information provided via RGBD-SfM
 depth image fusion.
 When we average over the five experiments discussed approximately 62% of
 the fused depths originate from the RGBD sensor alone, approximately 13%
 original from the SfM algorithm alone, and approximately 24% of the fused
 depths results from RGBD-SfM fusion.
 The addition of 13% of novel depth data is a significant contribution.
 Further, fused depths account for roughly 24% of depth image data and the
 measurement error for all of these measurements will be reduced by the
 fusion.
 Variance reduction will be greatest for low-variance SfM depth estimates
 which are typically in textured scene locations close to the camera.
 Yet, we note that by inspection of equation (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:fused_variance"

\end_inset

), it is theoretically impossible for the variance of any fusion result
 to increase.
 Another obvious observation from the Table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:Ratios-of-depth"

\end_inset

 is that the SfM has a largest contribution in Experiment 4 (outdoor scenario)
 since sensor fails outdoors and it has least contribution in Experiment
 5 (well lit indoor) where sensor is at its best.
\end_layout

\begin_layout Chapter

\change_inserted 215191885 1521596763
Conclusion and Futher work
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521596763
The SfM depth estimation complements the RGBD sensor measurements and can
 provide depths when RGBD sensors fail.
 The depth fusion algorithm provides the effective way to augment the RGBD
 depth stream and results in improved depth images.
 The experiments conducted shows these improvements in the experimental
 results for a number of scenarios where RGBD sensors fail including successfull
y capturing depth for out-of-range RGBD depth locations and successfully
 capturing depth measurements from specular and dark objects.
 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521605145
As future work, the proposed depth fusion can be generalized to address
 depth image locations the include measurements having non-Gaussian noise
 distributions, especially for the outdoor scenario the error noise are
 not necessarily Gaussian.
 Also, the depth image gets corrupted by random speckles because of infrared
 ray flooding, these speckles could be wrongly treated as depth value, since
 there values are in valid depth range.
 Fortunately, it is observed that these speckles have particular pattern
 and a prepossessing logic could be implemented to get rid of them.
 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521605101
Also, experimental results show that the scale and trajectory estimation
 of SfM depths are not always accurate.
 Back-propagation of RGBD-SfM fused depth imagery into the SfM algorithm
 can be exploited to improve the camera pose estimation in the camera pose
 graph.
 We know that the fused depths have measurements where SfM fails and also
 they have lower variance than SfM's, propagating them to SfM leads to improveme
nt in estimated depth values, which leads to better trajectory computation
 for future frames.
 
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "references_db"
options "ieeetr"

\end_inset


\end_layout

\end_body
\end_document
