#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass UNCC-thesis
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package none
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 0
\use_package mathtools 1
\use_package mhchem 0
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 2
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes true
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\author 215191885 "Akash"
\author 1699939148 "arwillis"
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pagenumbering{roman}
\end_layout

\begin_layout Plain Layout


\backslash
fbmatterchapterformat
\end_layout

\begin_layout Plain Layout

% Doctype should be either dissertation proposal, dissertation, or thesis.
\end_layout

\begin_layout Plain Layout

% If you're getting a master's, specify "thesis" below.
  
\end_layout

\begin_layout Plain Layout

% If you're getting a PhD, specify "dissertation" below.
\end_layout

\begin_layout Plain Layout


\backslash
doctype{thesis}
\end_layout

\begin_layout Plain Layout

%%%%%%%%%%%%%%%%     IMPORTANT! IMPORTANT! IMPORTANT! %%%%%%%%%%%%%%%%
\end_layout

\begin_layout Plain Layout

% The rules below MUST be followed for the abstract page and chapter titles
\end_layout

\begin_layout Plain Layout

% to be correctly formatted.
\end_layout

\begin_layout Plain Layout

%
\end_layout

\begin_layout Plain Layout

% 1.
 Only the first letter of the entire title should be capitalized to allow
 the 
\end_layout

\begin_layout Plain Layout

%    title to appear as required by the graduate school on the Abstract
 page.
\end_layout

\begin_layout Plain Layout

% 2.
 Write chapter titles in ALL CAPS.
\end_layout

\begin_layout Plain Layout

%
\end_layout

\begin_layout Plain Layout


\backslash
title{Structure From Motion}
\end_layout

\begin_layout Plain Layout


\backslash
author{Akash Chandra Shekar}
\end_layout

\begin_layout Plain Layout


\backslash
degree{Master of Science}
\end_layout

\begin_layout Plain Layout


\backslash
major{Computer Science}
\end_layout

\begin_layout Plain Layout


\backslash
publicationyear{2018}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
advisor{Dr.
 Andrew Willis}
\change_inserted 215191885 1521524005

\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

% Add the full name and title of all your committee members,
\end_layout

\begin_layout Plain Layout

% apart from your advisor, one by one.
  The style file expects
\end_layout

\begin_layout Plain Layout

% 3 to 5 committee members in addition to your advisor.
\end_layout

\begin_layout Plain Layout

\change_inserted 215191885 1521524112

\end_layout

\begin_layout Plain Layout

\change_inserted 215191885 1521524116


\backslash
committeeMember{Dr.
 Min Shin}
\change_unchanged

\end_layout

\begin_layout Plain Layout


\backslash
committeeMember{Dr.
 
\change_deleted 215191885 1521523590
Min Shin
\change_inserted 215191885 1521523595
Srinivas Akella
\change_unchanged
}
\end_layout

\begin_layout Plain Layout


\backslash
committeeMember{Dr.
 
\change_deleted 215191885 1521523646
Jianping Fan
\change_inserted 215191885 1521523647
Hamed Tabkhi
\change_unchanged
}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

% Generate the preliminary title page and copyright page.
\end_layout

\begin_layout Plain Layout


\backslash
maketitlepage
\end_layout

\begin_layout Plain Layout


\backslash
makecopyright
\end_layout

\end_inset


\end_layout

\begin_layout Abstract
The Structure from 
\change_deleted 1699939148 1515786609
m
\change_unchanged
Motion (SfM) systems use knowledge of a camera's image formation model and
 a time-sequence of images from the camera to estimate camera motion and
 3D scene structure.
\change_deleted 1699939148 1515786717
involves two aspects, one is to track the camera
\change_unchanged
 Formulations of this problem decompose the problem into two sub-problems:
 
\change_deleted 1699939148 1515786757
trajectory by 
\change_unchanged
 (1) solving for
\change_deleted 1699939148 1515786772
ing
\change_inserted 1699939148 1515786773
e
\change_unchanged
 
\change_deleted 1699939148 1515786851
nonlinear
\change_unchanged
the camera trajectory
\change_deleted 215191885 1516118794
 
\change_deleted 1699939148 1515786856
 equation
\change_unchanged
 and 
\change_deleted 1699939148 1515786861
other is to
\change_unchanged
solving for
\change_deleted 215191885 1516118794
 
\change_deleted 1699939148 1515786868
 estimate
\change_unchanged
 the depth of 3D scene points observed by the camera in multiple images.
 Solutions are provided by 
\change_deleted 1699939148 1515786913
by solving
\change_unchanged
 finding 
\change_deleted 1699939148 1515786917
Stereo 
\change_unchanged
correspondences between 
\change_deleted 1699939148 1515786923
of sub-set of 
\change_unchanged

\begin_inset Formula $(x,y)$
\end_inset

 locations
\change_deleted 1699939148 1515787163
pixels
\change_unchanged
 in images take from different poses taken at distinct times in the camera
 trajectory.
 
\change_deleted 1699939148 1515786976
The t
\change_unchanged
Traditional SfM
\change_deleted 1699939148 1515786982
Structure From Motion
\change_unchanged
 
\change_deleted 1699939148 1515786986
functions by 
\change_unchanged
find
\change_deleted 1699939148 1515786988
ing
\change_unchanged
 these correspondence
\change_inserted 1699939148 1515786991
s
\change_unchanged
 
\change_deleted 1699939148 1515786994
between pair of images,
\change_unchanged
 by extracting and tracking 
\change_deleted 1699939148 1515786999
the 
\change_unchanged
features
\change_deleted 1699939148 1515787197
 like
\change_unchanged
, e.g., corner points, 
\change_deleted 1699939148 1515787229
lines etc
\change_unchanged
match local regions within images.
 However, these 
\change_deleted 1699939148 1515787268
features
\change_unchanged
approaches 
\change_deleted 1699939148 1515787017
do not necessarily 
\change_unchanged
provide very sparse geometric descriptions of the scene structure and omit
 much of
\change_deleted 1699939148 1515787046
all
\change_unchanged
 the geometric information 
\change_deleted 1699939148 1515787052
regarding
\change_unchanged
in the image.
 
\change_deleted 1699939148 1515787057
The 
\change_unchanged
Other 
\change_deleted 1699939148 1515787062
more accurate method
\change_unchanged
, semi-dense, approaches 
\change_deleted 1699939148 1515787072
is to 
\change_unchanged
attempt to find large collections of correspondences by solving for pixel-level
\change_deleted 1699939148 1515787101
use the pixel intensity itself
\change_unchanged
 correspondences which provide more geometric information about the scene.
 Unfortunately, unlike features, pixel values may are susceptible to viewpoint,
 illumination, and other image formation phenomena which requires semi-dense
 approaches to
\change_deleted 1699939148 1515787356
to minimize
\change_unchanged
 consider 
\change_deleted 1699939148 1515787363
the P
\change_unchanged
photometric 
\change_deleted 1699939148 1515787370
and
\change_unchanged
correction 
\change_deleted 1699939148 1515787380
Geometric errors, by defining the robust error functions.
\change_unchanged
in order to robustly determine correspondences.
 
\end_layout

\begin_layout Acknowledgements
If you decide to have a acknowledgements page, your acknowledgement text
 would go here.
\end_layout

\begin_layout Acknowledgements
The Acknowledgement page should be brief, simple, and free of sentimentality
 or trivia.
 It is customary to recognize the role of the advisor, the other members
 of the advisory committee, and only those organizations or individuals
 who actually aided in the project.
 Further, you should acknowledge any outside source of financial assistance,
 such as GASP grants, contracts, or fellowships.
\end_layout

\begin_layout Dedication
If you decide to have a dedication page, your dedication text would go here.
\end_layout

\begin_layout Dedication
The Dedication page, if used, pays a special tribute to a person(s) who
 has given extraordinary encouragement or support to one's academic career.
\end_layout

\begin_layout Introduction
If you decide to have an introduction page, your introduction text would
 go here.
 
\end_layout

\begin_layout Introduction
Depending on the discipline or the requirements of the student's advisory
 committee, an Introduction may be included as a preliminary page.
\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Standard
\begin_inset FloatList figure

\end_inset


\end_layout

\begin_layout Standard
\begin_inset FloatList table

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newpage
\end_layout

\begin_layout Plain Layout


\backslash
renewcommand{
\backslash
nomname}{LIST OF ABBREVIATIONS}
\end_layout

\begin_layout Plain Layout

% uncomment line below to title your nomenclature list as LIST OF SYMBOLS
\end_layout

\begin_layout Plain Layout

%
\backslash
renewcommand{
\backslash
nomname}{LIST OF SYMBOLS}
\end_layout

\begin_layout Plain Layout

%
\end_layout

\begin_layout Plain Layout

% NOTE: IF YOU USE A LIST OF ABBREVIATIONS / LIST OF SYMBOLS and are using
 command-line LaTeX (not LyX) 
\end_layout

\begin_layout Plain Layout

% YOU MUST COMPILE THE NOMENCLATURE INDEX
\end_layout

\begin_layout Plain Layout

% example:
\end_layout

\begin_layout Plain Layout

% bash$> pdflatex msthesis.tex
\end_layout

\begin_layout Plain Layout

% bash$> makeindex msthesis.nlo -s nomencl.ist -o msthesis.nls
\end_layout

\begin_layout Plain Layout

% bash$> pdflatex msthesis.tex
\end_layout

\begin_layout Plain Layout

%
\end_layout

\begin_layout Plain Layout


\backslash
addcontentsline{toc}{chapter}{
\backslash
nomname}
\end_layout

\end_inset


\begin_inset CommandInset nomencl_print
LatexCommand printnomenclature
set_width "auto"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "ECE"
description "An acronym for Electrical and Computer Engineering."

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newpage
\end_layout

\begin_layout Plain Layout


\backslash
setcounter{page}{1}
\end_layout

\begin_layout Plain Layout


\backslash
pagenumbering{arabic}
\end_layout

\begin_layout Plain Layout

% 2 inch top spacing for new chapters
\end_layout

\begin_layout Plain Layout


\backslash
bodychapterformat
\end_layout

\end_inset


\end_layout

\begin_layout Chapter
INTRODUCTION
\change_inserted 215191885 1517257438

\end_layout

\begin_layout Standard

\change_inserted 215191885 1517282111
The RGB-D camera captures depth information along with the RGB color information
 for every pixel.
 These cameras have the projector to projects and sensors to capture the
 infrared light.
 The depth informations can be obtained mainly by two different approaches,
 one is by projecting the known pattern of infrared light speckles and analyzing
 the deformation of light reflected back by the object, this method is called
 the structured light method and in another approach the depths are calculated
 by measuring the time taken by the infrared ray to reflect back to the
 sensors, this is called the Time-of-Flight technology.
 In both the case the infrared rays are play a crucial role, however the
 limitations of RGB-D camera it that it fails to work outdoor because of
 whitewash of infrared rays from sun, even indoor the RGB-D fails around
 the transparent and specular objects with high reflective index.
 We propose a method to take advantage of the Structure from Motion(SfM)
 to estimate depth of scene whenever the RGB-D fails to provide the required
 depth.
 The partial depths provided by the RGB-D camera can be used as a ground
 truth to jump-start the SfM, also these ground truth depths helps estimate
 the scale of the environment which is otherwise not possible with the SFM
 alone.
 The synergy of RGB-D and SfM provide better depth estimation.
\end_layout

\begin_layout Standard
Structure from Motion (SfM) is 
\change_deleted 1699939148 1515787416
a
\change_unchanged
the photogrammetric process of estimating the three-dimensional structure
 of a scene from a set of two-dimensional images, this is achieved by tracking
 the motion of the camera
\change_deleted 1699939148 1515787425
s
\change_unchanged
 
\change_deleted 1699939148 1515787428
corresponding
\change_unchanged
generating 
\change_deleted 1699939148 1515787431
to 
\change_unchanged
these images and correlating information within the time-sequence of images.
 SfM has many applications in the field of robotics including augmented
 reality
\change_deleted 1699939148 1515787468
 and
\change_unchanged
, geoscience 
\change_deleted 1699939148 1515787471
etc
\change_inserted 1699939148 1515787478
and 
\series bold
\emph on
[put more here]
\change_unchanged

\series default
\emph default
.
 In 
\change_deleted 1699939148 1515787485
R
\change_unchanged
robotics, SfM is 
\change_deleted 1699939148 1515787489
mainly 
\change_unchanged
applied 
\change_deleted 1699939148 1515787498
to
\change_unchanged
as one approach for
\change_deleted 1699939148 1515787504
 implement the
\change_unchanged
 visual odometry, the process of estimating 
\change_deleted 1699939148 1515787514
where 
\change_unchanged
the egomotion of an robot 
\change_deleted 1699939148 1515787571
is estimated 
\change_unchanged
using only the inputs of cameras attached to it
\change_inserted 215191885 1516122714
, this is used highly in self driving car and unmanned aerial vehicle
\change_unchanged
.
 In the field of augmented reality, SfM used to estimate the 
\change_inserted 215191885 1516122744
lost 
\change_unchanged
depth
\change_inserted 215191885 1517275359
s of the objects from images
\change_deleted 215191885 1516122760
 maps of the scene
\change_unchanged
, 
\change_inserted 215191885 1516122917
this depth information enables to augment the virtual objects into the real
 world and even allows to interact with augmented objects.
 
\change_deleted 215191885 1516122822
which are later used to implement basic physical interaction with the environmen
t
\change_inserted 1699939148 1515787624
 [depth map not defined, basic physical interaction vague]
\change_unchanged
.
 
\change_inserted 1699939148 1515787662

\end_layout

\begin_layout Standard

\change_inserted 215191885 1516604377
The point in 3D can be estimated by triangulation of 2D projections from
 two or more images, i.e with the known camera orientation, we can back-project
 2D points from images onto 3D environment and try to determine their intersecti
on.
 In order to achieve that we need find a set of points in one image which
 can be identified as the same points in another image, this is called as
 image correspondence.
 
\change_inserted 1699939148 1515787680

\begin_inset Note Comment
status open

\begin_layout Plain Layout

\change_inserted 1699939148 1515787697
Maybe explain 3D reconstruction via triangulation
\change_unchanged

\end_layout

\end_inset


\change_deleted 215191885 1516127400
The core of the SfM involves solving the trigonometry [??? unclear what
 trig?] on set of images from camera with unknown calibration to extract
 the depth of an object [3d reconstruction explanation perhaps].
 The main requirement to do this is to find a correspondence between pair
 of images,
\change_unchanged
 
\change_inserted 215191885 1516127426
T
\change_deleted 215191885 1516127425
t
\change_unchanged
raditionally this was done by feature extraction and matching
\begin_inset CommandInset citation
LatexCommand cite
key "klein07parallel"

\end_inset

.
 The feature extraction is the process of selecting the key points in the
 image which are unique and distinguishable and represent them in a efficient
 descriptor,which are later used for matching in the other image.
 There are many possible choices for features and descriptors, like SIFT,
 SURF, ORB etc.
 However in more recent implementations
\begin_inset CommandInset citation
LatexCommand cite
key "Engel2014"

\end_inset

, instead of features, the image intensities are used directly for matching.
 Once the correspondence are established,
\change_inserted 215191885 1516131443
 the intersection between back-projected 3D points is estimated by minimize
 the re-projection error, which is done by imposing the epipolar constraint,
 which states that the image points and 3D points should be in a same plane.
 In order to address the noise in estimating the correspondence, a Gaussian
 model is assumed with zero mean, hence minimizing the re-projection error
 gives the maximum likelihood estimate of 3D point.
 
\change_deleted 215191885 1516128263
 the Pose of the cameras are estimated by imposing the Epipolar constraint,
 now with the estimated camera pose the three-dimensional structure (depth)
 is computed.
 
\change_inserted 215191885 1516165356
On the other hand, with the known image correspondence and their 3D points,
 we can estimate the camera pose and orientation with the similar procedure.
 Where we try to solve for relative camera ordination which minimizes the
 re-projection error between known 3D and 2D point.
 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1516174146
However, SfM is a chicken and egg problem with both depth and camera orientation
 unknown.This again tries to minimize the re-projection error between the
 observed 2D image points and predicted (re-projected) 2D image point,which
 is expressed as the sum of squares of a large number of nonlinear, real-valued
 functions.
 Thus, the minimization is achieved using nonlinear least-squares algorithms
 like Guass-Newton or Levenberg–Marquardt.
 This process is also called as Bundle adjustment, as it is analogous to
 the process of optimally adjusting bundles of light rays originating from
 each 3D feature and converging them on each camera's optical center.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1516173258
Over the years many real time approaches have be suggested to tackle this
 problem, and these approaches are broadly classified as Direct vs.
 Indirect, Dense vs.
 Sparse and Incremental vs.
 Global methods.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1516602673
Direct vs.
 Indirect: All the SfM approaches have a underlaying probabilistic model
 in order to address the noise in measurements and camera representation,
 which uses the Maximum Likelihood approach to maximize the probability
 of obtaining the actual measurements, this differentiation of SfM is based
 on how the probabilistic models are defined.
 The direct method as the name specifies, directly uses the sensor values
 (pixel values) – light received from a certain direction over a certain
 time period – as as input for probabilistic model, hence this method tries
 to minimize the photometric error.
 This is analogous to the process of image alignment, whose goal is to minimize
 the sum of squared error between two images, the template T and the image
 I warped back onto the coordinate frame of the template.
 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1516602980
\begin_inset Formula 
\begin{equation}
\underset{x}{\sum}\left[I(\omega(x,p)-T(x)\right]^{2}\label{wraping function}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1516603171
\begin_inset CommandInset ref
LatexCommand ref
reference "wraping function"

\end_inset

 is minimized with respect p, which in case of SFM are the camera orientation,
 this minimization is a process of non linear optimization.
 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1516640089
On the other hand, the indirect method uses the intermediate representation
 of sensor measurements as input for probabilistic model, the sensor values
 can be either represented as sparse set feature key-points, this need a
 efficient feature detectors like Scale Invariant Feature Transform (SIFT)
 to find the valid the correspondence between images, and the methods like
 Random Sample Consensus algorithm (RANSAC) are applied in order to avoid
 the outliers.
 An alternative to using key features, is to find the dense regularized
 optical flow, which tries to match the selected points from one image to
 another, assuming that both images are part of a sequence and relatively
 close to one another.
 Both this representation tries to minimize the geometric errors.
 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1516640094
Dense vs.
 Sparse: As the name suggests the sparse methods uses only selected set
 of independent points, where as dense methods attempt to use and reconstruct
 all pixels in the 2D image domain.
 On the same line there is a intermediate approaches (semi-dense) tries
 to use the largely connected and well-constrained subset of points if not
 all of them.
 The choice of this method affects the addition of geometry prior, in the
 sparse formulation, since there is no notion of neighborhood, and geometry
 parameters (key-point positions) are conditionally independent.
 Dense (or semi-dense) approaches on the other hand exploit the connectedness
 of the used image region to formulate a geometry prior, typically favoring
 smoothness.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1516181853
Incremental vs.
 Global:
\change_deleted 215191885 1516181838
There mainly two variants of SfM, 
\change_deleted 1699939148 1515787706
I
\change_deleted 215191885 1516181838
incremental and 
\change_deleted 1699939148 1515787709
G
\change_deleted 215191885 1516181838
global
\change_deleted 1699939148 1515787713
 SfM
\change_deleted 215191885 1516181838
.
 
\change_unchanged
Incremental SfM
\begin_inset CommandInset citation
LatexCommand cite
key "Snavely:2006:PTE:1141911.1141964"

\end_inset

 begins by first estimating the 3D structure and camera poses of just two
 cameras based on their relative pose.
 Then additional cameras are added on incrementally and 3D structure is
 refined as new parts of the scene are observed.

\change_inserted 1699939148 1515787726
 
\change_unchanged
On the other hand 
\change_deleted 1699939148 1515787728
G
\change_unchanged
global SfM
\begin_inset CommandInset citation
LatexCommand cite
key "Wilson2014"

\end_inset

 considers the entire problem at once
\change_deleted 1699939148 1515787735
,
\change_unchanged
.
 Here the objective is 
\change_deleted 1699939148 1515787745
it tries 
\change_unchanged
to estimate the global camera poses and 3D structure by removing outliers
 and by applying an averaging scheme
\change_inserted 1699939148 1515787760
 [cite ???]
\change_unchanged
.

\change_inserted 1699939148 1515787787
 [why is this discussed? are their results different? is one of interest
 to you?]
\change_unchanged
 
\change_inserted 215191885 1516640098
[cited eariler][isn't the LSD a incremental method]
\end_layout

\begin_layout Standard

\change_deleted 215191885 1516599029
Over the years many approaches have be suggested to tackle this problem,
 one of them is the Parallel Tracking and Mapping (PTAM)
\begin_inset CommandInset citation
LatexCommand cite
key "klein07parallel"

\end_inset

, it is a feature based system, here the tracking and mapping are split
 into two separate tasks, processed in parallel threads on dual core computer
 [global or local?].
 The map is represented by a collection of point features located in a world
 coordinate frame 
\change_deleted 1699939148 1515787826
W
\change_deleted 215191885 1516599029

\begin_inset Formula $W$
\end_inset

.
 These points feature represents a locally planar textured patch in the
 world, each point has coordinates in world frame, an unit patch normal
 and a reference to the patch source pixels.
 The map also contains 
\change_deleted 1699939148 1515787809
N
\change_deleted 215191885 1516599029

\begin_inset Formula $N$
\end_inset

 [use math font for variables] key frames, each key frame has an associated
 camera-center coordinate frame and the transformation between the frames
 and also stores a four level pyramid of gray-scale 8bpp (bits-per-pixel)
 images.
 The tracking is a two-stage process done from coarse-to-fine,when the new
 image is acquired, an initial coarse search searches only for 50 map points
 which appear at the highest levels of the current frame’s image pyramid,
 and this search is performed (with sub-pixel refinement) over a large search
 radius.
 A new pose is then calculated from these measurements.
 After this, up to 1000 of the remaining potentially visible image patches
 are re-projected into the image, and now the patch search is performed
 over a far tighter search region.
 Sub-pixel refinement is performed only on a high-level subset of patches.
 The final frame pose is calculated from both coarse and fine sets of image
 measurements together.The pose update is computed iteratively by minimizing
 a robust objective function of the re-projection error:
\end_layout

\begin_layout Standard

\change_deleted 215191885 1516599029
\begin_inset Formula 
\[
\mu^{\prime}=\underset{\mu}{argmin}\underset{j\in s}{\sum}Obj\left(\frac{\left\Vert e_{j}\right\Vert }{\sigma_{j}},\sigma_{T}\right)
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516599029
Where 
\begin_inset Formula $e_{j}$
\end_inset

is the re-projection error vector:
\end_layout

\begin_layout Standard

\change_deleted 215191885 1516599029
\begin_inset Formula 
\[
e_{j}=\left(\begin{array}{c}
\hat{u_{j}}\\
\hat{v_{j}}
\end{array}\right)-CamProj(exp(\mu)E_{CWp_{j}})
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516599029
\begin_inset Formula $Obj(.,\sigma_{T})$
\end_inset

 is the Tukey bi-weight objective function and 
\begin_inset Formula $\sigma_{T}$
\end_inset

 a robust (median-based) estimate of the distribution’s standard deviation
 derived from all the residuals.
 [why are you talking about PTAM? is it related to your thesis? what is
 your research here?]
\end_layout

\begin_layout Standard

\change_deleted 215191885 1516599029
Associated with the the key-frame in the map is a set 
\begin_inset Formula $S_{i}$
\end_inset

 of image measurements..
 For example, the 
\begin_inset Formula $jth$
\end_inset

 map point measured in key-frame 
\begin_inset Formula $i$
\end_inset

 would have been found at 
\begin_inset Formula $(\hat{u}_{ji},\hat{v}_{ji})^{T}$
\end_inset

 with standard deviation of 
\begin_inset Formula $\sigma_{ji}$
\end_inset

 pixels.
 Writing the current state of the map as 
\begin_inset Formula $\left\{ E_{k_{1}W}....E_{k_{N}w}\right\} $
\end_inset

 and 
\begin_inset Formula $\left\{ p_{1}...p_{M}\right\} $
\end_inset

, each image measurement also has an associated re-projection error 
\begin_inset Formula $e_{ji}$
\end_inset

.
 Bundle adjustment is applied to iteratively adjust the map so as to minimize
 the robust objective function:
\end_layout

\begin_layout Standard

\change_deleted 215191885 1516599029
\begin_inset Formula 
\[
\left\{ \left\{ \mu_{2..}\mu_{N}\right\} ,\left\{ p_{1}^{\prime}..p_{M}^{\prime}\right\} \right\} =\underset{\{\{\mu\},\{p\}\}}{argmin}\stackrel[i=1]{N}{\sum}\underset{j\in s}{\sum}Obj\left(\frac{\left\Vert e_{ji}\right\Vert }{\sigma_{ji}},\sigma_{T}\right)
\]

\end_inset


\change_unchanged

\end_layout

\begin_layout Standard
LSD SLAM 
\begin_inset CommandInset citation
LatexCommand cite
key "Engel2014"

\end_inset


\change_deleted 215191885 1516608635
 on the other hand
\change_unchanged
 is 
\change_deleted 215191885 1516599097
the
\change_unchanged
 
\change_inserted 215191885 1516599196
a semi-dense, 
\change_unchanged
direct method
\change_inserted 1699939148 1515788014
 [you have not defined sparse methods, dense methods, direct methods, or
 indirect methods do this in a review of approaches above – similar to that
 from Direct Sparse Odometry article (https://arxiv.org/pdf/1607.02565.pdf)
 from TUM]
\change_inserted 215191885 1516608663
 [have done changes]
\change_deleted 215191885 1516599158
, this circumvent the drawback of PTAM, which is a feature base method and
 only the information that conforms to the feature type can be used.
 LSD LSD SLAM on the other hand
\change_unchanged
 optimizes the geometry directly on the image intensities, which enables
 using all information in the image.In addition to higher accuracy and robustness
 in particular in environments with little key-points, this provides substantial
ly more information about the geometry of the environment.Images are aligned
 by Gauss -Newton minimization of the photometric error.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
E(\xi)=\underset{i}{\sum}(I_{ref}(P_{i})-I(\omega(p_{i},D_{ref}(p_{i}),\xi)))^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
Where 
\begin_inset Formula $D_{ref}$
\end_inset

 is the estimated depth of reference frame,
\begin_inset Formula $\xi\in se(3)$
\end_inset

 is Lie-algebra representation of rigid body motion and 
\begin_inset Formula $\omega$
\end_inset

 is the affine wrap function.
 The above error function gives the maximum-likelihood estimator for 
\begin_inset Formula $\xi$
\end_inset

 assuming i.i.d.
 Gaussian residuals.
 
\begin_inset Formula $\delta\xi^{(n)}$
\end_inset

 is computed for each iteration by solving for the minimum of Gauss-Newton
 second-order approximation of 
\change_deleted 1699939148 1515788038
E
\change_inserted 1699939148 1515788038

\begin_inset Formula $E$
\end_inset


\change_unchanged
:
\change_inserted 1699939148 1515788112
 [no description of image warping needed here, briefly discuss image warping
 for direct methods as a methods for image pixel matching see Baker2004
 http://www.ncorr.com/download/publications/bakerunify.pdf]
\change_unchanged

\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\delta\xi^{(n)}=-(J^{T}J)^{-1}J^{T}r(\xi^{(n)})
\]

\end_inset


\end_layout

\begin_layout Standard
with 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
J=\frac{\partial r(\epsilon\circ\delta\xi^{(n)})}{\partial\epsilon}
\]

\end_inset


\end_layout

\begin_layout Standard
The new estimate is then obtained by multiplication with the computed update
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\xi^{(n+1)}=\delta\xi^{(n)}\circ\xi^{(n)}
\]

\end_inset


\end_layout

\begin_layout Standard
The overall system is composed of three major components, tracking, depth
 map estimation and map optimization.
\end_layout

\begin_layout Standard
The tracking component continuously estimates the rigid body pose with respect
 to the current 
\change_deleted 215191885 1516608687
keyframe
\change_inserted 215191885 1516608687
key-frame
\change_unchanged
, using the pose of the previous frame as initialization.LSD slam tracks
 new frame by minimizing the variance-normalized photometric error
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
E_{p}(\xi_{ji})=\underset{p\in\Omega_{D_{i}}}{\sum}\left\Vert \frac{r_{p}^{2}(p,\xi_{ji})}{\sigma_{r_{p}(p,\xi_{ji})}^{2}}\right\Vert _{\delta}
\]

\end_inset


\end_layout

\begin_layout Standard
with
\begin_inset Formula 
\[
r_{p}(p,\xi_{ji})\coloneqq I_{i}(p)-I_{j}(\omega(p,D_{i}(p),\xi_{ji}))
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\sigma_{r_{p}(p,\xi_{ji})}^{2}\coloneqq2\sigma_{I}^{2}+\left(\frac{\partial r_{p}(p,\xi_{ji})}{\partial D_{i}(p)}\right)^{2}V_{i}(p)
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\left\Vert .\right\Vert $
\end_inset

 is the Huber norm
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\left\Vert r^{2}\right\Vert _{\delta}\coloneqq\begin{cases}
\frac{r^{2}}{2\delta}if\left\Vert r\right\Vert \leq\delta & ,\left\Vert r\right\Vert -\frac{\delta}{2}\end{cases}otherwise
\]

\end_inset


\end_layout

\begin_layout Standard
applied to the normalized residual.
 The depth map estimation component uses tracked frames to either refine
 or replace the current key-frame.
 Depth is refined by filtering over many per-pixel, small-baseline stereo
 comparisons coupled with interleaved spatial regularization.
 If the camera has moved too far, a new key-frame is initialized by projecting
 points from existing, close-by key-frames into it.
 Each key-frame is scaled such that its mean inverse depth is one, which
 enables more small-baseline stereo comparisons.
 For every new key-frame added, the possibility of loop closure is checked
 by performing the reciprocal tracking check.
 The map optimization component is responsible for the updating depth map
 into global map, it detect loop closure and scale drift by estimating similarit
y transform (sim(3)) to close by existing key-frames.The global map is represente
d as a pose graph consisting of key-frames as vertices's with 3D similarity
 transforms as edges, elegantly incorporating changing scale of the environment
 and allowing to detect and correct accumulated drift.
 Each key-frame consists of a camera image, an inverse depth map and variance
 of the inverse depth.Edges between key-frames contain their relative alignment
 as similarity transform, as well as corresponding covariance matrix.
 The map, consisting of a set of key-frames and tracked sim(3)-constraints,
 is continuously optimized in the background using pose graph optimization.
 The error function that is minimized is defined by (W defining the world
 frame) 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
E(\xi_{W1}....\xi_{Wn})\coloneqq\underset{(\xi_{ji},\Sigma_{ji})\in\varepsilon}{\sum}(\xi_{ji}\circ\xi_{Wi}^{-1}\circ\xi_{Wj})\Sigma_{ji}^{-1}(\xi_{ji}\circ\xi_{Wi}^{-1}\circ\xi_{Wj})
\]

\end_inset


\change_inserted 215191885 1521031169

\end_layout

\begin_layout Chapter

\change_inserted 215191885 1521031399
RELATED WORK AND BACKGROUND INFORMATION
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521031399
This article proposes fusion of SfM-estimated depths with depths captured
 from an RGBD image sensor.
 This section is dedicated to discussing the relevant aspects of the SfM
 algorithm and the sensed RGBD measurements necessary to explain the proposed
 fusion method.
 Specifically, this section reviews the theoretical details of the SfM algorithm
, methods for processing depth images including computing depth im- ages
 for arbitrary camera poses, and details existing knowledge regarding the
 sensor measurement noise for RGBD depth measurements
\end_layout

\begin_layout Standard

\change_deleted 215191885 1521031758
Methodology
\change_unchanged

\end_layout

\begin_layout Standard

\change_deleted 215191885 1521031840
Structure from motions (SfM) is the process of triangulating the three-dimension
al structure from two-dimensional images, along with estimating the motion
 of camera (visual odometer), hence it is some time called as Visual SLAM.

\change_unchanged
 
\change_inserted 215191885 1521037387

\end_layout

\begin_layout Section

\change_inserted 215191885 1521044107
Image alignment
\begin_inset CommandInset label
LatexCommand label
name "sec:Image-alignment"

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521039966
The image alignment is the process of tranforming (wraping) one image with
 respect to another image, with a goal to minimizng the total differance
 between the intensites 
\begin_inset CommandInset citation
LatexCommand cite
key "Baker:2004:LYU:964568.964604"

\end_inset

.
\begin_inset Formula 
\begin{equation}
\underset{x}{\sum}\left[I(\omega(x))-T(x)\right]^{2}\label{eq:image alignment}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521039875
In eq 
\begin_inset CommandInset ref
LatexCommand ref
reference "wraping function"

\end_inset

 the wrap function 
\begin_inset Formula $\omega$
\end_inset

 defroms image I along the image T in an atempt to minmize the differance
 between them.
 The wrap function which achive this goal is estimated incremental by perfroming
 gradiant descent.In order to achive this, we need to find the correspondace
 between images.
\change_unchanged

\end_layout

\begin_layout Subsubsection

\change_inserted 215191885 1521039811
Solving for Image Pixel Correspondences
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521039811
There are generically two different approaches for finding corresponding
 observations of the same 3D surface location in multiple images referred
 to as 
\emph on
direct 
\emph default
and 
\emph on
indirect 
\emph default

\begin_inset CommandInset citation
LatexCommand cite
key "7898369"

\end_inset


\emph on
.
 
\emph default
In this discussion, we refer to the correspondence problem as a source-to-target
 matching problem.
 Let 
\begin_inset Formula $\mathbf{I}_{t}(x,y)$
\end_inset

 denote an image recorded at time 
\begin_inset Formula $t$
\end_inset

 and 
\begin_inset Formula $\mathbf{I}_{t+\Delta t}(x,y)$
\end_inset

 denote a subsequent image measured at time 
\begin_inset Formula $t+\Delta t$
\end_inset

.
 The correspondence problem seeks to find a map that transforms pixels from
 the original 
\begin_inset Formula $(x,y)$
\end_inset

 coordinate field of 
\begin_inset Formula $\mathbf{I}_{t}$
\end_inset

 to new coordinate positions 
\begin_inset Formula $(x',y')$
\end_inset

 in 
\begin_inset Formula $\mathbf{I}_{t+\Delta t}$
\end_inset

 such that 
\begin_inset Formula $\mathbf{I}_{t}(x,y)$
\end_inset

 and 
\begin_inset Formula $\mathbf{I}_{t+\Delta t}(x',y')$
\end_inset

 correspond to images of the same 3D scene point.
 
\end_layout

\begin_layout Subsubsection*

\change_inserted 215191885 1521039811
Indirect Methods
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521039811
Indirect methods compute this mapping by detecting special 
\begin_inset Formula $(x,y)$
\end_inset

 locations referred to as features locations with a purpose-built feature
 detection algorithm, e.g., the Harris corner detector 
\begin_inset CommandInset citation
LatexCommand cite
key "Harris88acombined"

\end_inset

.
 A description of the image patch in the vicinity of each detected 
\begin_inset Formula $(x,y)$
\end_inset

 location is computed using some feature descriptor algorithm, e.g., Lowe's
 SIFT descriptor 
\begin_inset CommandInset citation
LatexCommand cite
key "Lowe2004"

\end_inset

.
 Feature descriptors seek to provide a vector of values from the image patch
 data that is invariant to the image variations that occur during camera
 motion.
 These include but are not limited to the following effects: illumination
 variation, affine and/or projective invariance, photometric invariance
 (brightness constancy), and scale invariance.
 Popular feature descriptors often prioritize scale and affine invariance
 as their strengths.
 The invariance property allows for correspondences to be computed by finding
 the mapping from the feature descriptor set calculated from image 
\begin_inset Formula $\mathbf{I}_{t}(x,y)$
\end_inset

 to the feature descriptor set calculated from image 
\begin_inset Formula $\mathbf{I}_{t+\Delta t}(x,y)$
\end_inset

.
\end_layout

\begin_layout Subsubsection*

\change_inserted 215191885 1521039811
Direct Methods
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521043355
Direct methods on the other hand typically iteratively solve for a set of
 transformation parameters that best align a pair of images by the minimization
 of pixel-wise errors.
 An image warping function, 
\begin_inset Formula $\omega(\mathbf{x})$
\end_inset

, maps a pixel location, 
\begin_inset Formula $\mathbf{x}=[x,y]^{t}$
\end_inset

, in the original coordinate field to new coordinate positions, 
\begin_inset Formula $\mathbf{x}'$
\end_inset

, such that both locations correspond to images.
 A classical solution to this problem is given by the Lucas-Kanade-Tomassi
 (LKT) camera tracking algorithm 
\begin_inset CommandInset citation
LatexCommand cite
key "Baker:2004:LYU:964568.964604"

\end_inset

.
 
\end_layout

\begin_layout Section

\change_inserted 215191885 1521037368
Two unknows of the SfM
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521040943
The SfM algorithm uses a time sequence of images from a moving camera to
 recover the 3D geometry of objects viewed by the camera.
 While this problem can be solved without a calibrated camera, reconstruction
 accuracy will adversely affected.
 This work assumes that the camera calibration 
\begin_inset CommandInset citation
LatexCommand cite
key "5534797"

\end_inset

 parameters are known.
 The SfM algorithm can be broken down into two key steps:
\end_layout

\begin_layout Enumerate

\change_inserted 215191885 1521040943
estimation of the camera pose, i.e., position and orientation, at the time
 each image was recorded,
\end_layout

\begin_layout Enumerate

\change_inserted 215191885 1521040943
estimation of the 3D structure of the scene.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521040943
As previously mentioned, typical SfM systems solve (1) by computing a map
 that associates pixels from the original 
\begin_inset Formula $(x,y)$
\end_inset

 coordinate field to new coordinate positions 
\begin_inset Formula $(x',y')$
\end_inset

 such that both locations correspond to images of the same 3D scene point
 and (2) via multi-view 3D surface reconstruction algorithm, e.g., bundle
 adjustment 
\begin_inset CommandInset citation
LatexCommand cite
key "Triggs:1999:BAM:646271.685629"

\end_inset

.
 In the following sections we provide an overview of aspects of the SfM
 algorithm necessary for the development of the proposed RGBD-SfM depth
 fusion algorithm.
\end_layout

\begin_layout Subsection

\change_inserted 215191885 1521032545
Camera Trjectory
\change_unchanged

\end_layout

\begin_layout Subsubsection*
Rigid Body Motion
\change_inserted 215191885 1521044093

\begin_inset CommandInset label
LatexCommand label
name "subsec:Rigid-Body-Motion"

\end_inset


\change_unchanged

\end_layout

\begin_layout Standard

\change_deleted 215191885 1521032805
One of the goals of SfM it to estimate the
\change_inserted 215191885 1521032806
The
\change_unchanged
 Camera trajectory
\change_deleted 215191885 1521032811
, which 
\change_unchanged
is a rigid body motion.
 We need an efficient model to represent and compute this rigid body motion.
 The camera position is represented by a 3D Vector in an Euclidean 
\change_deleted 1699939148 1515788126
S
\change_unchanged
space, this camera position is chosen to represent 
\change_inserted 215191885 1521032965
origin of 
\change_unchanged
the world frame and specify the translation and rotation of the scene relative
 to 
\change_deleted 215191885 1521032976
that
\change_inserted 215191885 1521032976
this
\change_unchanged
 frame.
 The rigid body motion itself is composed of a rotation and translation.
 
\end_layout

\begin_layout Standard
Traditionally, rotation is represented by a 
\begin_inset Formula $3\times3$
\end_inset

 special orthogonal matrix called rotational matrix.
 Special Orthogonal matrix SO(3) are the matrix which satisfy 
\begin_inset Formula $R^{T}R=RR^{T}=I$
\end_inset

 and have a determinant of 
\begin_inset Formula $+1$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
SO(3)=\{R\in\mathbb{R}^{3\times3}\mid R^{T}R=I,det(R)=+1\}
\]

\end_inset


\end_layout

\begin_layout Standard
The rotation transformation of the coordinates 
\begin_inset Formula $X_{c}$
\end_inset

of a point p relative to frame C to its coordinates 
\begin_inset Formula $X_{w}$
\end_inset

 relative to frame W is 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
X_{w}=R_{wc}X_{c}
\]

\end_inset


\end_layout

\begin_layout Standard
Also, because the rotational matrix are orthogonal, we have
\begin_inset Formula $R^{-1}=R^{T}$
\end_inset

, on this line the inverse transformation of coordinates are 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
X_{c}=R_{wc}^{-1}X_{w}=R_{wc}^{T}X_{w}
\]

\end_inset


\end_layout

\begin_layout Standard
The continuous rotation of a camera is described as a trajectory 
\begin_inset Formula $R(t):t\rightarrow SO(3)$
\end_inset

 in the space 
\begin_inset Formula $SO(3)$
\end_inset

.When the starting time is not t = 0, the relative motion between time 
\begin_inset Formula $t_{2}$
\end_inset

 and time 
\begin_inset Formula $t_{1}$
\end_inset

 will be denoted as 
\begin_inset Formula $R(t_{2},t_{1})$
\end_inset

.
 The composition law of the rotation group implies
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
R(t_{2,}t_{0})=R(t_{2,}t_{1})\times R(t_{1,}t_{0}),\vee t_{0}<t_{1}<t_{2}\in R
\]

\end_inset


\end_layout

\begin_layout Standard
On the other hand the translation is represented by a 
\begin_inset Formula $T\in R^{3}$
\end_inset

,
\begin_inset Formula $1\times3$
\end_inset

 vector which adds the translation values in each dimension.With this, the
 complete rigid body motion is represented by 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
X_{w}=R_{wc}X_{c}+T_{wc}\label{eq:1}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
However, the above equation is not linear but affine.
 We may convert this to linear by using homogeneous coordinates, where we
 append 1 for 
\begin_inset Formula $1\times3$
\end_inset

 vector and make it a 
\begin_inset Formula $1\times4$
\end_inset

 vector,
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\bar{X}=\left[\begin{array}{c}
X\\
1
\end{array}\right]=\left[\begin{array}{c}
X_{1}\\
X_{2}\\
X_{3}\\
1
\end{array}\right]\in\mathbb{R}^{4}
\]

\end_inset


\end_layout

\begin_layout Standard
With this new notation for point, we can rewrite the transformation from
 equation 6 as following
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\bar{X}_{w}=\left[\begin{array}{c}
X_{w}\\
1
\end{array}\right]=\left[\begin{array}{cc}
R_{wc} & T_{wc}\\
0 & 1
\end{array}\right]\left[\begin{array}{c}
X_{c}\\
1
\end{array}\right]=\bar{g}_{wc}\bar{X}_{c}\label{eq:2}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where the 
\begin_inset Formula $4\times4$
\end_inset

 matrix 
\begin_inset Formula $\bar{g}_{wc}\in R^{4\times4}$
\end_inset

is called the homogeneous representation of the rigid-body motion.
\end_layout

\begin_layout Standard
The set of all possible configurations of a rigid body can then be described
 by the space of rigid-body motions or special Euclidean transformations
 called SE(3)
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
SE(3)=\{\bar{g}=\left[\begin{array}{cc}
R & T\\
0 & 1
\end{array}\right]\mid R\in SO(3),T\in R^{3}\}\subset\mathbb{R}^{4\times4}\label{eq:se3}
\end{equation}

\end_inset


\change_inserted 215191885 1516636715

\end_layout

\begin_layout Standard

\change_inserted 215191885 1516638341
Finally, one other dimension we could consider is the scale, this can represente
d by a Similarity transformation 
\begin_inset Formula $Sim(3)$
\end_inset

, which is the composition of a rotation, translation and a uniform rescaling,
 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1516637515
\begin_inset Formula 
\begin{equation}
Sim(3)=\{\bar{g}=\left[\begin{array}{cc}
R & T\\
0 & s^{-1}
\end{array}\right]\mid R\in SO(3),T\in R^{3},s\in R\}\subset\mathbb{R}^{4\times4}\label{eq:sim3}
\end{equation}

\end_inset


\change_unchanged

\end_layout

\begin_layout Subsubsection*
Exponential Map
\end_layout

\begin_layout Standard
The special orthogonal group in three dimensions can be represented by a
 
\begin_inset Formula $3\times3$
\end_inset

 rotation matrix 
\begin_inset Formula $R\in SO(3)$
\end_inset

, which must satisfy the constraint 
\begin_inset Formula $R^{T}R=I$
\end_inset

, this implies that the 
\begin_inset Formula $SO(3)$
\end_inset

 transformations leaves the quantity 
\begin_inset Formula $x^{2}+y^{2}+z^{2}$
\end_inset

 invariant.
 The group 
\begin_inset Formula $SO(3)$
\end_inset

 has 9 parameters, but the invariance of the length produces six independent
 conditions, leaving three free parameters, Hence, the dimension of the
 space of rotation matrices 
\begin_inset Formula $SO(3)$
\end_inset

 should be only three, and six parameters out of the nine are in fact redundant.W
e can use this to have better representation of Rigid body motion.
\change_inserted 215191885 1516614036

\end_layout

\begin_layout Standard

\change_inserted 215191885 1516618836
The 
\begin_inset Formula $SO(3)$
\end_inset

, 
\begin_inset Formula $SE(3)$
\end_inset

 and 
\begin_inset Formula $SIM(3)$
\end_inset

are categorized under the special group called the Lie group, which represents
 the smooth differentiable manifolds.
 Every Lie group has a tangent space at identity called the Lie algebra,
 which is a vector space used to study the infinitesimal transformations.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1516618136
For rotation group 
\begin_inset Formula $SO(3)$
\end_inset

 its lie algebra 
\begin_inset Formula $so(3)$
\end_inset

 is represented by
\end_layout

\begin_layout Standard

\change_inserted 215191885 1516619360
\begin_inset Formula 
\[
so(3)\doteq\left\{ \hat{\omega}\in\mathbb{R}^{3\times3}\mid\omega\in\mathbb{R}^{3}\right\} 
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1516619394
where 
\begin_inset Formula $\hat{\omega}$
\end_inset

is a skew symmetric matrix for the vector 
\begin_inset Formula $\omega$
\end_inset

 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1516618683
The map from the space 
\begin_inset Formula $so(3)$
\end_inset

 to 
\begin_inset Formula $SO(3)$
\end_inset

 is called the exponential map.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1516618685
\begin_inset Formula 
\[
exp:so(3)\rightarrow SO(3);\hat{\omega}\mapsto e^{\hat{\omega}}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1516618599
\begin_inset Formula 
\begin{equation}
R(t)=e^{\hat{\omega}t}\label{exp map for so(3)}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1516619295
The equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "exp map for so(3)"

\end_inset

 represents a rotation around the axis 
\begin_inset Formula $\omega\in\mathbb{R}^{3}$
\end_inset

 by an angle of 
\begin_inset Formula $t$
\end_inset

 radians.
 And the inverse mapping is obtained by logarithm of SO(3)
\end_layout

\begin_layout Standard

\change_inserted 215191885 1516618286
\begin_inset Formula 
\[
log:SO(3)\rightarrow so(3);log(R)\mapsto\hat{\omega}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1516618390
We can extend this to full rigid body motion which also involves the translation
 along with the rotation, for rotation group 
\begin_inset Formula $SE(3)$
\end_inset

 its lie algebra 
\begin_inset Formula $se(3)$
\end_inset

 is represented by .
\end_layout

\begin_layout Standard

\change_inserted 215191885 1516618391
\begin_inset Formula 
\[
se(3)\doteq\left\{ \hat{\xi}=\left[\begin{array}{cc}
\hat{\omega} & \upsilon\\
0 & 0
\end{array}\right]\mid\hat{\omega}\in so(3),\upsilon\in\mathbb{R}^{3}\right\} \subset\mathbb{R}^{4\times4}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1516618477
with 
\begin_inset Formula $\upsilon(t)=\dot{T}(t)-\hat{\omega}(t)T(t)\in\mathbb{R}^{3}$
\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1516618546
Similarly, the exponential map from the space 
\begin_inset Formula $se(3)$
\end_inset

 to 
\begin_inset Formula $SE(3)$
\end_inset

is given by
\end_layout

\begin_layout Standard

\change_inserted 215191885 1516618478
\begin_inset Formula 
\[
exp:se(3)\rightarrow SE(3);\hat{\xi}\mapsto e^{\hat{\xi}}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1516618478
and as before inverse to the exponential map is defined by logarithm 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1516639609
\begin_inset Formula 
\[
log:SE(3)\rightarrow se(3);log(g)\mapsto\hat{\xi}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618781
We know that the continuous rotational motion represented by 
\begin_inset Formula $R(t):R\in SO(3)$
\end_inset

, must satisfy the following constraint
\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618781
\begin_inset Formula 
\[
R(t)R^{T}(t)=I
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618781
Differentiating the above equation with respect to time t gives
\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618781
\begin_inset Formula 
\[
\dot{R}(t)R^{T}(t)+R(t)\dot{R}^{T}(t)=0
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618781
\begin_inset Formula 
\[
\dot{R}(t)R^{T}(t)=-(\dot{R}(t)R^{T}(t))^{T}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618781
This shows that the matrix 
\begin_inset Formula $\dot{R}(t)R^{T}(t)\in\mathbb{R}^{3\times3}$
\end_inset

is a skew-symmetric matrix.This implies that there must exist a vector, say
 
\begin_inset Formula $\omega(t)\in\mathbb{R}^{3}$
\end_inset

,such that
\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618781
\begin_inset Formula 
\begin{equation}
\dot{R}(t)R^{T}(t)=\hat{\omega}(t)\label{eq:3-1}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618781
Multiplying both sides by 
\begin_inset Formula $R(t)$
\end_inset

 on the right yields
\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618781
\begin_inset Formula 
\begin{equation}
\dot{R}(t)=\hat{\omega}(t)R(t)\label{eq:3}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618781
In above equation, if 
\begin_inset Formula $R(t_{0})=I$
\end_inset

 for for 
\begin_inset Formula $t=t_{0}$
\end_inset

, we have 
\begin_inset Formula $\dot{R}(t_{0})=\hat{\omega}(t_{0})$
\end_inset

.
 Hence, around the identity matrix I, a skew-symmetric matrix gives a first-
 order approximation to a rotation matrix:
\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618781
\begin_inset Formula 
\[
R(t_{0}+dt)\thickapprox I+\hat{\omega}(t_{0})dt
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618781
This space of all skew-symmetric matrix represents the tangent space of
 the rotation group 
\begin_inset Formula $SO(3)$
\end_inset

 and it is the lie algebra 
\begin_inset Formula $so(3)$
\end_inset

 of the corresponding lie group.
\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618781
\begin_inset Formula 
\[
so(3)\doteq\left\{ \hat{\omega}\in\mathbb{R}^{3\times3}\mid\omega\in\mathbb{R}^{3}\right\} 
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618781
Once we have so(3), we need a way to map SO(3) to so(3).
 
\change_deleted 1699939148 1515788165
,
\change_deleted 215191885 1516618781
It is obvious that the solution for 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:3"

\end_inset

 is the matrix exponential 
\begin_inset Formula $e^{\hat{\omega}t}$
\end_inset

, where
\change_inserted 1699939148 1515788319
 [THIS IS TOO SIMILAR to source work elsewhere, bordering on plagiarism
 – REMOVE!, you should not be going into this much detail for the differential
 properties of rotations as this is not something you are comfortable with,
 write down the most important relationships, i.e., the differential of the
 rotation and why you need it for SfM, you will want to define sim3 transforms
 and how they are distinct from se3 transforms then leave this topic]
\change_inserted 215191885 1516640005
 [I wrote this, because I had done some study on exponential map between
 lie group and lie algebra and vis versa, I was hoping to keep this, but
 let me know your thoughts ] [exponential map of SIM(3) is way too complicated
 and I am working on it, I will incorporate it going forward.
 ]
\change_unchanged

\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
\begin_inset Formula 
\[
e^{\hat{\omega}t}=I+\hat{\omega}t+\frac{(\hat{\omega}t)^{2}}{2!}+\cdots+\frac{(\hat{\omega}t)^{n}}{n!}+\cdots
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
Hence, we have 
\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
\begin_inset Formula 
\begin{equation}
R(t)=e^{\hat{\omega}t}\label{eq:4}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
The above equation represents a rotation around the axis 
\begin_inset Formula $\omega\in\mathbb{R}^{3}$
\end_inset

 by an angle of 
\begin_inset Formula $t$
\end_inset

 radians.This map from the space 
\begin_inset Formula $so(3)$
\end_inset

 to 
\begin_inset Formula $SO(3)$
\end_inset

 is called the exponential map.
\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
\begin_inset Formula 
\[
exp:so(3)\rightarrow SO(3);\hat{\omega}\mapsto e^{\hat{\omega}}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
And the inverse mapping is obtained by logarithm of SO(3)
\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
\begin_inset Formula 
\[
log:SO(3)\rightarrow so(3);log(R)\mapsto\hat{\omega}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
We can extend this to full rigid body motion which also involves the translation
 along with the rotation.From 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:2"

\end_inset

, the continuous rigid body trajectory on SE(3) is given by
\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
\begin_inset Formula 
\[
g(t)=\left[\begin{array}{cc}
R(t) & T(t)\\
0 & 1
\end{array}\right]\in\mathbb{R}^{4\times4}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
With above representation, we have
\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
\begin_inset Formula 
\[
\dot{g}(t)g^{-1}(t)=\left[\begin{array}{cc}
\dot{R}(t)R^{T}(t) & \dot{T}(t)-\dot{R}(t)R^{T}(t)T(t)\\
0 & 0
\end{array}\right]\in\mathbb{R}^{4\times4}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
with 
\begin_inset Formula $\upsilon(t)=\dot{T}(t)-\hat{\omega}(t)T(t)\in\mathbb{R}^{3}$
\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:3-1"

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
\begin_inset Formula 
\begin{equation}
\hat{\xi(t)}=\left[\begin{array}{cc}
\hat{\omega}(t) & \upsilon(t)\\
0 & 0
\end{array}\right]\in\mathbb{R}^{4\times4}\label{eq:6}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
then we have
\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
\begin_inset Formula 
\[
\dot{g}(t)=(\dot{g}(t)g^{-1}(t))g(t)=\hat{\xi(t)}g(t)
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
where 
\begin_inset Formula $\hat{\xi}$
\end_inset

 is called the twist and can be used to approximate g(t) locally
\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
\begin_inset Formula 
\[
g(t+dt)\approx g(t)+\hat{\xi(t)}g(t)d(t)=(I+\hat{\xi(t)}dt)g(t)
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
Also the twist represent the tangent space (or Lie algebra) of the matrix
 group SE(3).
\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
\begin_inset Formula 
\[
se(3)\doteq\left\{ \hat{\xi}=\left[\begin{array}{cc}
\hat{\omega} & \upsilon\\
0 & 0
\end{array}\right]\mid\hat{\omega}\in so(3),\upsilon\in\mathbb{R}^{3}\right\} \subset\mathbb{R}^{4\times4}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
Similar 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:4"

\end_inset

.
 with the initial condition of 
\begin_inset Formula $g(0)=1,$
\end_inset

we have
\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
\begin_inset Formula 
\[
g(t)=e^{\hat{\xi}t}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
where
\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
\begin_inset Formula 
\[
e^{\hat{\xi}t}=I+\hat{\xi}t+\frac{(\hat{\xi}t)^{2}}{2!}+\cdots+\frac{(\hat{\xi}t)^{n}}{n!}+\cdots
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
This defines the the exponential map from the space 
\begin_inset Formula $se(3)$
\end_inset

 to 
\begin_inset Formula $SE(3)$
\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
\begin_inset Formula 
\[
exp:se(3)\rightarrow SE(3);\hat{\xi}\mapsto e^{\hat{\xi}}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
and as before inverse to the exponential map is defined by logarithm 
\end_layout

\begin_layout Standard

\change_deleted 215191885 1516618794
\begin_inset Formula 
\[
log:SE(3)\rightarrow se(3);log(g)\mapsto\hat{\xi}
\]

\end_inset


\change_inserted 215191885 1517225203

\end_layout

\begin_layout Subsubsection*

\change_inserted 215191885 1517225217
Visual Odometry
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521046655
With this efficiant represenation for rigid body tranformation eq 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:sim3"

\end_inset

,the trajectory of camera is determined by the process of direct image alignment
 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Image-alignment"

\end_inset

, i.e we can incrementaly find the relative pose 
\begin_inset Formula $\xi\in SIM(3)$
\end_inset

that minimize the photometric error between image pixels.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521045043
\begin_inset Formula 
\begin{equation}
\widehat{\xi}=\min_{\xi}\underset{\mathbf{x}}{\sum}\left(\mathbf{I}(\omega(\mathbf{x},\xi))-\mathbf{T}(\mathbf{x})\right)^{2}\label{eq:Direct image alignment}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521045080
In equation (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Direct image alignment"

\end_inset

), the warp function, 
\begin_inset Formula $\omega(\mathbf{x},\xi)$
\end_inset

, maps pixel locations, 
\begin_inset Formula $\mathbf{x}$
\end_inset

, in the template to pixel locations in the image 
\begin_inset Formula $\mathbf{I}(\mathbf{x})$
\end_inset

 using the warp transformation parameters 
\begin_inset Formula $\xi$
\end_inset

.
 For direct correspondence estimation, 
\begin_inset Formula $\xi$
\end_inset

 is a pose transformation of the viewing camera represented as a unknown
 6x1 vector.
 We then seek the camera pose transformation parameters, 
\begin_inset Formula $\widehat{\xi}$
\end_inset

, that minimize equation which provides the camera pose change that best
 explain the differences in these two of the same scene.
 Given two images and the camera pose change between them, one can take
 the information in one image, and through the warp function and map these
 values into the viewpoint of the other image establishing a correspondence.
 In this case the theoretical difference between expected and observed values
 for the image pair is zero if the camera pose change is known exactly and
 sensor noise and other outside influences are ignored.
 
\end_layout

\begin_layout Subsection

\change_inserted 215191885 1517228347
Depth Mapping
\end_layout

\begin_layout Subsubsection*
Camera model
\end_layout

\begin_layout Standard
The 2D image is formed by capturing the light energy (irradiance) for every
 pixel, this process can be mathematically represented by thin lens camera
 model, which describes the relationship between the three-dimensional coordinat
ors to its projection onto the image plane.
 The thin lens model is represented by a optical axis and a perpendicular
 plane called the focal plane.The thin lens itself is characterized by its
 focal length and diameter, the focal length is the distance from optic
 center to where all the ray intersect the optic axis, while the point of
 intersection itself is called the focus of the lens.
 One of the important properties to consider is that the rays entering the
 lens through optic center are undeflected while the rest of the rays are.
 With this model the irradiance on the image plane is obtained by the integratio
n of all the energy emitted from region of space contained in the cone determine
d by the geometry of the lens.
 
\end_layout

\begin_layout Standard

\change_deleted 1699939148 1515788341
\begin_inset Graphics
	filename images/thin_lens_Cam.png
	width 8cm
	height 6cm

\end_inset


\change_inserted 1699939148 1515788341

\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 1699939148 1515788341
\begin_inset Graphics
	filename images/thin_lens_Cam.png
	lyxscale 25
	width 8cm
	height 6cm

\end_inset


\change_unchanged

\end_layout

\begin_layout Plain Layout

\change_inserted 1699939148 1515788341
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1516609076
Thin lens camera model 
\change_deleted 215191885 1516609014
Put caption here explain variables and what I'm looking at
\change_unchanged

\end_layout

\end_inset


\change_unchanged

\end_layout

\end_inset


\change_unchanged

\end_layout

\begin_layout Standard
Using similar triangles, from above figure, we obtain the following fundamental
 equation of the thin lens
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\frac{1}{Z}+\frac{1}{z}=\frac{1}{f}
\]

\end_inset


\end_layout

\begin_layout Standard
For the simplification of calculation, we consider a Ideal camera model
 called the pinhole camera model, here the aperture of a thin lens is assumed
 to decreased to zero, all rays are forced to go through the optical center
 o, and therefore they remain undeflected.Consequently,as the aperture of
 the cone decreases to zero, the only points that contribute to the irradiance
 at the image point 
\begin_inset Formula $x=\left[x,y\right]$
\end_inset

 are on a line through the center 0 of the lens.
 If a point p has coordinates 
\begin_inset Formula $X=\left[X,Y,Z\right]$
\end_inset

 relative to a reference frame centered at the optical center 0, with its
 z-axis being the optical axis (of the lens), then it is immediate to see
 from similar triangles in Figure that the coordinates of p and its image
 x are related by the so-called ideal perspective projection.
\end_layout

\begin_layout Standard

\change_deleted 1699939148 1515788385
\begin_inset Graphics
	filename images/pinhole_cam.png
	width 8cm
	height 6cm

\end_inset


\change_inserted 1699939148 1515788385

\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 1699939148 1515788385
\begin_inset Graphics
	filename images/pinhole_cam.png
	width 8cm
	height 6cm

\end_inset


\change_unchanged

\end_layout

\begin_layout Plain Layout

\change_inserted 1699939148 1515788385
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1516609093
Pin-hole camera model 
\change_deleted 215191885 1516609091
Caption here
\change_unchanged

\end_layout

\end_inset


\change_unchanged

\end_layout

\end_inset


\change_unchanged

\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
x=-f\frac{X}{Z}\label{eq:pinhole camera x}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
y=-f\frac{Y}{Z}\label{eq:pinhole camera y}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 1699939148 1515788416
[write projection as a matrix transform similar to ROS]
\change_inserted 215191885 1516611116
[have it represented below]
\change_unchanged
This mapping of 3D point to 2D is called projection and is represented by
 
\begin_inset Formula $\pi$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\pi:R^{3}\rightarrow R^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
This is also written as 
\begin_inset Formula $x=\pi(X)$
\end_inset

.
 
\end_layout

\begin_layout Standard
The negative sign in the 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:pinhole camera x"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:pinhole camera y"

\end_inset

 makes the object appear upside down on the image plan, we can handle this
 by moving the image plane to front of optic center to 
\begin_inset Formula $\{z=-f\}$
\end_inset

 this will make 
\begin_inset Formula $(x,y)\rightarrow(-x,-y).$
\end_inset

There for the equation 2 and 3 changes to 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
x=f\frac{X}{Z}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
y=f\frac{Y}{Z}
\]

\end_inset


\end_layout

\begin_layout Standard
This can be represented in matrix form as 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
x=\left[\begin{array}{c}
x\\
y
\end{array}\right]=\frac{f}{Z}\left[\begin{array}{c}
X\\
Y
\end{array}\right]
\]

\end_inset

 In homogeneous coordinates, this relationship can be modified as
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
Z\left[\begin{array}{c}
x\\
y\\
1
\end{array}\right]=\left[\begin{array}{cccc}
f & 0 & 0 & 0\\
0 & f & 0 & 0\\
0 & 0 & 1 & 0
\end{array}\right]\left[\begin{array}{c}
X\\
Y\\
Z\\
1
\end{array}\right]
\]

\end_inset


\end_layout

\begin_layout Standard
The above equation can be decomposed into
\begin_inset Formula 
\[
\left[\begin{array}{cccc}
f & 0 & 0 & 0\\
0 & f & 0 & 0\\
0 & 0 & 1 & 0
\end{array}\right]=\left[\begin{array}{ccc}
f & 0 & 0\\
0 & f & 0\\
0 & 0 & 1
\end{array}\right]\left[\begin{array}{cccc}
1 & 0 & 0 & 0\\
0 & 1 & 0 & 0\\
0 & 0 & 1 & 0
\end{array}\right]
\]

\end_inset


\end_layout

\begin_layout Standard
with
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
K_{f}=\left[\begin{array}{cccc}
f & 0 & 0 & 0\\
0 & f & 0 & 0\\
0 & 0 & 1 & 0
\end{array}\right]\in R^{3\times3},\Pi_{0}=\left[\begin{array}{cccc}
1 & 0 & 0 & 0\\
0 & 1 & 0 & 0\\
0 & 0 & 1 & 0
\end{array}\right]\in\mathbb{R}{}^{3\times4}
\]

\end_inset


\end_layout

\begin_layout Standard
The matrix 
\begin_inset Formula $\Pi_{0}$
\end_inset

is a standard projection matrix.
\end_layout

\begin_layout Standard
With the rigid body transformation from 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:2"

\end_inset

, we can represent the overall geometric model for an ideal camera as below
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\lambda\left[\begin{array}{c}
x^{\prime}\\
y^{\prime}\\
1
\end{array}\right]=\left[\begin{array}{ccc}
f & 0 & 0\\
0 & f & 0\\
0 & 0 & 1
\end{array}\right]\left[\begin{array}{cccc}
1 & 0 & 0 & 0\\
0 & 1 & 0 & 0\\
0 & 0 & 1 & 0
\end{array}\right]\left[\begin{array}{cc}
R & T\\
0 & 1
\end{array}\right]\left[\begin{array}{c}
X_{0}\\
Y_{0}\\
Z_{0}\\
1
\end{array}\right]\label{eq:ideal camera model}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Where 
\begin_inset Formula $\lambda$
\end_inset

 is the unknown scale factor.
\end_layout

\begin_layout Standard
However, the above equation represents the ideal model where the retinal
 frame centered at the optical center with one axis aligned with the optical
 axis.But in practice, this does not true and the origin of the image coordinate
 frame typically in the upper-left corner of the image.we need to address
 this relationship between the retinal plane coordinate frame and the pixel
 array in our camera model.
 This can be represented by a special matrix as following
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
K_{s}=\left[\begin{array}{ccc}
s_{x} & s_{\theta} & o_{x}\\
0 & s_{y} & o_{y}\\
0 & 0 & 1
\end{array}\right]\in\mathbb{R}^{3\times3}
\]

\end_inset


\end_layout

\begin_layout Standard
with these new parameters the we can represent the interstice parameters
 of camera as following
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
K=K_{s}K_{f}=\left[\begin{array}{ccc}
s_{x} & s_{\theta} & o_{x}\\
0 & s_{y} & o_{y}\\
0 & 0 & 1
\end{array}\right]\left[\begin{array}{ccc}
f & 0 & 0\\
0 & f & 0\\
0 & 0 & 1
\end{array}\right]=\left[\begin{array}{ccc}
fs_{x} & s_{\theta} & o_{x}\\
0 & fs_{y} & o_{y}\\
0 & 0 & 1
\end{array}\right]
\]

\end_inset


\end_layout

\begin_layout Standard
where
\end_layout

\begin_layout Itemize
\begin_inset Formula $o_{x}$
\end_inset

: x-coordinate of the principal point in pixels, 
\end_layout

\begin_layout Itemize
\begin_inset Formula $o_{y}$
\end_inset

: y-coordinate of the principal point in pixels, 
\end_layout

\begin_layout Itemize
\begin_inset Formula $fs_{x}$
\end_inset

= 
\begin_inset Formula $\alpha_{x}$
\end_inset

 : size of unit length in horizontal pixels, 
\end_layout

\begin_layout Itemize
\begin_inset Formula $fs_{y}$
\end_inset

= 
\begin_inset Formula $\alpha_{y}$
\end_inset

 : size of unit length in vertical pixels, 
\end_layout

\begin_layout Itemize
\begin_inset Formula $\frac{\alpha_{x}}{\alpha_{y}}$
\end_inset

: aspect ratio 
\begin_inset Formula $\sigma$
\end_inset

, 
\end_layout

\begin_layout Itemize
\begin_inset Formula $fs_{\theta}$
\end_inset

: skew of the pixel, often close to zero.
\end_layout

\begin_layout Standard
Now, the ideal camera model 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:ideal camera model"

\end_inset

can be updated as 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\lambda\left[\begin{array}{c}
x^{\prime}\\
y^{\prime}\\
1
\end{array}\right]=\left[\begin{array}{ccc}
s_{x} & s_{\theta} & o_{x}\\
0 & s_{y} & o_{y}\\
0 & 0 & 1
\end{array}\right]\left[\begin{array}{cccc}
1 & 0 & 0 & 0\\
0 & 1 & 0 & 0\\
0 & 0 & 1 & 0
\end{array}\right]\left[\begin{array}{cc}
R & T\\
0 & 1
\end{array}\right]\left[\begin{array}{c}
X_{0}\\
Y_{0}\\
Z_{0}\\
1
\end{array}\right]\label{eq:camera model with internsic parameter}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
In the matrix notation,
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\lambda x=K\Pi_{0}gX_{0}\label{eq:camera model with internsic parameter matrix rep}
\end{equation}

\end_inset

 
\end_layout

\begin_layout Standard
To summarize, 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:camera model with internsic parameter matrix rep"

\end_inset

 represents the projection of three-dimensional coordinates 
\begin_inset Formula $X_{0}$
\end_inset

 by camera with orientation (extrinsic parameters)
\begin_inset Formula $g$
\end_inset

 and intrinsic parameters K, onto two-dimensional coordinate 
\begin_inset Formula $x$
\end_inset

 with known scale 
\begin_inset Formula $\lambda$
\end_inset


\end_layout

\begin_layout Standard
In addition to above linear distortion, if a camera has a wide field of
 view, there will be a significant distortion along radial directions called
 radial distortion.
 Such a distortion can be models by 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
x=x_{d}(1+a_{1}r^{2}+a_{2}r^{4})
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
y=y_{d}(1+a_{1}r^{2}+a_{2}r^{4})
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $(x_{d},y_{d})$
\end_inset

 are the coordinates of the distorted points, 
\begin_inset Formula $a_{1},a_{2}$
\end_inset

are the coefficients of radial distortion and 
\begin_inset Formula $r$
\end_inset

 is the radius of the radial distortion.
\change_inserted 215191885 1517237472

\end_layout

\begin_layout Subsubsection*
Epipolar geometry
\end_layout

\begin_layout Standard
Consider two images of the same point p from two camera position with relative
 pose 
\begin_inset Formula $(R,T)$
\end_inset

, where 
\begin_inset Formula $R\in SO(3)$
\end_inset

 is the relative orientation and 
\begin_inset Formula $T\in\mathbb{R}^{3}$
\end_inset

is the relative position,then if 
\begin_inset Formula $X_{1},X_{2}\in\mathbb{R}^{3}$
\end_inset

 are the 3-D coordinates of a point p relative to the two camera frames,
 by the rigid-body transformation we have
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
X_{2}=RX_{1}+T
\]

\end_inset

Now,let 
\begin_inset Formula $x_{1},x_{2}\in\mathbb{R}^{3}$
\end_inset

be the homogeneous coordinates of the projection of the same point p in
 the two image planes with respective unknown scales of 
\begin_inset Formula $\lambda_{1}$
\end_inset

and 
\begin_inset Formula $\lambda_{2}$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\lambda_{2}x_{2}=R\lambda_{1}x_{1}+T
\]

\end_inset


\end_layout

\begin_layout Standard
By multiplying both the side by 
\begin_inset Formula $\hat{T}$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\lambda_{2}\hat{T}x_{2}=\hat{T}R\lambda_{1}x_{1}
\]

\end_inset


\end_layout

\begin_layout Standard
By multiplying both the side by 
\begin_inset Formula $x_{2}$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
0=x_{2}^{T}\hat{T}R\lambda_{1}x_{1}\label{eq:epipolar constraint}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1516609203
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\change_inserted 215191885 1516609303
\begin_inset Graphics
	filename images/epipolar_geo.png
	width 8cm
	height 6cm

\end_inset


\end_layout

\begin_layout Plain Layout

\change_inserted 215191885 1516609203
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1516609245
Epipolar geometry
\change_unchanged

\end_layout

\end_inset


\change_unchanged

\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\change_deleted 215191885 1516609323

\begin_inset Graphics
	filename images/epipolar_geo.png
	width 8cm
	height 6cm

\end_inset


\change_unchanged

\end_layout

\begin_layout Standard
This is the epipolar constraint and the matrix 
\begin_inset Formula $E=\hat{T}R$
\end_inset

 is called the essential matrix.It encodes the relative pose between the
 two cameras.
 Geometrically, it impose that the The vector connecting the first camera
 center 
\begin_inset Formula $o_{1}$
\end_inset

 and the point p, the vector connecting 
\begin_inset Formula $o_{2}$
\end_inset

 and p, and the vector connecting the two optical centers 
\begin_inset Formula $o_{1}$
\end_inset

 and 
\begin_inset Formula $o_{2}$
\end_inset

 clearly form a triangle.
 Therefore, the three vectors lie on the same plane.Hence the their triple
 product which measures the volume of the parallelepiped is zero.
 
\end_layout

\begin_layout Subsubsection
The eight-point linear algorithm
\end_layout

\begin_layout Standard
With epipolar constrain between two images, we should be able to retrieve
 the relative pose of the cameras.
 The eight-point linear algorithm is a simple closed-form algorithm, it
 consists of two steps: First a matrix E is recovered from a number of epipolar
 constraints; then relative translation and orientation are extracted from
 E.
\end_layout

\begin_layout Standard
The entries of E are denoted by 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
E=\left[\begin{array}{ccc}
e_{11} & e_{12} & e_{13}\\
e_{21} & e_{22} & e_{23}\\
e_{31} & e_{32} & e_{33}
\end{array}\right]\in\mathbb{R}^{3\times3}
\]

\end_inset


\end_layout

\begin_layout Standard
The matrix E is reshaped into vector 
\begin_inset Formula $E\in\mathbb{R}^{9}$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
E=\left[e_{11},e_{21},e_{31},e_{12},e_{22},e_{32},e_{13},e_{23},e_{33}\right]^{T}
\]

\end_inset


\end_layout

\begin_layout Standard
and for 
\begin_inset Formula $x_{1}=\left[x_{1,}y_{1},z_{1}\right]^{T}\in\mathbb{R}^{3}$
\end_inset

and 
\begin_inset Formula $x_{2}=\left[x_{2,}y_{2},z_{2}\right]^{T}\in\mathbb{R}^{3}$
\end_inset

 define 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
a=[x_{1}x_{2},x_{1}y_{2},x_{1}z_{2},y_{1}x_{2},y_{1}y_{2},y_{1}z_{2},z_{1}x_{2},z_{1}y_{2},z_{1}z_{2}]\in\mathbb{R}^{9}
\]

\end_inset


\end_layout

\begin_layout Standard
With these new notations, we can rewrite the 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:epipolar constraint"

\end_inset

 as below
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
a^{T}E=0
\]

\end_inset


\end_layout

\begin_layout Standard
this representation emphasizes the linear dependence of the epipolar constraint
 on the elements of the essential matrix.
\end_layout

\begin_layout Standard
Now,with a set of corresponding image points 
\begin_inset Formula $(x_{1}^{j},x_{2}^{j}),j=1,2,....,n$
\end_inset

 we can define a matrix 
\begin_inset Formula $\chi\in\mathbb{R}^{n\times9}$
\end_inset

 associated with these measurements to be
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\chi=\left[a^{1},a^{2},...,a^{n}\right]^{T}
\]

\end_inset


\end_layout

\begin_layout Standard
In the absence of noise, the vector E satisfies
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\chi E=0
\]

\end_inset


\end_layout

\begin_layout Standard
In order to obtain the unique solution, the rank of the matrix 
\begin_inset Formula $\chi\in\mathbb{R}^{n\times9}$
\end_inset

 needs to be exactly eight.
 This should be the case when we have 
\begin_inset Formula $n\geq8$
\end_inset

 "ideal" corresponding points, hence the name The eight-point linear algorithm.
\end_layout

\begin_layout Standard
Because of errors in correspondences, we try to find the E that minimizes
 the least-squares error function 
\begin_inset Formula $\left\Vert \chi E\right\Vert ^{2}$
\end_inset

.We do this by choosing eigenvector of 
\begin_inset Formula $\chi^{T}\chi$
\end_inset

 that corresponds to its smallest eigenvalue.
\end_layout

\begin_layout Standard
Once we have E, we need to extract the pose (
\begin_inset Formula $R\in SO(3)$
\end_inset

 and 
\begin_inset Formula $T\in\mathbb{R}^{3}$
\end_inset

) from E, we know that 
\begin_inset CommandInset citation
LatexCommand cite
key "41368"

\end_inset

a nonzero matrix 
\begin_inset Formula $E\in\mathbb{R}^{3}$
\end_inset

 is an essential matrix if and only if E has a singular value decomposition
 
\begin_inset Formula $(SVD)E=U\varSigma V^{T}$
\end_inset

with 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\varSigma=diag\left\{ \sigma,\sigma,0\right\} 
\]

\end_inset


\end_layout

\begin_layout Standard
for some 
\begin_inset Formula $\sigma>0$
\end_inset

 and 
\begin_inset Formula $U,V,\in SO(3)$
\end_inset


\end_layout

\begin_layout Standard
With this we can obtain two relative pose
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
(\hat{T}_{1},R_{1})=(UR_{Z}(+\frac{\pi}{2})\Sigma U^{T},UR_{Z}^{T}(+\frac{\pi}{2})V^{T})
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
(\hat{T}_{2},R_{2})=(UR_{Z}(-\frac{\pi}{2})\Sigma U^{T},UR_{Z}^{T}(-\frac{\pi}{2})V^{T})
\]

\end_inset


\end_layout

\begin_layout Standard
Among these one with that gives the meaningful (positive) depth are selected
 as the valid pose.
\end_layout

\begin_layout Standard
With 
\begin_inset Formula $R_{Z}(\pm\frac{\pi}{2})=\left[\begin{array}{ccc}
0 & \pm1 & 0\\
\pm1 & 0 & 0\\
0 & 0 & 1
\end{array}\right]$
\end_inset


\end_layout

\begin_layout Subsubsection*
Structure Reconstruction
\end_layout

\begin_layout Standard
One remaining thing to find is the position of points in three-dimension
 by recovering their depths relative to each camera frame.With the estimated
 pose (Translation is T is defined up to the scale 
\begin_inset Formula $\gamma$
\end_inset

) and point correspondence, we have
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\lambda_{2}^{j}x_{2}^{j}=\lambda_{1}^{j}Rx_{1}^{j}+\gamma T
\]

\end_inset

 for 
\begin_inset Formula $j=1,2,3...,n$
\end_inset


\end_layout

\begin_layout Standard
where, 
\begin_inset Formula $\lambda_{1}$
\end_inset

and 
\begin_inset Formula $\lambda_{2}$
\end_inset

are depths with respect to the first and second camera frames, respectively.
 One of the depths is redundant, if 
\begin_inset Formula $\lambda_{1}$
\end_inset

 is known, we can estimate 
\begin_inset Formula $\lambda_{2}$
\end_inset

 as a function of 
\begin_inset Formula $(R,T)$
\end_inset

.Hence we can eliminate, say, 
\begin_inset Formula $\lambda_{2}$
\end_inset

 from the above equation by multiplying both sides by 
\begin_inset Formula $\hat{x_{2}}$
\end_inset

 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
0=\lambda_{1}^{j}x_{2}^{j}Rx_{1}^{j}+\gamma x_{2}^{j}T
\]

\end_inset


\end_layout

\begin_layout Standard
This is represented as linear equations
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
M^{j}\bar{\lambda^{j}}=\left[\hat{x_{2}^{j}}Rx_{1}^{j},\hat{x_{2}^{j}}T\right]\left[\begin{array}{c}
\lambda_{1}^{j}\\
\gamma
\end{array}\right]=0
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $M^{j}=\left[\hat{x_{2}^{j}}Rx_{1}^{j},\hat{x_{2}^{j}}T\right]\in\mathbb{R}^{3\times2}$
\end_inset

and 
\begin_inset Formula $\bar{\lambda^{j}}=\left[\begin{array}{c}
\lambda_{1}^{j}\\
\gamma
\end{array}\right]\in\mathbb{R}^{2}$
\end_inset

 for 
\begin_inset Formula $j=1,2,3...,n$
\end_inset


\end_layout

\begin_layout Standard
since all n equations above share the same 
\begin_inset Formula $\gamma$
\end_inset

; we define a vector 
\begin_inset Formula $\vec{\lambda}=\left[\lambda_{1}^{1},\lambda_{1}^{2},...,\lambda_{1}^{2},\gamma\right]$
\end_inset

 and a matrix 
\begin_inset Formula $M\in\mathbb{R}^{3n\times(n+1)}$
\end_inset

as
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
M=\left[\begin{array}{cccccc}
\hat{x}_{2}^{1}Rx_{1}^{1} & 0 & 0 & 0 & 0 & \hat{x}_{2}^{1}T\\
0 & \hat{x}_{2}^{2}Rx_{1}^{2} & 0 & 0 & 0 & \hat{x}_{2}^{2}T\\
0 & 0 & \ddots & 0 & 0 & \vdots\\
0 & 0 & 0 & \hat{x}_{2}^{n-1}Rx_{1}^{n-1} & 0 & \hat{x}_{2}^{n-1}T\\
0 & 0 & 0 & 0 & \hat{x}_{2}^{n}Rx_{1}^{n} & \hat{x}_{2}^{n}T
\end{array}\right]
\]

\end_inset


\end_layout

\begin_layout Standard
Then the equation
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
M\overrightarrow{\lambda}=0
\]

\end_inset


\end_layout

\begin_layout Standard
determines all the unknown depths up to a single universal scale.
 The linear least-squares estimate of 
\begin_inset Formula $\overrightarrow{\lambda}$
\end_inset

 is simply the eigenvector of 
\begin_inset Formula $M^{T}M$
\end_inset

s that corresponds to its smallest eigenvalue.
\change_inserted 215191885 1521047989

\end_layout

\begin_layout Subsection*

\change_inserted 215191885 1521047989
Solving for Scene Geometry
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521047989
There are generically two different approaches for 3D reconstruction referred
 to as 
\emph on
sparse
\emph default
 and 
\emph on
dense
\emph default
.
 Sparse methods reconstruct the 3D scene geometry only for a select subset
 of the entire image data 
\begin_inset CommandInset citation
LatexCommand cite
key "klein07parallel"

\end_inset

.
 This subset is often corner locations or locations marked by some type
 of feature extraction, e.g., SIFT or SURF.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Lowe2004,Bay:2008:SRF:1370312.1370556"

\end_inset

 This results in a sparse description of the 3D scene in terms of a point
 cloud.
 In contrast, dense methods 
\begin_inset CommandInset citation
LatexCommand cite
key "6696650"

\end_inset

 reconstruct as many 3D geometric locations as possible and seek to provide
 a complete description of the 3D scene.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521047989
Sparse reconstructions often benefit from having a lower computational cost
 but provide few 3D measurements.
 Dense reconstructions have higher computational cost but provide a much
 more complete description of the 3D scene.
 Dense reconstruction techniques have seen much recent interest, although
 a highly accurate, dense, and real-time SfM approach has remained elusive.
 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521047989
A third class of algorithms, referred to as 
\emph on
semi-dense
\emph default
 algorithms 
\begin_inset CommandInset citation
LatexCommand cite
key "6751290"

\end_inset

, seeks to strike a compromise between the sparse and dense methods.
 The reconstruction techniques used are most similar to dense methods, however,
 only a subset of all image pixels are reconstructed.
 These approaches leverage the high accuracy of dense reconstruction techniques,
 but are sparse enough to allow for real-time operation.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521070856
Here, reconstruction is limited to those pixels which possess high intensity
 gradient values.
 These regions often correspond to scene geometries such as edges, corners,
 and curves and to other areas of the scene that are highly textured.
 The thought here is that regions of the image that possess large changes
 in intensity convey more information than regions that possess less, thus
 semi-dense reconstructions provide a compressed version of the total scene
\end_layout

\begin_layout Subsection*

\change_inserted 215191885 1521070857
RGBD Measurement Noise
\begin_inset CommandInset label
LatexCommand label
name "subsec:RGBD_Measurement-Noise"

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521070857
The proposed fusion algorithm relies on experimental studies of accuracy
 and noise for RGBD sensor measurement, e.g., the Kinect sensor.
 Research in 
\begin_inset CommandInset citation
LatexCommand cite
key "7925286"

\end_inset

 shows that a Gaussian noise model provides a good fit to observed measurement
 errors on planar targets where the distribution parameters are mean 
\begin_inset Formula $0$
\end_inset

 and standard deviation
\begin_inset Formula $\sigma_{Z}=\frac{m}{2f_{x}b}Z^{2}$
\end_inset

 for depth measurements where 
\begin_inset Formula $\frac{m}{f_{x}b}=-2.85e^{-3}$
\end_inset

 is the linearized slope for the normalized disparity empirically found
 in 
\begin_inset CommandInset citation
LatexCommand cite
key "7925286"

\end_inset

.
 Since 3D the coordinates for 
\begin_inset Formula $(X,Y)$
\end_inset

 are a function of both the pixel location and the depth, their distributions
 are also known as shown below:
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521070857
\begin_inset Formula 
\begin{equation}
\begin{array}{ccc}
\sigma_{X} & = & \frac{x_{im}-c_{x}+\delta_{x}}{f_{x}}\sigma_{Z}=\frac{x_{im}-c_{x}+\delta_{x}}{f_{x}}(1.425e^{-3})Z^{2}\\
\sigma_{Y} & = & \frac{y_{im}-c_{y}+\delta_{y}}{f_{y}}\sigma_{Z}=\frac{y_{im}-c_{y}+\delta_{y}}{f_{y}}(1.425e^{-3})Z^{2}\\
\sigma_{Z} & = & \frac{m}{f_{x}b}Z^{2}\sigma_{d'}=(1.425e^{-3})Z^{2}
\end{array}\label{eq:noise_models}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521070857
These equations indicate that 3D coordinate measurement uncertainty increases
 as a quadratic function of the depth for all three coordinate values.
 However, the quadratic coefficient for the 
\begin_inset Formula $(X,Y)$
\end_inset

 coordinate standard deviation is at most half that in the depth direction,
 i.e., 
\begin_inset Formula $(\sigma_{X},\sigma_{Y})\approx0.5\sigma_{Z}$
\end_inset

 at the image periphery where 
\begin_inset Formula $\frac{x-c_{x}}{f}\approx0.5$
\end_inset

, and this value is significantly smaller for pixels close to the optical
 axis.
\change_unchanged

\end_layout

\begin_layout Section
Non linear Optimization
\end_layout

\begin_layout Standard
In practice, because of the noise in image correspondence and other errors
 we cannot measure the actual coordinates but only their noisy versions,
 say
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\widetilde{x}_{1}^{j}=x_{1}^{j}+\omega_{1}^{j}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\widetilde{x}_{2}^{j}=x_{2}^{j}+\omega_{2}^{j}
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $x_{1}^{j}$
\end_inset

and 
\begin_inset Formula $x_{2}^{j}$
\end_inset

are the ideal image coordinates and 
\begin_inset Formula $\omega_{1}^{j}=\left[\omega_{11}^{j},\omega_{12}^{j},0\right]^{T}$
\end_inset

and 
\begin_inset Formula $\omega_{2}^{j}=\left[\omega_{21}^{j},\omega_{22}^{j},0\right]^{T}$
\end_inset

are localization errors (called residuals) in the correspondence.Therefore,
 we need a way to optimize the parameters 
\begin_inset Formula $(x,R,T)$
\end_inset

 that minimize this errors.
\end_layout

\begin_layout Standard
One of the minimalistic approach to optimality is to minimize the squared
 2-norm of residuals, if we choose the first camera frame as the reference
 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\phi(x,R,T,\lambda)=\stackrel[j=1]{n}{\sum}\left\Vert \omega_{1}^{j}\right\Vert +\left\Vert \omega_{2}^{j}\right\Vert ^{2}=\stackrel[j=1]{n}{\sum}\left\Vert \widetilde{x}_{1}^{j}-x_{1}^{j}\right\Vert ^{2}+\left\Vert \widetilde{x}_{2}^{j}-\pi(R\lambda_{1}^{j}x_{1}^{j}+T)\right\Vert ^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
The above error is often called the 
\begin_inset Quotes eld
\end_inset

re-projection error
\begin_inset Quotes erd
\end_inset

, since 
\begin_inset Formula $x_{1}^{j}$
\end_inset

 and 
\begin_inset Formula $x_{2}^{j}$
\end_inset

are the recovered 3-D points projected back onto the image planes.
 This process of minimizing the above expression for the unknowns 
\begin_inset Formula $(R,T,x_{1},\lambda)$
\end_inset

 is known as bundle adjustment
\begin_inset CommandInset citation
LatexCommand cite
key "Triggs:1999:BAM:646271.685629"

\end_inset

.
\end_layout

\begin_layout Standard
One of the many ways to minimize this squared error is the Gauss-Newton
 method.
 Gauss-Newton is a iterative method for finding the value of the variables
 which minimizes the sum of squares,it starts with the initial guess and
 this method does not need the second derivatives (Hessian matrix) of the
 of function, which is often expensive and sometimes not possible to compute,
 instead the Hessian is approximated with the Jacobian matrix of the function.
 
\end_layout

\begin_layout Standard
For the least-square function of form
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\underset{x}{min}\underset{i}{\sum}r_{i}(x)^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
Gauss-Newton method iteratively solves 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
x_{t+1}=x_{t}-H^{-1}g
\]

\end_inset


\end_layout

\begin_layout Standard
with gradient g
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
g_{j}=2\underset{i}{\sum}r_{i}\frac{\partial r_{i}}{\partial x_{i}}
\]

\end_inset


\end_layout

\begin_layout Standard
and the Hessian H
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
H_{jk}=2\underset{i}{\sum}r_{i}(\frac{\partial r_{i}}{\partial x_{j}}\frac{\partial r_{i}}{\partial x_{k}}+r_{i}\frac{\partial^{2}r_{i}}{\partial x_{i}\partial x_{k}})
\]

\end_inset


\end_layout

\begin_layout Standard
by dropping the second order term we can approximate the Hessian matrix
 with Jacobian matrix 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
H_{jk}=2\underset{i}{\sum}J_{ij}J_{ik}
\]

\end_inset


\end_layout

\begin_layout Standard
with
\begin_inset Formula 
\[
J_{ij}=\frac{\partial r_{i}}{\partial x_{j}}
\]

\end_inset


\end_layout

\begin_layout Standard
The modification to Gauss-Newton method is The Levenberg-Marquardt algorithm,
 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
x_{t+1}=x_{t}-(H+\lambda I)^{-1}g
\]

\end_inset


\end_layout

\begin_layout Standard
this modification is a hybrid between the Newton method 
\begin_inset Formula $(\lambda=0)$
\end_inset

 and a gradient descent step size 
\begin_inset Formula $1/\lambda$
\end_inset

 for 
\begin_inset Formula $\lambda\longrightarrow\infty$
\end_inset


\change_inserted 215191885 1521070841

\end_layout

\begin_layout Chapter

\change_inserted 215191885 1521048998
LSD SLAM
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521048998
We are using LSD-SLAM 
\begin_inset CommandInset citation
LatexCommand cite
key "Engel2014"

\end_inset

 as our SfM implementation, it is  a 
\emph on
semi-dense
\emph default
, 
\emph on
direct
\emph default
 method which optimizes the geometry directly on the image intensities.
 LSD-SLAM provides as output a reconstruction of the observed 3D environment
 as a pose-graph of specially designated RGB image keyframes with associated
 semi-dense depth images.
 Direct correspondences are found between every RGB frame and each RGB keyframe
 to estimate the keyframe-to-RGB-frame relative camera pose.
 This is achieved by Levenberg-Marquardt optimization of the photometric
 error between the RGB image pair.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521048998
\begin_inset Formula 
\begin{equation}
E(\xi)=\underset{i}{\sum}(I_{ref}(P_{i})-I(\omega(p_{i},D_{ref}(p_{i}),\xi)))^{2}\label{eq:Photomatric Error}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521074363
Equation (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Photomatric Error"

\end_inset

) shows the specific image alignment object function and formalizes the
 form for equation (
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Image-alignment"

\end_inset

).
 Here, the warp function relies on the current estimate of depth 
\begin_inset Formula $D_{ref}$
\end_inset

 to determine the relative pose 
\begin_inset Formula $\xi\in sim(3)$
\end_inset

.
 The reference depth 
\begin_inset Formula $D_{ref}$
\end_inset

 is the depth associated with current key frame, these depth values could
 be initialized either with random values or with the depth measurements
 from a RGBD sensor to initiate the process.
 For every new frame tracked, the depths associated with the keyframe are
 continuously refined by performing the adaptive baseline stereo 3D reconstructi
on 
\begin_inset CommandInset citation
LatexCommand cite
key "6751290"

\end_inset

 between new tracked frames and the stack of previously tracked frames.
 When there is drastic pose change between the tracked frame and keyframe,
 the current tracked frame is promoted as a keyframe and depth map from
 previous keyframe is propagated to the new keyframe and regularized.
 Concurrently, all the keyframes are added as a nodes to a pose graph that
 stores the relative pose between the keyframes as edges/constraints 
\begin_inset CommandInset citation
LatexCommand cite
key "5979949"

\end_inset

.
 The pose graph stores the global trajectory of the camera in the 3D scene
 and an optimization algorithm processes the pose graph to improve the camera
 pose estimates as new image correspondences are found by image matching
 and loop closures.
 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521075243
The depth computation process involves finding the epiploar lines between
 new tracking frame and referance frames, along which the disparity for
 the pixle is determined.
 This process have two error sources, the error on disparity itself called
 the geometric disparity error and the error which encodes intensity differances
 called the photomatirc disparity error, The former error accounts for the
 magnitude of the image gradiant along the epliplor line , while the later
 one on the angle between the image gradient and the epipolar line.
\change_unchanged

\end_layout

\begin_layout Subsection*

\change_inserted 215191885 1521070779
Point Cloud Reconstruction
\begin_inset CommandInset label
LatexCommand label
name "subsec:Point-Cloud-Reconstruction"

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521070779
Measured 3D 
\begin_inset Formula $(X,Y,Z)$
\end_inset

 positions of sensed surfaces can be directly computed from the intrinsic
 camera parameters and depth image values.
 Here, the 
\begin_inset Formula $Z$
\end_inset

 coordinate is directly taken as the depth value and the 3D 
\begin_inset Formula $(X,Y)$
\end_inset

 coordinates are computed using the pinhole camera model.
 In a typical pinhole camera model 3D 
\begin_inset Formula $(X,Y,Z)$
\end_inset

 points are projected to 
\begin_inset Formula $(x,y)$
\end_inset

 image locations 
\begin_inset CommandInset citation
LatexCommand cite
key "Ma:2003:IVI:971144"

\end_inset

, e.g., for the image columns the 
\begin_inset Formula $x$
\end_inset

 image coordinate is 
\begin_inset Formula $x=f_{x}\frac{X}{Z}+c_{x}-\delta_{x}$
\end_inset

.
 However, for a depth image, this equation is re-organized to 
\begin_inset Quotes eld
\end_inset

back-project
\begin_inset Quotes erd
\end_inset

 the depth into the 3D scene and recover the 3D 
\begin_inset Formula $(X,Y)$
\end_inset

 coordinates as shown by equation (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:RGBD_reconstruction"

\end_inset

) 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521070779
\begin_inset Formula 
\begin{equation}
\begin{array}{ccc}
X & = & (x+\delta_{x}-c_{x})Z/f_{x}\\
Y & = & (y+\delta_{y}-c_{y})Z/f_{y}\\
Z & = & Z
\end{array}\label{eq:RGBD_reconstruction}
\end{equation}

\end_inset

where 
\begin_inset Formula $Z$
\end_inset

 denotes the sensed depth at image position 
\begin_inset Formula $(x,y)$
\end_inset

, 
\begin_inset Formula $(f_{x},f_{y})$
\end_inset

 denotes the camera focal length (in pixels), 
\begin_inset Formula $(c_{x},c_{x})$
\end_inset

 denotes the pixel coordinate of the image center, i.e., the principal point,
 and 
\begin_inset Formula $(\delta_{x},\delta_{y})$
\end_inset

 denote adjustments of the projected pixel coordinate to correct for camera
 lens distortion.
\end_layout

\begin_layout Subsection*

\change_inserted 215191885 1521070779
Point Cloud Re-Projection
\begin_inset CommandInset label
LatexCommand label
name "subsec:Point-Cloud-Re-Projection"

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521070779
Depth images can be simulated for camera sensor in arbitrary poses by 
\begin_inset Quotes eld
\end_inset

re-projection.
\begin_inset Quotes erd
\end_inset

 For discussion, assume that depth image 
\begin_inset Formula $\mathbf{d}(x,y)$
\end_inset

 has been recorded in the 
\begin_inset Quotes eld
\end_inset

standard
\begin_inset Quotes erd
\end_inset

 camera/optical coordinate system where the origin corresponds to the camera
 focal point, the 
\begin_inset Formula $z$
\end_inset

-axis corresponds to the depth/optical axis extending out into the viewed
 scene, the 
\begin_inset Formula $x$
\end_inset

-axis points towards the right and spans the image columns and the 
\begin_inset Formula $y$
\end_inset

-axis points downward and spans the image rows.
 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521070779
Let 
\begin_inset Formula $\mathbf{R}$
\end_inset

 denote the 3D rotation that rotates the coordinate axes of the standard
 coordinate system to align with the same axes of a second camera having
 arbitrary pose.
 Similarly, Let 
\begin_inset Formula $\mathbf{t}$
\end_inset

 denote the 3D translation vector describing the position of the focal point
 of a second camera having arbitrary pose.
 Using this notation, the re-projection algorithm consists of the following
 three steps:
\end_layout

\begin_layout Enumerate

\change_inserted 215191885 1521070779
Back-project 
\begin_inset Formula $\mathbf{d}(x,y)$
\end_inset

 to create an 
\begin_inset Formula $(X,Y,Z)$
\end_inset

 point cloud (as described in 
\begin_inset Formula $\S$
\end_inset


\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Point-Cloud-Reconstruction"

\end_inset

), 
\end_layout

\begin_layout Enumerate

\change_inserted 215191885 1521070779
Transform, i.e., rotate and translate, each point 
\begin_inset Formula $\mathbf{p}_{i}=[X,Y,Z]^{t}$
\end_inset

 in the point cloud to generate a new point 
\begin_inset Formula $\mathbf{p}_{i}^{'}=[X^{'},Y^{'},Z^{'}]^{t}$
\end_inset

 that lies in a standard optical coordinate system centered on the second
 camera's focal point and having orientation that aligns with corresponding
 
\begin_inset Formula $x,y,z$
\end_inset

-axes using equation (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:coordinate_system_change"

\end_inset

),
\begin_inset Formula 
\begin{equation}
\mathbf{p}_{i}^{'}=\mathbf{R}(\mathbf{p}_{i}-\mathbf{t})\label{eq:coordinate_system_change}
\end{equation}

\end_inset


\end_layout

\begin_layout Enumerate

\change_inserted 215191885 1521070779
Re-project the 
\begin_inset Formula $(X,Y,Z)$
\end_inset

 point cloud using the pinhole camera equations to compute the new depth
 image 
\begin_inset Formula $\mathbf{d}^{'}(x,y)=Z$
\end_inset

 using equation (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:re_projection"

\end_inset

).
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521070779
\begin_inset Formula 
\begin{equation}
\begin{array}{ccc}
x & = & f_{x}(\frac{X^{'}}{Z^{'}})-\delta_{x}+c_{x}\\
y & = & f_{y}(\frac{Y^{'}}{Z^{'}})-\delta_{y}+c_{y}\\
Z & = & Z^{'}
\end{array}\label{eq:re_projection}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521070779
Typically, the re-projected point cloud measurements fall at non-integer
 locations in the new depth image and the values of 
\begin_inset Formula $\mathbf{d}^{'}(x,y)$
\end_inset

 must then be interpolated via bilinear interpolation or some other interpolatio
n scheme (nearest neighbor).
 
\end_layout

\begin_layout Chapter

\change_inserted 215191885 1521071426
Methodology
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521071427
\begin_inset Float figure
wide true
sideways false
status collapsed

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521071427
\begin_inset Graphics
	filename /home/akash/southeastcon/2018_SeCon_RGBDSfM_Fusion/images/overview_coloum_span.png
	lyxscale 20
	width 7in
	height 2.5in
	keepAspectRatio

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1521071427
\begin_inset CommandInset label
LatexCommand label
name "fig:The-complete-process"

\end_inset

An overview of the proposed depth fusion algorithm.
\end_layout

\end_inset


\end_layout

\end_inset

The proposed fusion approach applies the semi-dense monocular reconstruction
 approach referred to as Large Scale Direct (LSD) SLAM 
\begin_inset CommandInset citation
LatexCommand cite
key "Engel2014"

\end_inset

.
 The LSD-SLAM algorithm solves the SfM problem using a 
\emph on
direct
\emph default
 method to compute pixel correspondences and a 
\emph on
semi-dense
\emph default
 method for 3D reconstruction.
 We select this approach as it does not require or impose any prior knowledge
 about the scene structure as required by 
\emph on
dense 
\emph default
reconstruction methods and it gives more 3D estimates than 
\emph on
sparse
\emph default
 approaches while having similar computational cost.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521071427
The LSD-SLAM SfM algorithm consists of the following three components:
\end_layout

\begin_layout Enumerate

\change_inserted 215191885 1521071427
A tracking component that estimates the pose of the camera
\end_layout

\begin_layout Enumerate

\change_inserted 215191885 1521071427
A depth map estimation component that estimates semi-dense depth images
 for keyframes
\end_layout

\begin_layout Enumerate

\change_inserted 215191885 1521071427
A map optimization component that seeks to create a 3D map of the environment
 that is self-consistent.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521071427
This work utilizes the first two components to explore fusion of RGBD depth
 images with SfM depth images.
 For this work, the map optimization component (3) is not used.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:The-complete-process"

\end_inset

 depicts an overview of the proposed depth fusion algorithm.
\end_layout

\begin_layout Subsection

\change_inserted 215191885 1521071427
Time and Spatial Sampling Issues
\begin_inset CommandInset label
LatexCommand label
name "subsec:Time-and-Spatial"

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521071427
As mentioned previously, RGBD sensors measure depth at a rate of 30 frames
 per second (fps) and LSD-SLAM computes depth images only for 
\emph on
keyframes 
\emph default
which is a sparse subset of the measured RGB frames.
 Further, keyframes are not generated uniformly in time but created when
 the SfM algorithm detects criteria required to create a new keyframe.
 This condition is triggered when the current camera pose is too far from
 the most recent keyframe camera pose and when the current frame tracking
 result is 
\begin_inset Quotes eld
\end_inset

good
\begin_inset Quotes erd
\end_inset

 in the sense that the image warping correspondence objective function suggests
 an accurate or low-error result.
 As a result, SfM-estimated depths exist only for those RGB images designated
 as keyframes.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521071427
Further, the spatial distribution of SfM-estimated depths within SfM keyframes
 are localized to only those pixels having 
\begin_inset Quotes eld
\end_inset

good
\begin_inset Quotes erd
\end_inset

 3D reconstruction characteristics.
 In this sense, the quality of the depth estimate depends on accurately
 matching pixels along epipolar lines inscribed in the image.
 The matching performance here is best when there is a significant change
 in the image intensities along the epipolar line.
 Hence, 3D depth reconstruction is limited to those pixels that lie at sharp
 intensity changes, i.e., 
\begin_inset Quotes eld
\end_inset

edge
\begin_inset Quotes erd
\end_inset

 pixels, and further limited to those 
\begin_inset Quotes eld
\end_inset

edge
\begin_inset Quotes erd
\end_inset

 pixels that lie on edges that are roughly perpendicular to the direction
 of the epipolar line (see 
\begin_inset CommandInset citation
LatexCommand cite
key "6751290"

\end_inset

 for details).
 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521071427
The LSD-SLAM algorithm estimates depth at 
\begin_inset Quotes eld
\end_inset

good
\begin_inset Quotes erd
\end_inset

 pixel positions as a 1-dimensional Gaussian distribution specified as a
 mean image 
\begin_inset Formula $\mu_{SfM}(x,y)$
\end_inset

, i.e., the estimated depth image, and a variance image 
\begin_inset Formula $\sigma_{SfM}^{2}(x,y)$
\end_inset

 such that the RGB keyframe pixel at location 
\begin_inset Formula $\mathbf{I}(x,y)$
\end_inset

 is estimated to have depth 
\begin_inset Formula $\mu_{SfM}(x,y)$
\end_inset

 with uncertainties given by 
\begin_inset Formula $\sigma_{SfM}^{2}(x,y)$
\end_inset

.
 In this sense, the keyframe image 
\begin_inset Formula $\mathbf{I}(x,y)$
\end_inset

 augmented with the estimated depth image 
\begin_inset Formula $\mu_{SfM}(x,y)$
\end_inset

 is analogous in format to sensed RGBD image data.
 Yet, the uncertainties for the image 
\begin_inset Formula $\mu_{SfM}(x,y)$
\end_inset

 are given by the image 
\begin_inset Formula $\sigma_{SfM}^{2}(x,y)$
\end_inset

 rather than the experimentally validated uncertainties discussed in 
\begin_inset ERT
status open

\begin_layout Plain Layout

\change_inserted 215191885 1521071427


\backslash
S
\end_layout

\end_inset


\begin_inset space ~
\end_inset

 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:RGBD_Measurement-Noise"

\end_inset

.
\end_layout

\begin_layout Subsection

\change_inserted 215191885 1521071427
Image Registration Issues
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521071427
Fusing depth measurements requires knowledge of the correspondence between
 the depth measurements generated from the RGBD sensor and the SfM algorithm.
 For SfM keyframes this correspondence is trivial due to the fact that RGBD
 sensors support hardware registration.
 Hardware registration co-locates the RGBD depth image measurements, 
\begin_inset Formula $\mathbf{d}_{rgbd}(x,y)$
\end_inset

, and RGB appearance values, 
\begin_inset Formula $\mathbf{I}(x,y)$
\end_inset

.
 Hence, for hardware-registered RGBD depth images, 
\begin_inset Formula $\mathbf{d}_{rgbd}(x,y)$
\end_inset

 is the measured depth of the surface having RGB pixel 
\begin_inset Formula $\mathbf{I}(x,y)$
\end_inset

.
 Similarly, SfM-estimated depths for an RGB keyframe, 
\begin_inset Formula $\mu d_{SfM}(x,y)$
\end_inset

, are the depths for the surface having RGB pixel 
\begin_inset Formula $\mathbf{I}(x,y)$
\end_inset

.
 Hence fusion is accomplished by fusing the measurements at corresponding
 
\begin_inset Formula $(x,y)$
\end_inset

 locations in the RGBD depth image, 
\begin_inset Formula $\mathbf{d}_{rgbd}(x,y)$
\end_inset

, and the SfM depth image, 
\begin_inset Formula $\mu\mathbf{d}_{SfM}(x,y)$
\end_inset

.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521479929
Depth correspondences for RGB images that are not SfM keyframes must be
 computed from one or more SfM keyframe depth images.
 This article uses the most recent, i.e., closest-in-time, keyframe to generate
 co-registered SfM depth images for arbitrary RGB images.
 To do so, the depth image from the most recent keyframe, 
\begin_inset Formula $\mathbf{d}_{SfM}(x,y)$
\end_inset

, is 
\begin_inset Quotes eld
\end_inset

back-projected
\begin_inset Quotes erd
\end_inset

 to create a 3D point cloud of SfM measurements.
 Using the estimated keyframe-to-camera pose change, the 3D measurements
 are then re-projected into the RGB camera image plane using the 3D projection
 equations for the camera provided via camera calibration.
 The resulting depth image, 
\begin_inset Formula $\widetilde{\mathbf{d}}_{SfM}(x,y)$
\end_inset

 is then co-registered with the RGBD depth image 
\begin_inset Formula $\mathbf{d}_{rgbd}(x,y)$
\end_inset

 and RGB image 
\begin_inset Formula $\mathbf{I}(x,y)$
\end_inset

.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521513963
However, due to the noise in depth computation and consequent trajectory
 estimation, the depth regestration is not alway correct, this should be
 adressed as early as possible to avoid the drift in their computations.
 We try to achive better depth registration by estimating the transformation
 error between the point clouds of sensor depth measurments and LSD depth
 estimation.
 Since the LSD estimations are not to the scale, we also need to estmate
 the scale for better regestation of depths.
 The Umeyama's Least-Square estimation 
\begin_inset CommandInset citation
LatexCommand cite
key "Umeyama:1991:LET:105514.105525"

\end_inset

 shows us how to estimate these 
\begin_inset Formula $Sim(3)$
\end_inset

 transformation parameters efficiantly.
 In his work, Umeyama shows how to estimates the rotation, translation and
 scale between two point clouds with known correspondance.
 In order to avoid errors, we use only the valid depths in both the point
 cloud for estimation.By using 
\begin_inset Formula $\mathbf{pc}_{rgbd}$
\end_inset

and 
\begin_inset Formula $\mathbf{pc}_{SfM}$
\end_inset

to represent the valid point clouds of sensor measurments and LSD estimation
 respectivly, we comput the mean, variances and covariance of both them
 as following.
\change_unchanged

\end_layout

\begin_layout Standard

\change_inserted 215191885 1521484409
\begin_inset Formula 
\[
\mu_{rgbd}=\frac{\sum\mathbf{pc}_{rgbd}}{n}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521484766
\begin_inset Formula 
\[
\mu_{SfM}=\frac{\sum\boldsymbol{pc}_{SfM}}{n}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521484766
\begin_inset Formula 
\[
\sigma_{rgbd}^{2}=\frac{\sum\left\Vert \boldsymbol{pc}_{rgbd}-\mu_{rgbd}\right\Vert ^{2}}{n}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521484795
\begin_inset Formula 
\[
\sigma_{SfM}^{2}=\frac{\sum\left\Vert \boldsymbol{pc}_{SfM}-\mu_{SfM}\right\Vert ^{2}}{n}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521501433
\begin_inset Formula 
\[
\Sigma_{SfM,rgbd}=\frac{\sum(\boldsymbol{pc}_{rgbd}-\mu_{rgbd})(\boldsymbol{pc}_{SfM}-\mu_{SfM})^{T}}{n}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521501595
With singular value decomposition of 
\begin_inset Formula $\Sigma_{SfM,rgbd}$
\end_inset

 as 
\begin_inset Formula $UDV^{T},$
\end_inset

the optimum rotation matrix 
\begin_inset Formula $R$
\end_inset

 which achieves the minimum error between point clouds is given by
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521501637
\begin_inset Formula 
\[
R=USV^{T}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521507419
\begin_inset Formula 
\[
S=I\begin{cases}
I & det(\Sigma_{SfM,rgbd})\geq0\\
diag(1,1,-1) & det(\Sigma_{SfM,rgbd})<0
\end{cases}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521507451
The scale adjustment 
\begin_inset Formula $c$
\end_inset

 is computed by
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521507699
\begin_inset Formula 
\[
c=\frac{1}{\sigma_{SfM}^{2}}trace(DS)
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521507819
With scale and rotation established, the translation parameters are determined
 as 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521508665
\begin_inset Formula 
\[
t=\mu_{rgbd}-cR\mu_{SfM}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521513995
with the 
\begin_inset Formula $Sim(3)$
\end_inset

 parameters we trasanform LSD point clouds before projecting them to find
 
\begin_inset Formula $\widetilde{\mathbf{d}}_{SfM}(x,y)$
\end_inset

 
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521514011
\begin_inset Formula 
\[
\tilde{\boldsymbol{pc_{SfM}}}=cR\boldsymbol{pc}_{SfM}+t
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521071427
Using these techniques co-registered SfM depth images can be computed for
 a general RGBD image.
 When RGBD images correspond to SfM keyframes the registration is 
\begin_inset Quotes eld
\end_inset

automatic,
\begin_inset Quotes erd
\end_inset

 i.e., no computation is necessary.
 In all other cases, a co-registered SfM depth image must be computed by
 reconstructing a 3D point cloud from a keyframe and then projecting the
 point cloud into the target RGB camera image using the estimated camera
 calibration and keyframe-to-camera-frame relative pose parameters.
\end_layout

\begin_layout Subsection

\change_inserted 215191885 1521071427
Resolving the Unknown SfM Scale
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521071427
The methods described in previous sections detail how co-registered SfM
 depth measurements are computed for every sensed RGBD frame.
 However, as discussed in previously, SfM depth images intrinsically have
 an unknown scale, 
\begin_inset Formula $\alpha$
\end_inset

, which reflects the fact that the solution for the scene structure is not
 geometrically unique, i.e., the same scene structure can be observed at a
 infinite number of distinct scales.
 Therefore fusion requires the scale of the SfM depth image to fit the scale
 of the real-world scene measured by the RGBD camera.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521071427
Given that the depth measurements for the RGBD depth image are co-registered
 with the SfM estimated depth image the scale parameter can be directly
 estimated by minimizing the sum of the squared depth errors 
\begin_inset CommandInset citation
LatexCommand cite
key "Bishop:2006:PRM:1162264"

\end_inset

 between the SfM depth image and the RGBD depth image.
 Let 
\begin_inset Formula $V$
\end_inset

 denote the set of 
\begin_inset Formula $(x,y)$
\end_inset

 positions that have valid depth measurements for 
\begin_inset Quotes eld
\end_inset

standard
\begin_inset Quotes erd
\end_inset

 fusion as described in 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Fusing-RGBD-Depths"

\end_inset

.
 Equation (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:scale error"

\end_inset

) shows the error function used to compute the unknown scale value and equation
 (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:depth_scale_solution"

\end_inset

) shows the solution 
\begin_inset Formula $\widehat{\alpha}$
\end_inset

 that minimizes this error.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521071427
\begin_inset Formula 
\begin{equation}
e(\alpha)=\sum_{(x,y)\epsilon V}\left\Vert \mathbf{d}_{rgbd}(x,y)-\alpha\mathbf{d}_{SfM}(x,y)\right\Vert ^{2}\label{eq:scale error}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521071427
\begin_inset Formula 
\begin{equation}
\widehat{\alpha}=\sum_{(x,y)\epsilon V}\frac{\mathbf{d}{}_{rgbd}(x,y)}{\mathbf{d}_{SfM}(x,y)}\label{eq:depth_scale_solution}
\end{equation}

\end_inset


\end_layout

\begin_layout Subsection

\change_inserted 215191885 1521071427
RGBD and SfM Depth Fusion
\begin_inset CommandInset label
LatexCommand label
name "subsec:Fusing-RGBD-Depths"

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521071427
For fusing measurements we consider the structured-light measurement of
 the RGBD sensor to generate a distribution for the unknown true depth of
 the scene surfaces at each RGBD 
\begin_inset Formula $(x,y)$
\end_inset

 pixel in the depth image.
 These measurements are considered to be independent and identically distributed
 to the measurements of the true unknown depth of the scene surfaces from
 the registered SfM estimated depths.
 With these assumptions, solving the depth fusion problem is equivalent
 to estimating the posterior distribution of the true scene depth at each
 
\begin_inset Formula $(x,y)$
\end_inset

 position given the distributions for the RGBD and SfM depth values.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521071427
Fortunately, previous sections show that Gaussian models are appropriate
 distributions for both the RGBD and SfM depth values and the parameters
 of these models are either known (see 
\begin_inset ERT
status open

\begin_layout Plain Layout

\change_inserted 215191885 1521071427


\backslash
S
\end_layout

\end_inset


\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:RGBD_Measurement-Noise"

\end_inset

) or estimated continuously (see 
\begin_inset ERT
status open

\begin_layout Plain Layout

\change_inserted 215191885 1521071427


\backslash
S
\end_layout

\end_inset


\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Time-and-Spatial"

\end_inset

).
 When both distributions are Gaussian, the posterior distribution can be
 found analytically and is a well-known result used in pattern recognition
 and other prediction frameworks, e.g., the Kalman filter as discussed in
 
\begin_inset CommandInset citation
LatexCommand cite
key "Maybeck79stochasticsmodels"

\end_inset

.
 Specifically, let the Gaussian noise for RGBD depth at position 
\begin_inset Formula $(x,y)$
\end_inset

 be represented as 
\begin_inset Formula $\mathcal{N}(\mathbf{d}_{rgbd},\sigma_{rgbd}^{2})$
\end_inset

 and the Gaussian noise for the co-registered SfM depth image at position
 
\begin_inset Formula $(x,y)$
\end_inset

 be 
\begin_inset Formula $\mathcal{N}(\mu_{SfM},\sigma_{SfM}^{2})$
\end_inset

.
 The posterior distribution on the unknown true depth at position 
\begin_inset Formula $(x,y)$
\end_inset

 is also Gaussian and let 
\begin_inset Formula $\mathcal{N}(\mu_{fused},\sigma_{fused}^{2})$
\end_inset

 denote the mean and variance parameters of this distribution.
 Equations (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:fused_mean"

\end_inset

) and (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:fused_variance"

\end_inset

) provide optimal estimates of the mean and variance of the fused depth
 at position 
\begin_inset Formula $(x,y)$
\end_inset

.
 The best estimate of the fused depth is given by the highest probability
 value in the posterior distribution which is the mean fused image, 
\begin_inset Formula $\mu_{fused}(x,y)$
\end_inset

.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521071427
\begin_inset Formula 
\begin{equation}
\mu_{fused}=\frac{\mathbf{d}_{rgbd}\sigma_{SfM}^{2}+\mu_{SfM}\sigma_{rgbd}^{2}}{\sigma_{SfM}^{2}+\sigma_{rgbd}^{2}}\label{eq:fused_mean}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521514889
\begin_inset Formula 
\begin{equation}
\sigma_{fused}^{2}=\frac{\sigma_{rgbd}^{2}\sigma_{SfM}^{2}}{\sigma_{rgbd}^{2}+\sigma_{SfM}^{2}}\label{eq:fused_variance}
\end{equation}

\end_inset


\end_layout

\begin_layout Chapter

\change_inserted 215191885 1521515010
Results
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521515011
\begin_inset Float figure
wide true
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521515011
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521515011
\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "1in"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521515011
\begin_inset Graphics
	filename images/exp1/intensity_image.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521515011
\begin_inset Graphics
	filename images/exp2/intensity.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521516923
\begin_inset Graphics
	filename images/exp3/intenstiy.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521516923
\begin_inset Graphics
	filename images/in_to_out_with_transformation_error/intensity.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521516923
\begin_inset Graphics
	filename images/indoor_good_scale_and_transform/intensity.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\change_inserted 215191885 1521515011
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1521515011

\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521515011
\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "1in"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521515011
\begin_inset Graphics
	filename images/exp1/sensor_measurments.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521515011
\begin_inset Graphics
	filename images/exp2/sensor_measurments.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521516546
\begin_inset Graphics
	filename images/exp3/sensor_depth.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521516754
\begin_inset Graphics
	filename images/in_to_out_with_transformation_error/sensor_measurment.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521516754
\begin_inset Graphics
	filename images/indoor_good_scale_and_transform/sensor_depth.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\change_inserted 215191885 1521515011
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1521515011

\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521515011
\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "1in"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521515011
\begin_inset Graphics
	filename images/exp1/scaled_lsd_depth.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521515011
\begin_inset Graphics
	filename images/exp2/scaled_lsd.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521516530
\begin_inset Graphics
	filename images/exp3/scaled_Depth.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521516757
\begin_inset Graphics
	filename images/in_to_out_with_transformation_error/scaled_lsd_depth.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521516757
\begin_inset Graphics
	filename images/indoor_good_scale_and_transform/scaled_lsd.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\change_inserted 215191885 1521515011
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1521515011

\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521515011
\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "1in"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521515011
\begin_inset Graphics
	filename images/exp1/fused_depth_sets.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521515011
\begin_inset Graphics
	filename images/exp2/depth_Fusion_set.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521516534
\begin_inset Graphics
	filename images/exp3/sensor_depth_set.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521516764
\begin_inset Graphics
	filename images/in_to_out_with_transformation_error/fused_set.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521516764
\begin_inset Graphics
	filename images/indoor_good_scale_and_transform/fused_set.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\change_inserted 215191885 1521515011
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1521515011

\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521515011
\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "1in"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521515011
\begin_inset Graphics
	filename images/exp1/std_deviation_colormap.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521515011
\begin_inset Graphics
	filename images/exp2/std_deviation_colormap.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521516538
\begin_inset Graphics
	filename images/exp3/std_deviation_colmap.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521516771
\begin_inset Graphics
	filename images/in_to_out_with_transformation_error/std_dev_colMap.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521516771
\begin_inset Graphics
	filename images/indoor_good_scale_and_transform/std_deviation_colormap.png
	lyxscale 10
	height 1in

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\change_inserted 215191885 1521515011
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1521515011

\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521515011
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1521515011
Results for three experiments are shown.
 Images shown are organized into separate columns.
 Column (a) shows a grayscale image of the scene (b) shows the sensed RGBD
 depth image (c) shows the SfM-estimated depth image (d) shows the fused
 image and (e) shows the standard deviation for fused depths (in 
\begin_inset Formula $m.$
\end_inset

).
 The fused image has been color-coded as follows: (white) denotes depth
 locations sensed only by the RGBD sensor, (yellow) denotes depth locations
 only sensed via SfM, (red) denotes fused (RGBD+SfM) depth locations and
 (black) denotes depth locations without RGBD or SfM measurements.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521514993
We conducted experiments with different setups to test and analyze the depth
 fusion algorithm.
 Experiments recorded RGBD image sequenced from ORBBEC Astra RGBD sensors
 and applied LSD-SLAM to their image streams using their factory-provided
 intrinsic camera calibration parameters.
 Each experiment included approximately 60 seconds of RGBD image data at
 the rate of 30 fps.
 Experimentally recorded RGB images were processed offline by the LSD-SLAM
 algorithm to generate SfM depth images.
 Experiments initialize the LSD-SLAM algorithm with the first recorded depth
 image from the RGBD sensor to facilitate the initial scale approximation.
 The output from the LSD-SLAM algorithm consisting of the relative pose
 for every tracked frame and the depth map for each keyframe was then captured
 to disk.
 The fusion algorithm was then run offline on the recorded RGBD image stream
 and LSD-SLAM output files to generate the results shown in this section.
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center

\change_inserted 215191885 1521514993
\begin_inset Tabular
<lyxtabular version="3" rows="8" columns="4">
<features tabularvalignment="middle">
<column alignment="center" valignment="top" width="0pt">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell multirow="3" alignment="center" valignment="middle" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521514993

\size footnotesize
Experiment
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521514993

\size footnotesize
RGBD-only depths
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521514993

\size footnotesize
SfM-only depths
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521514993

\size footnotesize
Fused depths
\end_layout

\end_inset
</cell>
</row>
<row>
<cell multirow="4" alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521514993

\size footnotesize
(%)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521514993

\size footnotesize
(%)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521514993

\size footnotesize
(%) 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521514993
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521514993
67.7
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521514993
6.7
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521514993
25.5
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521514993
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521514993
55.5
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521514993
10.0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521514993
34.3
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521514993
3
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521514993
68.1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521514993
15.1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521514993
16.6
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521522076
4
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521522089
51.0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521522116
25.7
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521522139
23.1
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521522077
5
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521522195
68.3
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521522214
8.8
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521522249
22.8
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521514993

\size footnotesize
Average
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521522298
62.12
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521522330
13.26
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\change_inserted 215191885 1521522369
24.46
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout

\change_inserted 215191885 1521514993
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 215191885 1521514993
Ratios of depth measurements to total depths
\begin_inset CommandInset label
LatexCommand label
name "tab:Ratios-of-depth"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 215191885 1521514993
Experiment 1 depicts a indoor office scene at the university.
 This scene includes specular and dark surface structures at close range
 that are not measured by the RGBD sensor.
 Yet, the SfM algorithm estimates depths at a number of locations (on the
 podium) where ther are significant intensity changes.
 These additional depths are evident in the fused results, which includes
 SfM-only depth measurements in regions in the vicinity of image edges.
 The experiment demonstrates that depth fusion can improve depth images
 by obtaining depths from surfaces not measurable by the RGBD sensor.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521519752
Experiment 2 depicts a hallway in the UNCC EPIC building that includes both
 specular surfaces and high intensity illumination from overhead lights.
 The experiment is a second example showing that depth fusion bolsters over
 all depth measurement performance by providing the depths when RGBD sensors
 fail.
 The SfM takes advantage of the patterns on the surface and estimates the
 depths irrespective of the nature of its reflectance properties and color.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521518425
Experiment 3 depicts the UNCC faculty conference hall.
 Here a number of scene structures lie beyond the measurement range of the
 RGBD sensor.
 Yet, the SfM algorithm is able to estimate the depth of these scene structures
 (albeit at high variance) providing depths that would otherwise not be
 possible.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521521717
Experiment 4 depicts indoor to outdoor transition.
 By initializing the LSD SLAM indoor, we get the scale estimated.
 We can see that in outdoor, when the sensor measurements fails because
 of the infrared ray flooding, depth estimates from LSD SLAM are used.
 Often because of IR flooding the sensor measurements gets corrupted, in
 those scenarios we can consider replacing entire corrupted depth sensor
 measurements by LSD depth estimation instead of fusion for better results.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521521903
Experiment 5 depicts indoor with well lit conditions, even though the sensors
 are supposed work completely fine in this scenario, there are rare chances
 of the them failing at steep surfaces.
 During such failures, depth fusion comes handy.
\end_layout

\begin_layout Standard

\change_inserted 215191885 1521523429
Table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:Ratios-of-depth"

\end_inset

 quantifies the amount of additional depth information provided via RGBD-SfM
 depth image fusion.
 When we average over the five experiments discussed approximately 62% of
 the fused depths originate from the RGBD sensor alone, approximately 13%
 original from the SfM algorithm alone, and approximately 24% of the fused
 depths results from RGBD-SfM fusion.
 The addition of 13% of novel depth data is a significant contribution.
 Further, fused depths account for roughly 24% of depth image data and the
 measurement error for all of these measurements will be reduced by the
 fusion.
 Variance reduction will be greatest for low-variance SfM depth estimates
 which are typically in textured scene locations close to the camera.
 Yet, we note that by inspection of equation (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:fused_variance"

\end_inset

), it is theoretically impossible for the variance of any fusion result
 to increase.
 Another obvious observation from the Table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:Ratios-of-depth"

\end_inset

 is that the SfM has a largest contribution in Experiment 4 (outdoor scenario)
 since sensor fails outdoors and it has least contribution in Experiment
 5 (well lit indoor) where sensor is at its best.
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "references_db"
options "ieeetr"

\end_inset


\end_layout

\end_body
\end_document
